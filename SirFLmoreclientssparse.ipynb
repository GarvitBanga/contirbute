{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SirFLmoreclientssparse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdeMLDgYgMB5",
        "outputId": "b977ea54-3581-484a-f277-ffec2b49542f"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT8wGfuwhM09"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import tensorflow as tfs\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2, random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CABdsel4jI4C"
      },
      "source": [
        "height=28\n",
        "width=28\n",
        "depth=1\n",
        "\n",
        "inputShape = (height, width, depth)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeP3bd4jhVb7",
        "outputId": "c8e1abfc-d8af-4ad9-b62a-2af68449bb20"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test)=mnist.load_data()\n",
        "print(x_train.shape)\n",
        "x_train= x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train/=255\n",
        "x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test/=255\n",
        "# x_train=np.array(x_train).reshape((-1,height, width, depth))\n",
        "# x_test=np.array(x_test).reshape((-1,height, width, depth))\n",
        "print(y_test.shape)\n",
        "y_train=np.array(y_train).reshape(-1,1)\n",
        "y_test=np.array(y_test).reshape(-1,1)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA2C0sm6Pnme",
        "outputId": "a4801780-e607-46f6-caea-a3bf4da9f2fe"
      },
      "source": [
        "from  keras.utils import np_utils\n",
        "# y_train = np_utils.to_categorical(y_train)\n",
        "# y_test= np_utils.to_categorical(y_test)\n",
        "print(y_train[0])\n",
        "print(y_train.shape,y_test.shape)\n",
        "# y_train=y_train.reshape(60000,1)\n",
        "# y_test=y_test.reshape(10000,1)\n",
        "# print(y_train[0])\n",
        "# print(y_train.shape,y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5]\n",
            "(60000, 1) (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNobqP1lhXxZ",
        "outputId": "64367f51-8fa1-4e1a-801f-3c8ec4dafecf"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train[0],x_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "[5] [[[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.01176471]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.49411765]\n",
            "  [0.53333336]\n",
            "  [0.6862745 ]\n",
            "  [0.10196079]\n",
            "  [0.6509804 ]\n",
            "  [1.        ]\n",
            "  [0.96862745]\n",
            "  [0.49803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.11764706]\n",
            "  [0.14117648]\n",
            "  [0.36862746]\n",
            "  [0.6039216 ]\n",
            "  [0.6666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.88235295]\n",
            "  [0.6745098 ]\n",
            "  [0.99215686]\n",
            "  [0.9490196 ]\n",
            "  [0.7647059 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.19215687]\n",
            "  [0.93333334]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.9843137 ]\n",
            "  [0.3647059 ]\n",
            "  [0.32156864]\n",
            "  [0.32156864]\n",
            "  [0.21960784]\n",
            "  [0.15294118]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.7137255 ]\n",
            "  [0.96862745]\n",
            "  [0.94509804]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.3137255 ]\n",
            "  [0.6117647 ]\n",
            "  [0.41960785]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8039216 ]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.16862746]\n",
            "  [0.6039216 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.05490196]\n",
            "  [0.00392157]\n",
            "  [0.6039216 ]\n",
            "  [0.99215686]\n",
            "  [0.3529412 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.54509807]\n",
            "  [0.99215686]\n",
            "  [0.74509805]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.04313726]\n",
            "  [0.74509805]\n",
            "  [0.99215686]\n",
            "  [0.27450982]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.13725491]\n",
            "  [0.94509804]\n",
            "  [0.88235295]\n",
            "  [0.627451  ]\n",
            "  [0.42352942]\n",
            "  [0.00392157]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.31764707]\n",
            "  [0.9411765 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.46666667]\n",
            "  [0.09803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.1764706 ]\n",
            "  [0.7294118 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.5882353 ]\n",
            "  [0.10588235]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.0627451 ]\n",
            "  [0.3647059 ]\n",
            "  [0.9882353 ]\n",
            "  [0.99215686]\n",
            "  [0.73333335]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.9764706 ]\n",
            "  [0.99215686]\n",
            "  [0.9764706 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.18039216]\n",
            "  [0.50980395]\n",
            "  [0.7176471 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8117647 ]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.15294118]\n",
            "  [0.5803922 ]\n",
            "  [0.8980392 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.98039216]\n",
            "  [0.7137255 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09411765]\n",
            "  [0.44705883]\n",
            "  [0.8666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7882353 ]\n",
            "  [0.30588236]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09019608]\n",
            "  [0.25882354]\n",
            "  [0.8352941 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.31764707]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.67058825]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7647059 ]\n",
            "  [0.3137255 ]\n",
            "  [0.03529412]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.21568628]\n",
            "  [0.6745098 ]\n",
            "  [0.8862745 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.95686275]\n",
            "  [0.52156866]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.53333336]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.83137256]\n",
            "  [0.5294118 ]\n",
            "  [0.5176471 ]\n",
            "  [0.0627451 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgvP-TsiozNq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X1, X2, y1, y2 = train_test_split(x_train, \n",
        "#                                                     y_train, \n",
        "#                                                     test_size=0.25, \n",
        "#                                                     random_state=42)\n",
        "# print(len(X1),len(y2))\n",
        "# X3,X4,y3,y4=train_test_split(X1, \n",
        "#                                                     y1, \n",
        "#                                                     test_size=1/3, \n",
        "#                                                     random_state=42)\n",
        "# print(len(X3),len(y4))\n",
        "# X5,X6,y5,y6=train_test_split(X3, \n",
        "#                                                     y3, \n",
        "#                                                     test_size=0.5, \n",
        "#                                                     random_state=42)\n",
        "# print(len(X5),len(y6))\n",
        "# print(y_train[0])\n",
        "# abc=x_train[0].copy()\n",
        "# abc=abc.reshape((28,28))\n",
        "# from matplotlib import pyplot as plt\n",
        "# plt.imshow(abc,cmap=\"Greys\")\n",
        "# plt.show()\n",
        "from sklearn.utils import shuffle\n",
        "x_train,y_train = shuffle(x_train, y_train, random_state=42)\n",
        "# X1,y1=x_train[:6000],y_train[:6000]\n",
        "# X2,y2=x_train[6000:12000],y_train[6000:12000]\n",
        "# X3,y3=x_train[12000:18000],y_train[12000:18000]\n",
        "# X4,y4=x_train[18000:24000],y_train[18000:24000]\n",
        "# X5,y5=x_train[24000:30000],y_train[24000:30000]\n",
        "# X6,y6=x_train[30000:36000],y_train[30000:36000]\n",
        "# X7,y7=x_train[36000:42000],y_train[36000:42000]\n",
        "# X8,y8=x_train[42000:48000],y_train[42000:48000]\n",
        "# X9,y9=x_train[48000:54000],y_train[48000:54000]\n",
        "# X10,y10=x_train[54000:60000],y_train[54000:60000]\n",
        "ix=0\n",
        "trainX=list()\n",
        "trainy=list()\n",
        "for ind11 in range(60):\n",
        "  trainX.append(x_train[ix:ix+1000])\n",
        "  trainy.append(y_train[ix:ix+1000])\n",
        "  ix=ix+1000\n",
        "# print(y_train[0])\n",
        "# abc=x_train[0].copy()\n",
        "# abc=abc.reshape((28,28))\n",
        "# from matplotlib import pyplot as plt\n",
        "# plt.imshow(abc,cmap=\"Greys\")\n",
        "# plt.show()\n",
        "# X7,X8,y7,y8=train_test_split(X5, \n",
        "#                                                     y5, \n",
        "#                                                     test_size=0.5, \n",
        "#                                                     random_state=42)\n",
        "# print(len(X7),len(y8))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMRKYwSfBXRu"
      },
      "source": [
        "# X1=X7\n",
        "# y1=y7\n",
        "# X3=X8\n",
        "# y3=y8\n",
        "# X5=X6\n",
        "# y5=y6\n",
        "\n",
        "# print(len(X1),len(X2),len(X3),len(X4),len(X5),len(X6),len(X7),len(X8),len(X9),len(X10))#,len(X11),len(X12),len(X13),len(X14),len(X15))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u05FbK5sB8Ne"
      },
      "source": [
        "# print(len(y1),len(y2),len(y3),len(y4))\n",
        "# print(X1.shape,y1.shape)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SR1I8sxCINA"
      },
      "source": [
        "# trainX=list()\n",
        "# trainy=list()\n",
        "# trainX.append(X1)\n",
        "# trainy.append(y1)\n",
        "# trainX.append(X2)\n",
        "# trainy.append(y2)\n",
        "# trainX.append(X3)\n",
        "# trainy.append(y3)\n",
        "# trainX.append(X4)\n",
        "# trainy.append(y4)\n",
        "# trainX.append(X5)\n",
        "# trainy.append(y5)\n",
        "# trainX.append(X6)\n",
        "# trainy.append(y6)\n",
        "# trainX.append(X7)\n",
        "# trainy.append(y7)\n",
        "# trainX.append(X8)\n",
        "# trainy.append(y8)\n",
        "# trainX.append(X9)\n",
        "# trainy.append(y9)\n",
        "# trainX.append(X10)\n",
        "# trainy.append(y10)\n",
        "\n",
        "# trainX.append(X11)\n",
        "# trainy.append(y11)\n",
        "# trainX.append(X12)\n",
        "# trainy.append(y12)\n",
        "# trainX.append(X13)\n",
        "# trainy.append(y13)\n",
        "# trainX.append(X14)\n",
        "# trainy.append(y14)\n",
        "# trainX.append(X15)\n",
        "# trainy.append(y15)\n",
        "# trainX.append(X5)\n",
        "# trainy.append(y5)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8JEyJXps0iX"
      },
      "source": [
        "noise_factor=0.95\n",
        "trainX[0] = trainX[0] + noise_factor * tf.random.normal(shape=trainX[0].shape)\n",
        "#trainX[1] = trainX[1] + 0.5 * tf.random.normal(shape=trainX[1].shape)\n",
        "# trainX[0] = trainX[0] + 0.15 * tf.random.normal(shape=trainX[0].shape)\n",
        "# trainX[1] = trainX[1] + 0.2 * tf.random.normal(shape=trainX[1].shape)\n",
        "# trainX[2] = trainX[2] + 0.15 * tf.random.normal(shape=trainX[2].shape)\n",
        "# trainX[3] = trainX[3] + 0.2 * tf.random.normal(shape=trainX[3].shape)\n",
        "# trainX[4] = trainX[4] + 0.2 * tf.random.normal(shape=trainX[4].shape)\n",
        "trainX[5] = trainX[5] + 0.85 * tf.random.normal(shape=trainX[5].shape)\n",
        "trainX[36] = trainX[36] + 0.9 * tf.random.normal(shape=trainX[36].shape)\n",
        "trainX[25] = trainX[25] + 0.8 * tf.random.normal(shape=trainX[25].shape)\n",
        "# trainX[6] = trainX[6] + 0.25 * tf.random.normal(shape=trainX[6].shape)\n",
        "# trainX[7] = trainX[7] + 0.2 * tf.random.normal(shape=trainX[7].shape)\n",
        "# trainX[8] = trainX[8] + 0.25 * tf.random.normal(shape=trainX[8].shape)\n",
        "# trainX[9] = trainX[9] + 0.2 * tf.random.normal(shape=trainX[9].shape)\n",
        "# trainX[10] = trainX[10] + 0.2 * tf.random.normal(shape=trainX[10].shape)\n",
        "# trainX[11] = trainX[11] + 0.25 * tf.random.normal(shape=trainX[11].shape)\n",
        "# trainX[8] = trainX[8] + 0.2 * tf.random.normal(shape=trainX[8].shape)\n",
        "# trainX[13] = trainX[13] + 0.25 * tf.random.normal(shape=trainX[13].shape)\n",
        "# trainX[14] = trainX[14] + 0.2 * tf.random.normal(shape=trainX[14].shape)\n",
        "\n",
        "# trainX[2] = trainX[2] + 0.7 * tf.random.normal(shape=trainX[2].shape)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqvga8B6CJxf",
        "outputId": "0e609cc2-09ca-4235-eb91-3ab35d30b898"
      },
      "source": [
        "trainX=np.array(trainX)\n",
        "trainy=np.array(trainy)\n",
        "print(trainX.shape,trainy.shape)\n",
        "print(trainy[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 1000, 28, 28, 1) (60, 1000, 1)\n",
            "[[7]\n",
            " [3]\n",
            " [8]\n",
            " [9]\n",
            " [3]\n",
            " [9]\n",
            " [7]\n",
            " [7]\n",
            " [5]\n",
            " [4]\n",
            " [2]\n",
            " [5]\n",
            " [6]\n",
            " [8]\n",
            " [1]\n",
            " [7]\n",
            " [4]\n",
            " [8]\n",
            " [0]\n",
            " [7]\n",
            " [9]\n",
            " [3]\n",
            " [1]\n",
            " [7]\n",
            " [6]\n",
            " [5]\n",
            " [3]\n",
            " [3]\n",
            " [3]\n",
            " [6]\n",
            " [0]\n",
            " [8]\n",
            " [4]\n",
            " [9]\n",
            " [5]\n",
            " [1]\n",
            " [8]\n",
            " [7]\n",
            " [8]\n",
            " [8]\n",
            " [0]\n",
            " [8]\n",
            " [2]\n",
            " [0]\n",
            " [8]\n",
            " [5]\n",
            " [5]\n",
            " [2]\n",
            " [2]\n",
            " [0]\n",
            " [1]\n",
            " [5]\n",
            " [9]\n",
            " [3]\n",
            " [0]\n",
            " [6]\n",
            " [2]\n",
            " [2]\n",
            " [6]\n",
            " [4]\n",
            " [0]\n",
            " [1]\n",
            " [8]\n",
            " [4]\n",
            " [0]\n",
            " [2]\n",
            " [4]\n",
            " [7]\n",
            " [7]\n",
            " [4]\n",
            " [4]\n",
            " [0]\n",
            " [1]\n",
            " [4]\n",
            " [8]\n",
            " [6]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [9]\n",
            " [6]\n",
            " [3]\n",
            " [6]\n",
            " [9]\n",
            " [9]\n",
            " [4]\n",
            " [8]\n",
            " [6]\n",
            " [6]\n",
            " [1]\n",
            " [9]\n",
            " [5]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [4]\n",
            " [9]\n",
            " [1]\n",
            " [0]\n",
            " [4]\n",
            " [2]\n",
            " [8]\n",
            " [5]\n",
            " [4]\n",
            " [4]\n",
            " [4]\n",
            " [4]\n",
            " [8]\n",
            " [2]\n",
            " [4]\n",
            " [5]\n",
            " [4]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [5]\n",
            " [4]\n",
            " [2]\n",
            " [1]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [8]\n",
            " [2]\n",
            " [7]\n",
            " [0]\n",
            " [4]\n",
            " [0]\n",
            " [6]\n",
            " [3]\n",
            " [5]\n",
            " [2]\n",
            " [2]\n",
            " [5]\n",
            " [1]\n",
            " [1]\n",
            " [4]\n",
            " [5]\n",
            " [2]\n",
            " [2]\n",
            " [9]\n",
            " [3]\n",
            " [6]\n",
            " [2]\n",
            " [9]\n",
            " [9]\n",
            " [5]\n",
            " [3]\n",
            " [4]\n",
            " [8]\n",
            " [7]\n",
            " [3]\n",
            " [9]\n",
            " [7]\n",
            " [9]\n",
            " [9]\n",
            " [0]\n",
            " [1]\n",
            " [8]\n",
            " [0]\n",
            " [5]\n",
            " [6]\n",
            " [0]\n",
            " [2]\n",
            " [8]\n",
            " [0]\n",
            " [6]\n",
            " [7]\n",
            " [1]\n",
            " [8]\n",
            " [3]\n",
            " [8]\n",
            " [2]\n",
            " [9]\n",
            " [4]\n",
            " [5]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [3]\n",
            " [1]\n",
            " [7]\n",
            " [8]\n",
            " [8]\n",
            " [1]\n",
            " [8]\n",
            " [8]\n",
            " [9]\n",
            " [9]\n",
            " [8]\n",
            " [8]\n",
            " [0]\n",
            " [1]\n",
            " [5]\n",
            " [7]\n",
            " [4]\n",
            " [3]\n",
            " [7]\n",
            " [4]\n",
            " [4]\n",
            " [4]\n",
            " [1]\n",
            " [9]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [5]\n",
            " [0]\n",
            " [4]\n",
            " [6]\n",
            " [5]\n",
            " [1]\n",
            " [6]\n",
            " [6]\n",
            " [8]\n",
            " [3]\n",
            " [2]\n",
            " [2]\n",
            " [5]\n",
            " [2]\n",
            " [8]\n",
            " [7]\n",
            " [6]\n",
            " [0]\n",
            " [2]\n",
            " [9]\n",
            " [9]\n",
            " [0]\n",
            " [5]\n",
            " [3]\n",
            " [4]\n",
            " [3]\n",
            " [3]\n",
            " [6]\n",
            " [7]\n",
            " [2]\n",
            " [7]\n",
            " [0]\n",
            " [5]\n",
            " [9]\n",
            " [6]\n",
            " [8]\n",
            " [0]\n",
            " [1]\n",
            " [5]\n",
            " [9]\n",
            " [0]\n",
            " [6]\n",
            " [9]\n",
            " [2]\n",
            " [7]\n",
            " [1]\n",
            " [3]\n",
            " [4]\n",
            " [7]\n",
            " [8]\n",
            " [8]\n",
            " [9]\n",
            " [9]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [6]\n",
            " [6]\n",
            " [1]\n",
            " [7]\n",
            " [9]\n",
            " [4]\n",
            " [8]\n",
            " [5]\n",
            " [1]\n",
            " [4]\n",
            " [7]\n",
            " [7]\n",
            " [0]\n",
            " [2]\n",
            " [2]\n",
            " [9]\n",
            " [5]\n",
            " [7]\n",
            " [1]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [4]\n",
            " [5]\n",
            " [5]\n",
            " [7]\n",
            " [4]\n",
            " [9]\n",
            " [6]\n",
            " [3]\n",
            " [8]\n",
            " [1]\n",
            " [7]\n",
            " [8]\n",
            " [5]\n",
            " [4]\n",
            " [6]\n",
            " [9]\n",
            " [2]\n",
            " [3]\n",
            " [0]\n",
            " [7]\n",
            " [7]\n",
            " [1]\n",
            " [4]\n",
            " [5]\n",
            " [3]\n",
            " [9]\n",
            " [0]\n",
            " [7]\n",
            " [1]\n",
            " [3]\n",
            " [4]\n",
            " [0]\n",
            " [7]\n",
            " [7]\n",
            " [9]\n",
            " [7]\n",
            " [8]\n",
            " [3]\n",
            " [4]\n",
            " [8]\n",
            " [4]\n",
            " [1]\n",
            " [3]\n",
            " [9]\n",
            " [0]\n",
            " [4]\n",
            " [0]\n",
            " [3]\n",
            " [5]\n",
            " [4]\n",
            " [5]\n",
            " [7]\n",
            " [0]\n",
            " [8]\n",
            " [0]\n",
            " [0]\n",
            " [2]\n",
            " [6]\n",
            " [6]\n",
            " [4]\n",
            " [7]\n",
            " [6]\n",
            " [1]\n",
            " [9]\n",
            " [6]\n",
            " [0]\n",
            " [5]\n",
            " [8]\n",
            " [7]\n",
            " [1]\n",
            " [3]\n",
            " [9]\n",
            " [1]\n",
            " [0]\n",
            " [5]\n",
            " [2]\n",
            " [0]\n",
            " [6]\n",
            " [1]\n",
            " [1]\n",
            " [8]\n",
            " [6]\n",
            " [0]\n",
            " [0]\n",
            " [6]\n",
            " [0]\n",
            " [4]\n",
            " [9]\n",
            " [9]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [2]\n",
            " [5]\n",
            " [3]\n",
            " [0]\n",
            " [9]\n",
            " [9]\n",
            " [4]\n",
            " [3]\n",
            " [7]\n",
            " [4]\n",
            " [5]\n",
            " [0]\n",
            " [1]\n",
            " [9]\n",
            " [1]\n",
            " [4]\n",
            " [6]\n",
            " [5]\n",
            " [2]\n",
            " [8]\n",
            " [1]\n",
            " [7]\n",
            " [7]\n",
            " [0]\n",
            " [6]\n",
            " [0]\n",
            " [6]\n",
            " [4]\n",
            " [7]\n",
            " [7]\n",
            " [3]\n",
            " [1]\n",
            " [9]\n",
            " [0]\n",
            " [6]\n",
            " [5]\n",
            " [4]\n",
            " [1]\n",
            " [8]\n",
            " [6]\n",
            " [5]\n",
            " [2]\n",
            " [5]\n",
            " [3]\n",
            " [1]\n",
            " [6]\n",
            " [6]\n",
            " [8]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [5]\n",
            " [3]\n",
            " [9]\n",
            " [9]\n",
            " [7]\n",
            " [1]\n",
            " [7]\n",
            " [7]\n",
            " [8]\n",
            " [3]\n",
            " [8]\n",
            " [8]\n",
            " [8]\n",
            " [0]\n",
            " [5]\n",
            " [1]\n",
            " [4]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [6]\n",
            " [8]\n",
            " [1]\n",
            " [6]\n",
            " [7]\n",
            " [7]\n",
            " [5]\n",
            " [2]\n",
            " [3]\n",
            " [0]\n",
            " [7]\n",
            " [9]\n",
            " [6]\n",
            " [8]\n",
            " [2]\n",
            " [8]\n",
            " [0]\n",
            " [6]\n",
            " [9]\n",
            " [9]\n",
            " [1]\n",
            " [5]\n",
            " [5]\n",
            " [9]\n",
            " [4]\n",
            " [4]\n",
            " [9]\n",
            " [0]\n",
            " [0]\n",
            " [5]\n",
            " [6]\n",
            " [8]\n",
            " [6]\n",
            " [0]\n",
            " [0]\n",
            " [3]\n",
            " [5]\n",
            " [0]\n",
            " [8]\n",
            " [9]\n",
            " [2]\n",
            " [2]\n",
            " [4]\n",
            " [5]\n",
            " [1]\n",
            " [6]\n",
            " [1]\n",
            " [2]\n",
            " [9]\n",
            " [4]\n",
            " [0]\n",
            " [9]\n",
            " [1]\n",
            " [6]\n",
            " [1]\n",
            " [0]\n",
            " [5]\n",
            " [9]\n",
            " [7]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [4]\n",
            " [7]\n",
            " [7]\n",
            " [2]\n",
            " [1]\n",
            " [1]\n",
            " [7]\n",
            " [0]\n",
            " [2]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [0]\n",
            " [8]\n",
            " [7]\n",
            " [8]\n",
            " [9]\n",
            " [4]\n",
            " [2]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [3]\n",
            " [9]\n",
            " [8]\n",
            " [4]\n",
            " [3]\n",
            " [2]\n",
            " [9]\n",
            " [7]\n",
            " [7]\n",
            " [4]\n",
            " [3]\n",
            " [7]\n",
            " [6]\n",
            " [7]\n",
            " [1]\n",
            " [6]\n",
            " [4]\n",
            " [7]\n",
            " [4]\n",
            " [5]\n",
            " [2]\n",
            " [9]\n",
            " [9]\n",
            " [8]\n",
            " [3]\n",
            " [5]\n",
            " [8]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [9]\n",
            " [3]\n",
            " [4]\n",
            " [7]\n",
            " [4]\n",
            " [7]\n",
            " [4]\n",
            " [0]\n",
            " [5]\n",
            " [1]\n",
            " [4]\n",
            " [4]\n",
            " [7]\n",
            " [3]\n",
            " [3]\n",
            " [4]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [9]\n",
            " [2]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [4]\n",
            " [5]\n",
            " [8]\n",
            " [0]\n",
            " [2]\n",
            " [9]\n",
            " [8]\n",
            " [5]\n",
            " [3]\n",
            " [9]\n",
            " [4]\n",
            " [5]\n",
            " [1]\n",
            " [3]\n",
            " [9]\n",
            " [1]\n",
            " [2]\n",
            " [8]\n",
            " [8]\n",
            " [1]\n",
            " [6]\n",
            " [2]\n",
            " [8]\n",
            " [0]\n",
            " [8]\n",
            " [0]\n",
            " [7]\n",
            " [7]\n",
            " [6]\n",
            " [2]\n",
            " [9]\n",
            " [5]\n",
            " [7]\n",
            " [3]\n",
            " [6]\n",
            " [7]\n",
            " [7]\n",
            " [2]\n",
            " [7]\n",
            " [0]\n",
            " [4]\n",
            " [2]\n",
            " [4]\n",
            " [9]\n",
            " [3]\n",
            " [2]\n",
            " [7]\n",
            " [6]\n",
            " [2]\n",
            " [6]\n",
            " [8]\n",
            " [4]\n",
            " [4]\n",
            " [6]\n",
            " [0]\n",
            " [0]\n",
            " [7]\n",
            " [5]\n",
            " [8]\n",
            " [4]\n",
            " [6]\n",
            " [0]\n",
            " [6]\n",
            " [7]\n",
            " [7]\n",
            " [5]\n",
            " [4]\n",
            " [9]\n",
            " [7]\n",
            " [6]\n",
            " [8]\n",
            " [0]\n",
            " [8]\n",
            " [4]\n",
            " [5]\n",
            " [3]\n",
            " [0]\n",
            " [9]\n",
            " [0]\n",
            " [0]\n",
            " [8]\n",
            " [4]\n",
            " [0]\n",
            " [5]\n",
            " [6]\n",
            " [9]\n",
            " [7]\n",
            " [6]\n",
            " [1]\n",
            " [2]\n",
            " [6]\n",
            " [8]\n",
            " [7]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [7]\n",
            " [6]\n",
            " [5]\n",
            " [8]\n",
            " [8]\n",
            " [1]\n",
            " [5]\n",
            " [4]\n",
            " [1]\n",
            " [1]\n",
            " [8]\n",
            " [4]\n",
            " [2]\n",
            " [5]\n",
            " [5]\n",
            " [7]\n",
            " [7]\n",
            " [4]\n",
            " [6]\n",
            " [7]\n",
            " [9]\n",
            " [9]\n",
            " [4]\n",
            " [2]\n",
            " [8]\n",
            " [9]\n",
            " [5]\n",
            " [8]\n",
            " [2]\n",
            " [8]\n",
            " [6]\n",
            " [4]\n",
            " [2]\n",
            " [1]\n",
            " [0]\n",
            " [9]\n",
            " [7]\n",
            " [9]\n",
            " [7]\n",
            " [6]\n",
            " [0]\n",
            " [8]\n",
            " [6]\n",
            " [8]\n",
            " [5]\n",
            " [5]\n",
            " [8]\n",
            " [2]\n",
            " [6]\n",
            " [3]\n",
            " [2]\n",
            " [9]\n",
            " [2]\n",
            " [3]\n",
            " [0]\n",
            " [3]\n",
            " [2]\n",
            " [4]\n",
            " [9]\n",
            " [1]\n",
            " [5]\n",
            " [7]\n",
            " [8]\n",
            " [1]\n",
            " [8]\n",
            " [4]\n",
            " [9]\n",
            " [1]\n",
            " [3]\n",
            " [1]\n",
            " [0]\n",
            " [5]\n",
            " [5]\n",
            " [8]\n",
            " [0]\n",
            " [4]\n",
            " [7]\n",
            " [9]\n",
            " [4]\n",
            " [2]\n",
            " [0]\n",
            " [5]\n",
            " [5]\n",
            " [0]\n",
            " [1]\n",
            " [6]\n",
            " [7]\n",
            " [3]\n",
            " [3]\n",
            " [6]\n",
            " [4]\n",
            " [1]\n",
            " [8]\n",
            " [1]\n",
            " [7]\n",
            " [2]\n",
            " [0]\n",
            " [8]\n",
            " [9]\n",
            " [0]\n",
            " [3]\n",
            " [1]\n",
            " [7]\n",
            " [9]\n",
            " [0]\n",
            " [6]\n",
            " [5]\n",
            " [2]\n",
            " [2]\n",
            " [8]\n",
            " [7]\n",
            " [6]\n",
            " [1]\n",
            " [3]\n",
            " [0]\n",
            " [0]\n",
            " [9]\n",
            " [7]\n",
            " [1]\n",
            " [9]\n",
            " [5]\n",
            " [0]\n",
            " [9]\n",
            " [0]\n",
            " [3]\n",
            " [2]\n",
            " [9]\n",
            " [8]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [8]\n",
            " [3]\n",
            " [2]\n",
            " [2]\n",
            " [0]\n",
            " [0]\n",
            " [6]\n",
            " [6]\n",
            " [9]\n",
            " [7]\n",
            " [1]\n",
            " [7]\n",
            " [1]\n",
            " [5]\n",
            " [6]\n",
            " [8]\n",
            " [4]\n",
            " [8]\n",
            " [7]\n",
            " [7]\n",
            " [8]\n",
            " [3]\n",
            " [9]\n",
            " [3]\n",
            " [6]\n",
            " [0]\n",
            " [0]\n",
            " [7]\n",
            " [2]\n",
            " [3]\n",
            " [3]\n",
            " [3]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [9]\n",
            " [5]\n",
            " [9]\n",
            " [8]\n",
            " [0]\n",
            " [4]\n",
            " [8]\n",
            " [6]\n",
            " [9]\n",
            " [4]\n",
            " [7]\n",
            " [2]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [3]\n",
            " [6]\n",
            " [0]\n",
            " [9]\n",
            " [5]\n",
            " [8]\n",
            " [9]\n",
            " [8]\n",
            " [2]\n",
            " [7]\n",
            " [2]\n",
            " [7]\n",
            " [8]\n",
            " [7]\n",
            " [0]\n",
            " [3]\n",
            " [3]\n",
            " [6]\n",
            " [9]\n",
            " [5]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [1]\n",
            " [5]\n",
            " [1]\n",
            " [9]\n",
            " [3]\n",
            " [6]\n",
            " [2]\n",
            " [8]\n",
            " [7]\n",
            " [5]\n",
            " [8]\n",
            " [2]\n",
            " [8]\n",
            " [8]\n",
            " [8]\n",
            " [7]\n",
            " [6]\n",
            " [8]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [3]\n",
            " [1]\n",
            " [8]\n",
            " [6]\n",
            " [8]\n",
            " [8]\n",
            " [0]\n",
            " [0]\n",
            " [3]\n",
            " [8]\n",
            " [4]\n",
            " [5]\n",
            " [0]\n",
            " [2]\n",
            " [1]\n",
            " [9]\n",
            " [2]\n",
            " [8]\n",
            " [5]\n",
            " [1]\n",
            " [2]\n",
            " [7]\n",
            " [6]\n",
            " [7]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [4]\n",
            " [1]\n",
            " [3]\n",
            " [7]\n",
            " [8]\n",
            " [2]\n",
            " [6]\n",
            " [5]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [2]\n",
            " [8]\n",
            " [5]\n",
            " [5]\n",
            " [4]\n",
            " [8]\n",
            " [5]\n",
            " [5]\n",
            " [4]\n",
            " [1]\n",
            " [0]\n",
            " [7]\n",
            " [6]\n",
            " [2]\n",
            " [3]\n",
            " [1]\n",
            " [1]\n",
            " [8]\n",
            " [5]\n",
            " [9]\n",
            " [3]\n",
            " [5]\n",
            " [3]\n",
            " [0]\n",
            " [2]\n",
            " [7]\n",
            " [6]\n",
            " [8]\n",
            " [4]\n",
            " [9]\n",
            " [3]\n",
            " [6]\n",
            " [3]\n",
            " [7]\n",
            " [0]\n",
            " [4]\n",
            " [9]\n",
            " [7]\n",
            " [1]\n",
            " [1]\n",
            " [8]\n",
            " [1]\n",
            " [2]\n",
            " [7]\n",
            " [5]\n",
            " [3]\n",
            " [5]\n",
            " [6]\n",
            " [1]\n",
            " [6]\n",
            " [8]\n",
            " [2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeJEvl4RszIh"
      },
      "source": [
        "samplecdf=list()\n",
        "samplecdf1=[]\n",
        "dicttrain = {}\n",
        "dicttest={}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaSZFbVvDEuc"
      },
      "source": [
        "class MODEL:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32,(3,3),input_shape=inputShape) )\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64,(3,3)) )\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dropout(0.4))\n",
        "        no_classes=10\n",
        "        model.add(Dense(10))#for output layer\n",
        "        model.add(Activation(\"softmax\"))#sigmoid\n",
        "        # model = Sequential()\n",
        "        # model.add(Dense(200, input_shape=inputShape))\n",
        "        # model.add(Activation(\"relu\"))\n",
        "        # model.add(Dense(200))\n",
        "        # model.add(Activation(\"relu\"))\n",
        "        # model.add(Dense(1))\n",
        "        # model.add(Activation(\"softmax\"))\n",
        "        return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DhadKvkzLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feea89d4-6bac-4860-9b11-c6ffbbbed169"
      },
      "source": [
        "import numpy as np\n",
        "from sympy import symbols, Eq, solve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "import pandas as pd\n",
        "from scipy.stats import ks_2samp\n",
        "def test_modelt(X_test, Y_test,  model, comm_round,i):\n",
        "    if(i==1):\n",
        "      cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    elif(i==2):\n",
        "      cce = tf.keras.losses.BinaryCrossentropy()\n",
        "    elif(i==3):\n",
        "      cce=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      #cce = tf.keras.losses.KLDivergence()\n",
        "    elif(i==4):\n",
        "      cce = tf.keras.losses.Poisson()\n",
        "    elif(i==5):\n",
        "      cce = tf.keras.losses.MeanSquaredError()\n",
        "    elif(i==6):\n",
        "      cce = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "    elif(i==7):\n",
        "      cce = tf.keras.losses.CosineSimilarity()\n",
        "    elif(i==8):\n",
        "      cce = tf.keras.losses.Huber()\n",
        "    elif(i==9):\n",
        "      cce = tf.keras.losses.LogCosh()\n",
        "    elif(i==10):\n",
        "      cce = tf.keras.losses.Hinge()\n",
        "    elif(i==11):\n",
        "      cce = tf.keras.losses.SquaredHinge()\n",
        "    elif(i==12):\n",
        "      cce = focal\n",
        "    elif(i==13):\n",
        "      cce = expo\n",
        "    elif(i==14):\n",
        "      cce = sml\n",
        "      \n",
        "      #focal\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    lossindiv=list()\n",
        "    print(Y_test.shape,logits.shape)\n",
        "    loss = cce(Y_test, logits)\n",
        "\n",
        "    for ind in range(len(logits)):\n",
        "      loss1=cce(Y_test[ind],logits[ind])\n",
        "      lossindiv.append(loss1)\n",
        "    print(\"lossindiv\")\n",
        "    #samplecdf.append(lossindiv)\n",
        "    abc=np.array(lossindiv)\n",
        "    #print(abc)\n",
        "    print(np.mean(abc))\n",
        "    #samplecdf1.append(np.mean(abc))\n",
        "    print(\"logits\")\n",
        "    # print(logits)\n",
        "    print(\"size\",logits.shape)\n",
        "    print(\"loss\")\n",
        "    #     proto_tensor = tf.make_tensor_proto(loss)  # convert `tensor a` to a proto tensor\n",
        "    # lossarray=tf.make_ndarray(proto_tensor)\n",
        "\n",
        "    # print(loss)\n",
        "    # print(\"size\",loss.shape)\n",
        "\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    # Y_test = np_utils.to_categorical(Y_test)\n",
        "    # acc1 = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    # print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc1, loss))\n",
        "\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # loss,acc=model.evaluate(x_test,y_test)\n",
        "    return acc, loss\n",
        "def test_model(X_test, Y_test,  model, comm_round,i):\n",
        "    if(i==1):\n",
        "      cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    elif(i==2):\n",
        "      cce = tf.keras.losses.BinaryCrossentropy()\n",
        "    elif(i==3):\n",
        "      cce=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      #cce = tf.keras.losses.KLDivergence()\n",
        "    elif(i==4):\n",
        "      cce = tf.keras.losses.Poisson()\n",
        "    elif(i==5):\n",
        "      cce = tf.keras.losses.MeanSquaredError()\n",
        "    elif(i==6):\n",
        "      cce = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "    elif(i==7):\n",
        "      cce = tf.keras.losses.CosineSimilarity()\n",
        "    elif(i==8):\n",
        "      cce = tf.keras.losses.Huber()\n",
        "    elif(i==9):\n",
        "      cce = tf.keras.losses.LogCosh()\n",
        "    elif(i==10):\n",
        "      cce = tf.keras.losses.Hinge()\n",
        "    elif(i==11):\n",
        "      cce = tf.keras.losses.SquaredHinge()\n",
        "    elif(i==12):\n",
        "      cce = focal\n",
        "    elif(i==13):\n",
        "      cce = expo\n",
        "    elif(i==14):\n",
        "      cce = sml\n",
        "      \n",
        "      #focal\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    lossindiv=list()\n",
        "    print(Y_test.shape,logits.shape)\n",
        "    loss = cce(Y_test, logits)\n",
        "\n",
        "    for ind in range(len(logits)):\n",
        "      loss1=cce(Y_test[ind],logits[ind])\n",
        "      lossindiv.append(loss1)\n",
        "    print(\"lossindiv\")\n",
        "    samplecdf.append(lossindiv)\n",
        "    abc=np.array(lossindiv)\n",
        "    #print(abc)\n",
        "    print(np.mean(abc))\n",
        "    samplecdf1.append(np.mean(abc))\n",
        "    print(\"logits\")\n",
        "    # print(logits)\n",
        "    print(\"size\",logits.shape)\n",
        "    print(\"loss\")\n",
        "    #     proto_tensor = tf.make_tensor_proto(loss)  # convert `tensor a` to a proto tensor\n",
        "    # lossarray=tf.make_ndarray(proto_tensor)\n",
        "\n",
        "    # print(loss)\n",
        "    # print(\"size\",loss.shape)\n",
        "\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    # Y_test = np_utils.to_categorical(Y_test)\n",
        "    # acc1 = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    # print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc1, loss))\n",
        "\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # loss,acc=model.evaluate(x_test,y_test)\n",
        "    return acc, loss\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    print(type(weight[0]))\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "def calculate_model_weightage(loss_list):\n",
        "    '''function for scaling a models weights'''\n",
        "    \n",
        "    weightage = []\n",
        "    steps = len(loss_list)\n",
        "    sum=0\n",
        "    for i in range(steps-1):\n",
        "      sum=sum+loss_list[i]\n",
        "    new_avg=sum/3\n",
        "    xx=(loss_list[2]/loss_list[0])+(loss_list[2]/loss_list[1])+(loss_list[2]/loss_list[2])\n",
        "    xx1=(steps-1)*(loss_list[2])\n",
        "    yy=1\n",
        "    yy1=loss_list[3]\n",
        "    zz=1\n",
        "    zz1=new_avg\n",
        "    print(\"xx,yy,zz\",xx,yy,zz)\n",
        "    print(\"xx1,yy1,zz1\",xx1,yy1,zz1)\n",
        "\n",
        "    x, y = symbols('x y')\n",
        "    eq1 = Eq(xx*x + yy*y - zz,0)\n",
        "    eq2 = Eq(xx1*x + yy1*y -zz1,0)\n",
        "    sol_dict = solve((eq1,eq2), (x, y))\n",
        "    print(f'x = {sol_dict[x]}')\n",
        "    print(f'y = {sol_dict[y]}')\n",
        "    a=sol_dict[x]\n",
        "    a=float(a)\n",
        "    b=sol_dict[y]\n",
        "    b=float(b)\n",
        "    weightage.append((loss_list[2]/loss_list[0])*a)\n",
        "    weightage.append((loss_list[2]/loss_list[1])*a)\n",
        "    weightage.append(a)\n",
        "    weightage.append(b)\n",
        "    # weightage.append(0.25)\n",
        "    # weightage.append(0.25)\n",
        "    # weightage.append(0.25)\n",
        "    # weightage.append(0.25)\n",
        "    return weightage\n",
        "def calculate_model_weightage(clientsstatus,clienttrainingloss):\n",
        "  goodsample=-1\n",
        "  goodsamplevalue=1000\n",
        "  badsample=-1\n",
        "  badsamplevalue=1000#inplace of 1000 use infinty\n",
        "  n = len(clienttrainingloss)\n",
        "  clienttrainingloss=np.array(clienttrainingloss)\n",
        "\n",
        "  for clientid in range(0,n):\n",
        "    if(clienttrainingloss[clientid]<goodsamplevalue and clientsstatus[clientid]=='g'):\n",
        "      goodsample=clientid\n",
        "      goodsamplevalue=clienttrainingloss[clientid]\n",
        "    elif(float(clienttrainingloss[clientid])<badsamplevalue and clientsstatus[clientid]=='b'):\n",
        "      badsample=clientid\n",
        "      badsamplevalue=clienttrainingloss[clientid]\n",
        "  if(badsample!=-1):\n",
        "    sumfornewavg=0\n",
        "    countfornewavg=0\n",
        "    paramdx1=[]\n",
        "    paramdy1=[]\n",
        "    paramdx2=[]\n",
        "    paramdy2=[]\n",
        "    for clientid in range(len(clienttrainingloss)):\n",
        "      if(clientsstatus[clientid]=='g'):\n",
        "        sumfornewavg=sumfornewavg+clienttrainingloss[clientid]\n",
        "        countfornewavg=countfornewavg+1\n",
        "        paramdx1.append(clienttrainingloss[goodsample]/clienttrainingloss[clientid])\n",
        "      elif(clientsstatus[clientid]=='b'):\n",
        "        paramdy1.append(clienttrainingloss[badsample]/clienttrainingloss[clientid])\n",
        "    newavg=sumfornewavg/countfornewavg\n",
        "    paramx1=sum(paramdx1)\n",
        "    paramy1=sum(paramdy1)\n",
        "    ind1=0\n",
        "    ind2=0\n",
        "    for clientid in range(len(clienttrainingloss)):\n",
        "      if(clientsstatus[clientid]=='g'):\n",
        "        paramdx2.append(paramdx1[ind1]*clienttrainingloss[clientid])\n",
        "        ind1=ind1+1\n",
        "      elif(clientsstatus[clientid]=='b'):\n",
        "        paramdy2.append(paramdy1[ind2]*clienttrainingloss[clientid])\n",
        "        ind2=ind2+1\n",
        "    paramx2=sum(paramdx2)\n",
        "    paramy2=sum(paramdy2)\n",
        "    totalpercent=1.0\n",
        "    a, b = symbols('x y')\n",
        "    eq1 = Eq(paramx1*a + paramy1*b -totalpercent ,0)\n",
        "    eq2 = Eq(paramx2*a + paramy2*b -newavg ,0)\n",
        "    sol_dict = solve((eq1,eq2), (a, b))\n",
        "    print(f'x = {sol_dict[a]}')\n",
        "    print(f'y = {sol_dict[b]}')\n",
        "    x=sol_dict[a]\n",
        "    x=float(x)\n",
        "    y=sol_dict[b]\n",
        "    y=float(y)\n",
        "    weightage=[]\n",
        "    ind1=0\n",
        "    ind2=0\n",
        "    for clientid in range(len(clienttrainingloss)):\n",
        "      if(clientsstatus[clientid]=='g'):\n",
        "        weightage.append(paramdx1[ind1]*x)\n",
        "        ind1=ind1+1\n",
        "      elif(clientsstatus[clientid]=='b'):\n",
        "        weightage.append(y*paramdy1[ind2])\n",
        "        ind2=ind2+1\n",
        "    print(weightage)\n",
        "    return weightage\n",
        "  else:\n",
        "    weightage=[]\n",
        "    n=len(clienttrainingloss)\n",
        "    for nsamp in range(n):\n",
        "      weightage.append(1/n)\n",
        "    return weightage\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''function for scaling a models weights'''\n",
        "    \n",
        "    \n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "      \n",
        "    return avg_grad\n",
        "def lambdas(ecdfbest, ecdfworst):\n",
        "  ind=0.001\n",
        "  #ecdf2 = ECDF(samplecdf[13])\n",
        "  #pyplot.plot(ecdf2.x,ecdf2.y)\n",
        "  lambdastar=1000\n",
        "  distmax=1000#infinity\n",
        "  #abc1=ecdfbest.copy()\n",
        "  #ecdfbest=np.array(ecdfbest)\n",
        "  #ecdfworst=np.array(ecdfworst)\n",
        "  ecdf2 = ECDF(ecdfbest)\n",
        "  #abc1.sort()\n",
        "  F1=ecdf2(ecdfbest)\n",
        "  for ind1 in range(10000):\n",
        "    #abc2=ecdfworst.copy()\n",
        "    #print(abc2[13])\n",
        "    #abc2.pop(9)\n",
        "    ind=ind+0.0001\n",
        "    ecdf4 = ECDF(ecdfworst)\n",
        "    #abc2.sort()\n",
        "    F2=ecdf4(ecdfworst)/ecdf4(ind)\n",
        "    for indd in range(len(F2)):\n",
        "      if F2[indd]>1:\n",
        "        F2[indd]=1\n",
        "    ksdist,ksvalue = ks_2samp(F1,F2)\n",
        "    print(\"i\",ind,\"\\t\",ksdist)\n",
        "    #ksdist=float(ksdist)\n",
        "    if(ksdist<=distmax):\n",
        "      lambdastar=ind\n",
        "      distmax=ksdist\n",
        "    elif(ksdist>distmax):\n",
        "      return lambdastar\n",
        "  return lambdastar\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQMGvLN7DiLY"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXKPs2nih4e2",
        "outputId": "1d9863a2-4147-4c9d-f394-7fdb668c21c3"
      },
      "source": [
        "lr = 0.001 \n",
        "comms_round = 50\n",
        "#loss='categorical_crossentropy'\n",
        "#metrics = ['accuracy']\n",
        "adam = Adam(lr=0.001)\n",
        "print(trainy[2].shape)\n",
        "from tensorflow.keras import backend as K\n",
        "losses=['categorical_crossentropy','binary_crossentropy','sparse_categorical_crossentropy']#,'kl_divergence','mean_squared_error','mean_squared_logarithmic_error','cosine_similarity','huber_loss','log_cosh','hinge','squared_hinge',focal,expo,sml]\n",
        "i=2\n",
        "#file1 = open(\"myfileproject7.txt\",'a+')#append mode \n",
        "for loss_fn in losses[2:3]:\n",
        "  global1= MODEL()\n",
        "  global_model = global1.build()\n",
        "  loss_list=list()\n",
        "  i=i+1\n",
        "  if(i==10 or i==11):\n",
        "    for comm_round in range(101):\n",
        "      global_weights = global_model.get_weights()\n",
        "      scaled_local_weight_list = list()\n",
        "      index=list({1,2,3,0})\n",
        "      #random.shuffle(index)\n",
        "      print(\"hello\",index)\n",
        "      for ind in range(index):\n",
        "        local = MODEL()\n",
        "        local_model=local.build()\n",
        "        \n",
        "        local_model.compile(loss=loss_fn, optimizer=adam, metrics=['accuracy'])\n",
        "        local_model.set_weights(global_weights)\n",
        "        #trainy[ind]=np.array(trainy[ind]).reshape(-1,1)\n",
        "        history=local_model.fit(mtrainX[ind],mtrainy[ind], epochs=1)#,validation_data=(x_test,y_test))\n",
        "        print(\"Accuracy: \",history.history[\"accuracy\"][0])\n",
        "        if(comm_round==1):\n",
        "          loss_list.append(history.history[\"loss\"][0])\n",
        "        scaling_factor=1.0/15 #1/no.ofclients\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        K.clear_session()\n",
        "      average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "      #global_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "      global_model.set_weights(average_weights)\n",
        "      #val_acc,val_loss=global_model.evaluate(x_train,y_train)\n",
        "      #print(val_loss,val_acc)\n",
        "      global_acc, global_loss = test_model(mx_train,my_train, global_model, comm_round,i)\n",
        "      print(global_loss, global_acc)\n",
        "      tglobal_acc, tglobal_loss = test_model(mtest1,mtesty, global_model, comm_round,i)\n",
        "      print(global_loss, global_acc)\n",
        "      file1 = open(\"myfileproject9.txt\",'a+')\n",
        "      file1.write(\"%s \\t\" %loss_fn)\n",
        "      file1.write(\"%f \\t\" %global_loss)\n",
        "      file1.write(\"%f \\t\" %global_acc)\n",
        "      # file1.write(\"%f \\t\" %history.history[\"val_loss\"][49])\n",
        "      # file1.write(\"%f \\t\" %history.history[\"val_accuracy\"][49])\n",
        "      file1.write(\"%f \\t\" %tglobal_loss)\n",
        "      file1.write(\"%f \\n\" %tglobal_acc)\n",
        "      file1.close()\n",
        "  else:\n",
        "    for comm_round in range(2):\n",
        "      global_weights = global_model.get_weights()\n",
        "      scaled_local_weight_list = list()\n",
        "      index=(0,1,2,3,4,5,6,7,8,9)#,10,11,12,13,14)\n",
        "      #random.shuffle(index)\n",
        "      print(\"hello\",index)\n",
        "      wtmatrix=list()\n",
        "      for ind in range(0,60):\n",
        "        local = MODEL()\n",
        "        local_model=local.build()\n",
        "        local_model.compile(loss=loss_fn, optimizer=adam, metrics=['accuracy'])\n",
        "        local_model.set_weights(global_weights)\n",
        "        # trainy[ind]=np.array(trainy[ind]).reshape(-1,1)\n",
        "        history=local_model.fit(trainX[ind],trainy[ind], epochs=30)#,validation_data=(x_test,y_test))\n",
        "        print(\"Accuracy: \",history.history[\"accuracy\"][29])\n",
        "        tglobal_acc1, tglobal_loss1 = test_model(trainX[ind],trainy[ind], local_model, comm_round,i)\n",
        "        dicttrain[ind]=tglobal_loss1\n",
        "        tglobal_acc2, tglobal_loss2 = test_model(x_test[:500],y_test[:500], local_model, comm_round,i)\n",
        "        dicttest[ind]=tglobal_loss2\n",
        "        print(tglobal_loss1, tglobal_acc1)\n",
        "        print(tglobal_loss2, tglobal_acc2)\n",
        "        \n",
        "        \n",
        "        # if(comm_round==0):\n",
        "        #   scaling_factor=(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1)#1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15,1.0/15)#(0.25,0.25,0.25,0.25) #1/no.ofclients\n",
        "        #   scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor[ind])\n",
        "        #   scaled_local_weight_list.append(scaled_weights)\n",
        "        # K.clear_session()\n",
        "\n",
        "          # marklist = sorted(dicttrain.items(), key=lambda x:x[1])\n",
        "          # dicttrain = dict(marklist)\n",
        "          # marklist = sorted(dicttest.items(), key=lambda x:x[1])\n",
        "          # dicttest = dict(marklist)\n",
        "          # res = list(dicttest.keys())[0]\n",
        "          # bestclientid=res\n",
        "          # res = list(dicttrain.keys())[-1]\n",
        "          # worstclientid=res\n",
        "          # starlambda=lambdas(samplecdf[2*bestclientid+1],samplecdf[2*worstclientid])\n",
        "          # cleintsstatus={}\n",
        "          # for clientid,value in dicttrain: #loop for running through training client ids to divide into goood and bad clients\n",
        "          #   if(value<starlambda):\n",
        "          #     cleintsstatus[clientid]='g'\n",
        "          #   else:\n",
        "          #     cleintsstatus[clientid]='b'\n",
        "          # clienttrainingloss=[]\n",
        "          # goodclients=[]\n",
        "          # badclients=[]\n",
        "          # for ind1 in range(len(samplecdf)-2):\n",
        "          #   cleinttrainingloss.append(samplecdf[ind1])\n",
        "          #   #if(clientsstatus[i]=='g')goodclients.append()\n",
        "          #   ind1=ind1+1\n",
        "          \n",
        "\n",
        "          \n",
        "\n",
        "          \n",
        "\n",
        "        if(comm_round==0):\n",
        "          #loss_list.append(history.history[\"loss\"][0])\n",
        "          wtmatrix.append(local_model.get_weights())\n",
        "        else:\n",
        "          scaled_weights = scale_model_weights(local_model.get_weights(), float(weightage[ind]))\n",
        "          scaled_local_weight_list.append(scaled_weights)\n",
        "          # K.clear_session()\n",
        "      if(comm_round==0):\n",
        "        # print(loss_list)\n",
        "      #   weightage=calculate_model_weightage(loss_list)\n",
        "      #   print(type(weightage[0]))\n",
        "      #   print(weightage)\n",
        "      #   #print(wtmatrix)\n",
        "      #   for rn in range(len(wtmatrix)):\n",
        "      #     scaled_weights = scale_model_weights(wtmatrix[rn], weightage[rn])\n",
        "      #     scaled_local_weight_list.append(scaled_weights)\n",
        "      #     K.clear_session()\n",
        "        marklist = sorted(dicttrain.items(), key=lambda x:x[1])\n",
        "        dicttrain = dict(marklist)\n",
        "        marklist = sorted(dicttest.items(), key=lambda x:x[1])\n",
        "        dicttest = dict(marklist)\n",
        "        res = list(dicttest.keys())[0]\n",
        "        bestclientid=res\n",
        "        res = list(dicttrain.keys())[-1]\n",
        "        worstclientid=res\n",
        "        print(\"bestclientid\",bestclientid)\n",
        "        print(\"worstclientid\",worstclientid)\n",
        "        trainalllossclients=samplecdf.copy()\n",
        "        trainalllossclients=np.array(trainalllossclients)\n",
        "        trainagglossclients=samplecdf1.copy()\n",
        "        trainagglossclients=np.array(trainagglossclients)\n",
        "        print(trainagglossclients)\n",
        "        besttestclient=2*bestclientid+1\n",
        "        worsttrainclient=2*worstclientid\n",
        "        clienttrainingloss=[]\n",
        "        for ind1 in range(0,len(trainagglossclients),2):\n",
        "          clienttrainingloss.append(trainagglossclients[ind1])\n",
        "        starlambda=lambdas(trainalllossclients[besttestclient],clienttrainingloss)\n",
        "        print(\"starlambda\",starlambda)\n",
        "        clientsstatus={}\n",
        "        for clientid,value in dicttrain.items(): #loop for running through training client ids to divide into goood and bad clients\n",
        "          if(value<starlambda):\n",
        "            clientsstatus[clientid]='g'\n",
        "          else:\n",
        "            clientsstatus[clientid]='b'\n",
        "\n",
        "        goodclients=[]\n",
        "        badclients=[]\n",
        "\n",
        "          #if(clientsstatus[i]=='g')goodclients.append()\n",
        "          #ind1=ind1+2\n",
        "        print(\"clienttrainingloss\",clienttrainingloss)\n",
        "        weightage=calculate_model_weightage(clientsstatus,clienttrainingloss)\n",
        "        print(\"weightage of clients\",weightage)\n",
        "        for rn in range(len(wtmatrix)):\n",
        "          scaled_weights = scale_model_weights(wtmatrix[rn], weightage[rn])\n",
        "          scaled_local_weight_list.append(scaled_weights)\n",
        "          # K.clear_session()        \n",
        "      average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "      #global_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "      global_model.set_weights(average_weights)\n",
        "      #val_acc,val_loss=global_model.evaluate(x_train,y_train)\n",
        "      #print(val_loss,val_acc)\n",
        "      global_acc, global_loss = test_modelt(x_train,y_train, global_model, comm_round,i)\n",
        "      print(loss_fn,global_loss, global_acc)\n",
        "      tglobal_acc, tglobal_loss = test_modelt(x_test,y_test, global_model, comm_round,i)\n",
        "      print(tglobal_loss, tglobal_acc)\n",
        "      file1 = open(\"myfileproject9.txt\",'a+')\n",
        "      file1.write(\"%s \\t\" %loss_fn)\n",
        "      file1.write(\"%f \\t\" %global_loss)\n",
        "      file1.write(\"%f \\t\" %global_acc)\n",
        "      # file1.write(\"%f \\t\" %history.history[\"val_loss\"][49])\n",
        "      # file1.write(\"%f \\t\" %history.history[\"val_accuracy\"][49])\n",
        "      file1.write(\"%f \\t\" %tglobal_loss)\n",
        "      file1.write(\"%f \\n\" %tglobal_acc)\n",
        "      file1.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # model.fit(x_train, y_train, epochs=1, batch_size=32,validation_data=(x_test, y_test))\n",
        "    # a=model.get_weights()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1000, 1)\n",
            "hello (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 2.3328 - accuracy: 0.1240\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2656 - accuracy: 0.1620\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.1169 - accuracy: 0.2640\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.8096 - accuracy: 0.3880\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5736 - accuracy: 0.4640\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.4314 - accuracy: 0.5140\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3208 - accuracy: 0.5530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2163 - accuracy: 0.5950\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1923 - accuracy: 0.5970\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.1169 - accuracy: 0.6260\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.0215 - accuracy: 0.6460\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.0044 - accuracy: 0.6610\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.9638 - accuracy: 0.6800\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.9212 - accuracy: 0.6840\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8354 - accuracy: 0.7190\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8160 - accuracy: 0.7280\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8103 - accuracy: 0.7060\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7681 - accuracy: 0.7260\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7456 - accuracy: 0.7380\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7127 - accuracy: 0.7520\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7285 - accuracy: 0.7440\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6822 - accuracy: 0.7690\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6228 - accuracy: 0.7750\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6419 - accuracy: 0.7640\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6254 - accuracy: 0.7780\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.5715 - accuracy: 0.8160\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5567 - accuracy: 0.8090\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.5343 - accuracy: 0.8170\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5733 - accuracy: 0.8010\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4776 - accuracy: 0.8370\n",
            "Accuracy:  0.8370000123977661\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.23004499\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.400% | global_loss: 0.23004502058029175\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.5424042\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.600% | global_loss: 0.5424042344093323\n",
            "tf.Tensor(0.23004502, shape=(), dtype=float32) 0.114\n",
            "tf.Tensor(0.54240423, shape=(), dtype=float32) 0.106\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 1.5068 - accuracy: 0.4650\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.5223 - accuracy: 0.8370\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3227 - accuracy: 0.9010\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2413 - accuracy: 0.9290\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2252 - accuracy: 0.9250\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1664 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1243 - accuracy: 0.9570\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1163 - accuracy: 0.9620\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1063 - accuracy: 0.9640\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0836 - accuracy: 0.9730\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0764 - accuracy: 0.9720\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0673 - accuracy: 0.9770\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9820\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0674 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0490 - accuracy: 0.9840\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0356 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9820\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0602 - accuracy: 0.9750\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0464 - accuracy: 0.9870\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0503 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9850\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0281 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0492 - accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0321 - accuracy: 0.9890\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0418 - accuracy: 0.9850\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0385 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0351 - accuracy: 0.9890\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0290 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0241 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0009715151\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.900% | global_loss: 0.0009715150226838887\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12295126\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.12295124679803848\n",
            "tf.Tensor(0.000971515, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.12295125, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 1.5135 - accuracy: 0.4740\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4920 - accuracy: 0.8480\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3037 - accuracy: 0.9060\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2702 - accuracy: 0.9090\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1978 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1822 - accuracy: 0.9380\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1386 - accuracy: 0.9620\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1235 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0933 - accuracy: 0.9750\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1127 - accuracy: 0.9590\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0805 - accuracy: 0.9750\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0785 - accuracy: 0.9730\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0600 - accuracy: 0.9830\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0669 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0491 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0507 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9850\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9820\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0413 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.9880\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0242 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0387 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0239 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0324 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0302 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0269 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0179 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0288 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0272 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0016675836\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.001667583710514009\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13986965\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.13986964523792267\n",
            "tf.Tensor(0.0016675837, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.13986965, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5391 - accuracy: 0.4620\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5255 - accuracy: 0.8190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3341 - accuracy: 0.8950\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2548 - accuracy: 0.9160\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2230 - accuracy: 0.9380\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2075 - accuracy: 0.9290\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1567 - accuracy: 0.9490\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1346 - accuracy: 0.9510\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1292 - accuracy: 0.9560\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1178 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1149 - accuracy: 0.9580\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0881 - accuracy: 0.9690\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0746 - accuracy: 0.9820\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0642 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0649 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0622 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0921 - accuracy: 0.9700\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0556 - accuracy: 0.9830\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0578 - accuracy: 0.9780\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0581 - accuracy: 0.9780\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0525 - accuracy: 0.9780\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0449 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0542 - accuracy: 0.9830\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0447 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0433 - accuracy: 0.9860\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0339 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9880\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 0.9800\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9870\n",
            "Accuracy:  0.9869999885559082\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.002327778\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.000% | global_loss: 0.0023277781438082457\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10834009\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.10834009200334549\n",
            "tf.Tensor(0.0023277781, shape=(), dtype=float32) 0.11\n",
            "tf.Tensor(0.10834009, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.4559 - accuracy: 0.5030\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4947 - accuracy: 0.8410\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3351 - accuracy: 0.8940\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2424 - accuracy: 0.9260\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2183 - accuracy: 0.9350\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1579 - accuracy: 0.9420\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1238 - accuracy: 0.9640\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1400 - accuracy: 0.9570\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1298 - accuracy: 0.9530\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1061 - accuracy: 0.9660\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0764 - accuracy: 0.9790\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0719 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0726 - accuracy: 0.9720\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 0.9780\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9740\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0476 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0482 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0297 - accuracy: 0.9900\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0394 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.9840\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0359 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0324 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0268 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0424 - accuracy: 0.9840\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014624083\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.600% | global_loss: 0.0014624083414673805\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13333914\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.13333912193775177\n",
            "tf.Tensor(0.0014624083, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.13333912, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 2.3255 - accuracy: 0.1120\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.1957 - accuracy: 0.2110\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.0759 - accuracy: 0.2530\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.9839 - accuracy: 0.2840\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.8345 - accuracy: 0.3730\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.6477 - accuracy: 0.4350\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5183 - accuracy: 0.4780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.4621 - accuracy: 0.4750\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3703 - accuracy: 0.5340\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3244 - accuracy: 0.5360\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3103 - accuracy: 0.5540\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2632 - accuracy: 0.5880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2207 - accuracy: 0.5820\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2435 - accuracy: 0.5900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1864 - accuracy: 0.5840\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1662 - accuracy: 0.6010\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1798 - accuracy: 0.5780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0946 - accuracy: 0.6230\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0969 - accuracy: 0.6160\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1080 - accuracy: 0.6130\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1071 - accuracy: 0.6340\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0408 - accuracy: 0.6280\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0784 - accuracy: 0.6390\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.0137 - accuracy: 0.6490\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0443 - accuracy: 0.6390\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0591 - accuracy: 0.6290\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0313 - accuracy: 0.6430\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0207 - accuracy: 0.6430\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9819 - accuracy: 0.6670\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9590 - accuracy: 0.6790\n",
            "Accuracy:  0.6790000200271606\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.58180416\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 12.900% | global_loss: 0.5818042159080505\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "1.1726711\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 12.400% | global_loss: 1.1726711988449097\n",
            "tf.Tensor(0.5818042, shape=(), dtype=float32) 0.129\n",
            "tf.Tensor(1.1726712, shape=(), dtype=float32) 0.124\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5526 - accuracy: 0.4610\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5776 - accuracy: 0.8220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3340 - accuracy: 0.8970\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2495 - accuracy: 0.9120\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2098 - accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1710 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 0.9560\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1347 - accuracy: 0.9540\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1032 - accuracy: 0.9650\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9740\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0784 - accuracy: 0.9730\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0767 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0697 - accuracy: 0.9780\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0510 - accuracy: 0.9820\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9850\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0492 - accuracy: 0.9830\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9880\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0445 - accuracy: 0.9840\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9810\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9890\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 0.9860\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0273 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0372 - accuracy: 0.9890\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0306 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0018476449\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.200% | global_loss: 0.0018476450350135565\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1409755\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.14097550511360168\n",
            "tf.Tensor(0.001847645, shape=(), dtype=float32) 0.082\n",
            "tf.Tensor(0.1409755, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5515 - accuracy: 0.4870\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5600 - accuracy: 0.8120\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3560 - accuracy: 0.8840\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.2796 - accuracy: 0.9050\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2335 - accuracy: 0.9260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1777 - accuracy: 0.9400\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1634 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1548 - accuracy: 0.9460\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1264 - accuracy: 0.9560\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0998 - accuracy: 0.9650\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1051 - accuracy: 0.9680\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1027 - accuracy: 0.9660\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0872 - accuracy: 0.9680\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0726 - accuracy: 0.9760\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0881 - accuracy: 0.9690\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0717 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0673 - accuracy: 0.9760\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0646 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0623 - accuracy: 0.9800\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0491 - accuracy: 0.9800\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0602 - accuracy: 0.9730\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0296 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0292 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0308 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0307 - accuracy: 0.9870\n",
            "Accuracy:  0.9869999885559082\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014664432\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.100% | global_loss: 0.0014664430636912584\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.104663394\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.10466339439153671\n",
            "tf.Tensor(0.0014664431, shape=(), dtype=float32) 0.091\n",
            "tf.Tensor(0.104663394, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5394 - accuracy: 0.4810\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5579 - accuracy: 0.8300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3302 - accuracy: 0.8910\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2535 - accuracy: 0.9200\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1961 - accuracy: 0.9410\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1550 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1531 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1272 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1162 - accuracy: 0.9550\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0911 - accuracy: 0.9760\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0918 - accuracy: 0.9730\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0806 - accuracy: 0.9750\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0673 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0722 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0898 - accuracy: 0.9690\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0605 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9800\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0556 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0546 - accuracy: 0.9800\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 0.9900\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0595 - accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0469 - accuracy: 0.9840\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0584 - accuracy: 0.9800\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9820\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0246 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0308 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.002033013\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.002033012919127941\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.090177394\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.09017738699913025\n",
            "tf.Tensor(0.002033013, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.09017739, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5640 - accuracy: 0.4850\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6002 - accuracy: 0.7980\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3993 - accuracy: 0.8710\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2898 - accuracy: 0.9120\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2500 - accuracy: 0.9260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2243 - accuracy: 0.9280\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1643 - accuracy: 0.9430\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1739 - accuracy: 0.9360\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1366 - accuracy: 0.9550\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1423 - accuracy: 0.9510\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1203 - accuracy: 0.9610\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1117 - accuracy: 0.9600\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0918 - accuracy: 0.9710\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0899 - accuracy: 0.9700\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0919 - accuracy: 0.9660\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0844 - accuracy: 0.9720\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0625 - accuracy: 0.9760\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0666 - accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0652 - accuracy: 0.9790\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9850\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9830\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.9820\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0354 - accuracy: 0.9890\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0701 - accuracy: 0.9780\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0437 - accuracy: 0.9840\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0304 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0445 - accuracy: 0.9850\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9850\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0276 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0023028555\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.002302855486050248\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14053018\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.14053018391132355\n",
            "tf.Tensor(0.0023028555, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.14053018, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5588 - accuracy: 0.4640\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5188 - accuracy: 0.8190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3535 - accuracy: 0.8830\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2395 - accuracy: 0.9190\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2719 - accuracy: 0.9030\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1835 - accuracy: 0.9460\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1552 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1450 - accuracy: 0.9510\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1462 - accuracy: 0.9510\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1090 - accuracy: 0.9600\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1027 - accuracy: 0.9630\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0892 - accuracy: 0.9710\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0704 - accuracy: 0.9800\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0758 - accuracy: 0.9730\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0670 - accuracy: 0.9750\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0489 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0537 - accuracy: 0.9800\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0633 - accuracy: 0.9780\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9880\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0510 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0501 - accuracy: 0.9840\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9770\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0455 - accuracy: 0.9830\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0357 - accuracy: 0.9860\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0234 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.9870\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 0.9870\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013366585\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.100% | global_loss: 0.0013366587227210402\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1297173\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.12971730530261993\n",
            "tf.Tensor(0.0013366587, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.1297173, shape=(), dtype=float32) 0.094\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.4387 - accuracy: 0.5090\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5136 - accuracy: 0.8440\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3391 - accuracy: 0.9030\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.9230\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2006 - accuracy: 0.9290\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1924 - accuracy: 0.9390\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1633 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1231 - accuracy: 0.9620\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0975 - accuracy: 0.9730\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1013 - accuracy: 0.9720\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0851 - accuracy: 0.9710\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0857 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0709 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0688 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0740 - accuracy: 0.9720\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0500 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0415 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0743 - accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9840\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0342 - accuracy: 0.9900\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0363 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0450 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 0.9870\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0311 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0272 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0217 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0009882265\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.000% | global_loss: 0.000988226616755128\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.06658494\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.06658493727445602\n",
            "tf.Tensor(0.0009882266, shape=(), dtype=float32) 0.08\n",
            "tf.Tensor(0.06658494, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5230 - accuracy: 0.4550\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5594 - accuracy: 0.8260\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3189 - accuracy: 0.9010\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2748 - accuracy: 0.9220\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2007 - accuracy: 0.9380\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1575 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1553 - accuracy: 0.9470\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1295 - accuracy: 0.9470\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1094 - accuracy: 0.9710\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1074 - accuracy: 0.9650\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1113 - accuracy: 0.9620\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1000 - accuracy: 0.9680\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0873 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0923 - accuracy: 0.9730\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0629 - accuracy: 0.9750\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0762 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9800\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0476 - accuracy: 0.9810\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0385 - accuracy: 0.9880\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0507 - accuracy: 0.9820\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9890\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9880\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9880\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 0.9890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0397 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0449 - accuracy: 0.9850\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0194 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014950467\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.500% | global_loss: 0.0014950466575101018\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10230145\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.10230143368244171\n",
            "tf.Tensor(0.0014950467, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.10230143, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5308 - accuracy: 0.4660\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5669 - accuracy: 0.8150\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3425 - accuracy: 0.8930\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2768 - accuracy: 0.9140\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2334 - accuracy: 0.9240\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2039 - accuracy: 0.9270\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1574 - accuracy: 0.9440\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1295 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1148 - accuracy: 0.9580\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1408 - accuracy: 0.9490\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1126 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0824 - accuracy: 0.9780\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0666 - accuracy: 0.9760\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9740\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0742 - accuracy: 0.9740\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0785 - accuracy: 0.9770\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0559 - accuracy: 0.9800\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0389 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0443 - accuracy: 0.9820\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9800\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9880\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 0.9830\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0285 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0291 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0258 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0226 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0444 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Accuracy:  0.9869999885559082\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00371726\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.400% | global_loss: 0.0037172604352235794\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.15016037\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.15016035735607147\n",
            "tf.Tensor(0.0037172604, shape=(), dtype=float32) 0.114\n",
            "tf.Tensor(0.15016036, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5643 - accuracy: 0.4620\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5230 - accuracy: 0.8230\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3727 - accuracy: 0.8800\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2598 - accuracy: 0.9090\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2213 - accuracy: 0.9270\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1921 - accuracy: 0.9340\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1625 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1338 - accuracy: 0.9520\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1160 - accuracy: 0.9610\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0977 - accuracy: 0.9730\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0867 - accuracy: 0.9730\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1045 - accuracy: 0.9600\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0880 - accuracy: 0.9740\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0720 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0783 - accuracy: 0.9750\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9850\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0632 - accuracy: 0.9750\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0669 - accuracy: 0.9780\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0434 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9910\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0423 - accuracy: 0.9850\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9840\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0338 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0419 - accuracy: 0.9840\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0377 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0286 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0017272817\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.100% | global_loss: 0.0017272818367928267\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12302438\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.12302438914775848\n",
            "tf.Tensor(0.0017272818, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.12302439, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5109 - accuracy: 0.4790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4530 - accuracy: 0.8600\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2977 - accuracy: 0.9040\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2138 - accuracy: 0.9450\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1804 - accuracy: 0.9450\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1401 - accuracy: 0.9520\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1300 - accuracy: 0.9620\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1311 - accuracy: 0.9580\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1033 - accuracy: 0.9670\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0788 - accuracy: 0.9760\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9800\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0879 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0797 - accuracy: 0.9760\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0749 - accuracy: 0.9730\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0741 - accuracy: 0.9730\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0524 - accuracy: 0.9860\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9900\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0536 - accuracy: 0.9810\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0402 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0410 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9810\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0231 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0263 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0252 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0015146609\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.900% | global_loss: 0.001514660893008113\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14365588\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.14365588128566742\n",
            "tf.Tensor(0.0015146609, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.14365588, shape=(), dtype=float32) 0.094\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.7273 - accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6405 - accuracy: 0.8030\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4055 - accuracy: 0.8810\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3273 - accuracy: 0.8910\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2345 - accuracy: 0.9240\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2114 - accuracy: 0.9320\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1830 - accuracy: 0.9420\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1670 - accuracy: 0.9490\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9440\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1206 - accuracy: 0.9580\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1530 - accuracy: 0.9470\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9630\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1320 - accuracy: 0.9530\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1410 - accuracy: 0.9550\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1130 - accuracy: 0.9650\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0721 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0796 - accuracy: 0.9710\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0846 - accuracy: 0.9680\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0749 - accuracy: 0.9710\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0739 - accuracy: 0.9760\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9760\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0705 - accuracy: 0.9770\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0614 - accuracy: 0.9830\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0547 - accuracy: 0.9810\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0394 - accuracy: 0.9870\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0490 - accuracy: 0.9810\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0555 - accuracy: 0.9810\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0468 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0433 - accuracy: 0.9870\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9820\n",
            "Accuracy:  0.9819999933242798\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0038589004\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.200% | global_loss: 0.003858900163322687\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10625673\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.10625673830509186\n",
            "tf.Tensor(0.0038589002, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.10625674, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.4967 - accuracy: 0.4700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5138 - accuracy: 0.8190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3181 - accuracy: 0.9000\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2283 - accuracy: 0.9380\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2169 - accuracy: 0.9300\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1610 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1695 - accuracy: 0.9440\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1506 - accuracy: 0.9460\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 0.9590\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1106 - accuracy: 0.9660\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0902 - accuracy: 0.9680\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0864 - accuracy: 0.9640\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0847 - accuracy: 0.9760\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0668 - accuracy: 0.9760\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0566 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9870\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0452 - accuracy: 0.9870\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0738 - accuracy: 0.9750\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0665 - accuracy: 0.9750\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0654 - accuracy: 0.9790\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0411 - accuracy: 0.9880\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0314 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0304 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 0.9860\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0264 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014400362\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.900% | global_loss: 0.0014400363434106112\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.105209835\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.1052098423242569\n",
            "tf.Tensor(0.0014400363, shape=(), dtype=float32) 0.109\n",
            "tf.Tensor(0.10520984, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5305 - accuracy: 0.4770\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5487 - accuracy: 0.8350\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3411 - accuracy: 0.8970\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2548 - accuracy: 0.9070\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1970 - accuracy: 0.9420\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1788 - accuracy: 0.9400\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1433 - accuracy: 0.9550\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1324 - accuracy: 0.9570\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1088 - accuracy: 0.9600\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1028 - accuracy: 0.9670\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0796 - accuracy: 0.9760\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0754 - accuracy: 0.9750\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0847 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0767 - accuracy: 0.9790\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9760\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0732 - accuracy: 0.9770\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0754 - accuracy: 0.9760\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0633 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0455 - accuracy: 0.9870\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0496 - accuracy: 0.9820\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9880\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0456 - accuracy: 0.9850\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 0.9820\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0506 - accuracy: 0.9820\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9850\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0266 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0184 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013106434\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 7.500% | global_loss: 0.001310643507167697\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11431988\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.11431986838579178\n",
            "tf.Tensor(0.0013106435, shape=(), dtype=float32) 0.075\n",
            "tf.Tensor(0.11431987, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5521 - accuracy: 0.4660\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5352 - accuracy: 0.8260\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3312 - accuracy: 0.9020\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2539 - accuracy: 0.9230\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1964 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1763 - accuracy: 0.9440\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1821 - accuracy: 0.9430\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1377 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1071 - accuracy: 0.9660\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1133 - accuracy: 0.9600\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0900 - accuracy: 0.9710\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0858 - accuracy: 0.9690\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0535 - accuracy: 0.9830\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0630 - accuracy: 0.9790\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0768 - accuracy: 0.9740\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0566 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0595 - accuracy: 0.9740\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0842 - accuracy: 0.9740\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0641 - accuracy: 0.9810\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0633 - accuracy: 0.9820\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0346 - accuracy: 0.9890\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0290 - accuracy: 0.9890\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9840\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0315 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9880\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0276 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0241 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0301 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0011774825\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.300% | global_loss: 0.0011774825397878885\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.114851855\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.11485185474157333\n",
            "tf.Tensor(0.0011774825, shape=(), dtype=float32) 0.083\n",
            "tf.Tensor(0.114851855, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5867 - accuracy: 0.4550\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5818 - accuracy: 0.8100\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3632 - accuracy: 0.8830\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2955 - accuracy: 0.9130\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2604 - accuracy: 0.9180\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1717 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1644 - accuracy: 0.9490\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1452 - accuracy: 0.9550\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1143 - accuracy: 0.9600\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1029 - accuracy: 0.9670\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0999 - accuracy: 0.9600\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0969 - accuracy: 0.9690\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0700 - accuracy: 0.9780\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0968 - accuracy: 0.9640\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0994 - accuracy: 0.9660\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0476 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9770\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0699 - accuracy: 0.9790\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9820\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9790\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0454 - accuracy: 0.9840\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9840\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0489 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9820\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0416 - accuracy: 0.9860\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0377 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0277 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013081634\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.000% | global_loss: 0.001308163278736174\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08960504\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.08960503339767456\n",
            "tf.Tensor(0.0013081633, shape=(), dtype=float32) 0.11\n",
            "tf.Tensor(0.08960503, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5429 - accuracy: 0.4510\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5745 - accuracy: 0.8080\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3341 - accuracy: 0.8990\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2636 - accuracy: 0.9010\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2261 - accuracy: 0.9180\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1705 - accuracy: 0.9440\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1783 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 0.9640\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1048 - accuracy: 0.9630\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0864 - accuracy: 0.9680\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0761 - accuracy: 0.9730\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0855 - accuracy: 0.9680\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0858 - accuracy: 0.9720\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0910 - accuracy: 0.9710\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9790\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0717 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0477 - accuracy: 0.9820\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0672 - accuracy: 0.9790\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0470 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9800\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9770\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0268 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0359 - accuracy: 0.9830\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 0.9890\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0254 - accuracy: 0.9900\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0369 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0352 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0019821331\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.001982133137062192\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12913129\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.12913130223751068\n",
            "tf.Tensor(0.0019821331, shape=(), dtype=float32) 0.09\n",
            "tf.Tensor(0.1291313, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5552 - accuracy: 0.4480\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5229 - accuracy: 0.8220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3128 - accuracy: 0.9020\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2789 - accuracy: 0.9100\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1975 - accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1588 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1832 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1550 - accuracy: 0.9480\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1103 - accuracy: 0.9670\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1157 - accuracy: 0.9630\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0985 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1059 - accuracy: 0.9630\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0857 - accuracy: 0.9710\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0908 - accuracy: 0.9670\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0577 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0539 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0536 - accuracy: 0.9810\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0653 - accuracy: 0.9780\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0600 - accuracy: 0.9750\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0481 - accuracy: 0.9880\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0409 - accuracy: 0.9850\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9850\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0418 - accuracy: 0.9810\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9830\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0255 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 0.9840\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 0.9850\n",
            "Accuracy:  0.9850000143051147\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0028323703\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.000% | global_loss: 0.002832370810210705\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.111988924\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.11198892444372177\n",
            "tf.Tensor(0.0028323708, shape=(), dtype=float32) 0.1\n",
            "tf.Tensor(0.111988924, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.6111 - accuracy: 0.4280\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5535 - accuracy: 0.8220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3260 - accuracy: 0.8990\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2739 - accuracy: 0.9070\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2321 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1934 - accuracy: 0.9410\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1653 - accuracy: 0.9440\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1458 - accuracy: 0.9550\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1287 - accuracy: 0.9540\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1114 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0855 - accuracy: 0.9740\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1076 - accuracy: 0.9680\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0897 - accuracy: 0.9680\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0986 - accuracy: 0.9620\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0741 - accuracy: 0.9760\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0762 - accuracy: 0.9730\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0786 - accuracy: 0.9770\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0735 - accuracy: 0.9750\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0686 - accuracy: 0.9740\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0934 - accuracy: 0.9720\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0669 - accuracy: 0.9770\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0489 - accuracy: 0.9820\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0475 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0462 - accuracy: 0.9820\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9850\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9850\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9880\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0371 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0458 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00213198\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.100% | global_loss: 0.002131979912519455\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12806782\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.12806782126426697\n",
            "tf.Tensor(0.00213198, shape=(), dtype=float32) 0.101\n",
            "tf.Tensor(0.12806782, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5276 - accuracy: 0.4840\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6057 - accuracy: 0.8110\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3786 - accuracy: 0.8850\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2901 - accuracy: 0.9220\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2456 - accuracy: 0.9240\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2316 - accuracy: 0.9280\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1649 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1368 - accuracy: 0.9460\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1063 - accuracy: 0.9680\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1193 - accuracy: 0.9660\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1023 - accuracy: 0.9650\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0871 - accuracy: 0.9690\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0924 - accuracy: 0.9700\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0926 - accuracy: 0.9720\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0741 - accuracy: 0.9740\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0622 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0613 - accuracy: 0.9770\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0490 - accuracy: 0.9810\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0642 - accuracy: 0.9720\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0388 - accuracy: 0.9880\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0410 - accuracy: 0.9850\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0480 - accuracy: 0.9840\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0455 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0467 - accuracy: 0.9860\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0449 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0269 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0398 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0025607056\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.002560705877840519\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1656142\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.16561418771743774\n",
            "tf.Tensor(0.0025607059, shape=(), dtype=float32) 0.092\n",
            "tf.Tensor(0.16561419, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 2.3190 - accuracy: 0.1220\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.1182 - accuracy: 0.2330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.8835 - accuracy: 0.3390\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.8079 - accuracy: 0.3540\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.7069 - accuracy: 0.4090\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.6401 - accuracy: 0.4330\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.6547 - accuracy: 0.4130\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.6323 - accuracy: 0.4300\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5873 - accuracy: 0.4580\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5900 - accuracy: 0.4430\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5814 - accuracy: 0.4540\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5523 - accuracy: 0.4600\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5240 - accuracy: 0.4660\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5429 - accuracy: 0.4540\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5391 - accuracy: 0.4750\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5295 - accuracy: 0.4620\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5165 - accuracy: 0.4780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.4605 - accuracy: 0.5040\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4549 - accuracy: 0.5010\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5129 - accuracy: 0.4680\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4856 - accuracy: 0.4960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5006 - accuracy: 0.4660\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4922 - accuracy: 0.4890\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.4778 - accuracy: 0.4810\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4673 - accuracy: 0.4700\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4364 - accuracy: 0.5110\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4506 - accuracy: 0.4880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4898 - accuracy: 0.4720\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4497 - accuracy: 0.4750\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.4239 - accuracy: 0.5030\n",
            "Accuracy:  0.503000020980835\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "1.1345551\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.000% | global_loss: 1.1345551013946533\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "1.4281735\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 1.428173542022705\n",
            "tf.Tensor(1.1345551, shape=(), dtype=float32) 0.11\n",
            "tf.Tensor(1.4281735, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5481 - accuracy: 0.4580\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5716 - accuracy: 0.8140\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3776 - accuracy: 0.8740\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2829 - accuracy: 0.8970\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2198 - accuracy: 0.9350\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1637 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1389 - accuracy: 0.9510\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1477 - accuracy: 0.9520\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1373 - accuracy: 0.9530\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1031 - accuracy: 0.9610\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0882 - accuracy: 0.9760\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0798 - accuracy: 0.9720\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0787 - accuracy: 0.9690\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0852 - accuracy: 0.9700\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0625 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0781 - accuracy: 0.9780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0488 - accuracy: 0.9910\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0470 - accuracy: 0.9850\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0583 - accuracy: 0.9820\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0515 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0606 - accuracy: 0.9780\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0477 - accuracy: 0.9830\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0381 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9890\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9880\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0291 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0018490399\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.600% | global_loss: 0.0018490399233996868\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11981964\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.11981964111328125\n",
            "tf.Tensor(0.0018490399, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.11981964, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.6145 - accuracy: 0.4430\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5677 - accuracy: 0.8290\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3458 - accuracy: 0.9000\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2535 - accuracy: 0.9210\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2503 - accuracy: 0.9210\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1921 - accuracy: 0.9460\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1611 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1607 - accuracy: 0.9430\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1122 - accuracy: 0.9610\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0993 - accuracy: 0.9690\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1133 - accuracy: 0.9640\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0994 - accuracy: 0.9660\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0765 - accuracy: 0.9700\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0595 - accuracy: 0.9790\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0706 - accuracy: 0.9790\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0500 - accuracy: 0.9860\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9870\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0451 - accuracy: 0.9840\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0575 - accuracy: 0.9840\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0797 - accuracy: 0.9750\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0526 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0369 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0432 - accuracy: 0.9860\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0217 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0426 - accuracy: 0.9860\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 0.9900\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0021891766\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.400% | global_loss: 0.0021891766227781773\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.06568361\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.06568360328674316\n",
            "tf.Tensor(0.0021891766, shape=(), dtype=float32) 0.104\n",
            "tf.Tensor(0.0656836, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4939 - accuracy: 0.4850\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5373 - accuracy: 0.8330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3295 - accuracy: 0.8950\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2485 - accuracy: 0.9250\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2037 - accuracy: 0.9290\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1598 - accuracy: 0.9470\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1186 - accuracy: 0.9650\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1158 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1170 - accuracy: 0.9640\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0756 - accuracy: 0.9790\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0750 - accuracy: 0.9780\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0780 - accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0777 - accuracy: 0.9700\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0505 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0510 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0413 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9840\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0443 - accuracy: 0.9870\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9810\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0297 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9840\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0253 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0338 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0300 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0382 - accuracy: 0.9830\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0282 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0012013449\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.400% | global_loss: 0.0012013450032100081\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.19316553\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.19316555559635162\n",
            "tf.Tensor(0.001201345, shape=(), dtype=float32) 0.084\n",
            "tf.Tensor(0.19316556, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5861 - accuracy: 0.4680\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5545 - accuracy: 0.8220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3595 - accuracy: 0.8820\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3281 - accuracy: 0.8890\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2212 - accuracy: 0.9260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2264 - accuracy: 0.9360\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1724 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1691 - accuracy: 0.9470\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1341 - accuracy: 0.9600\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1062 - accuracy: 0.9700\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1021 - accuracy: 0.9680\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0998 - accuracy: 0.9690\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1250 - accuracy: 0.9610\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0898 - accuracy: 0.9720\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0848 - accuracy: 0.9730\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0731 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0717 - accuracy: 0.9740\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0624 - accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0621 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0392 - accuracy: 0.9910\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0568 - accuracy: 0.9810\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0552 - accuracy: 0.9800\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0547 - accuracy: 0.9810\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9870\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0516 - accuracy: 0.9810\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0412 - accuracy: 0.9860\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0347 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0356 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0031561784\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.700% | global_loss: 0.0031561783980578184\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08491987\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.08491986244916916\n",
            "tf.Tensor(0.0031561784, shape=(), dtype=float32) 0.097\n",
            "tf.Tensor(0.08491986, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5907 - accuracy: 0.4560\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5772 - accuracy: 0.7990\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3756 - accuracy: 0.8790\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2849 - accuracy: 0.9000\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2349 - accuracy: 0.9230\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1876 - accuracy: 0.9390\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1734 - accuracy: 0.9390\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1463 - accuracy: 0.9500\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1462 - accuracy: 0.9470\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1249 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0897 - accuracy: 0.9700\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1030 - accuracy: 0.9670\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0977 - accuracy: 0.9590\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0905 - accuracy: 0.9660\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0868 - accuracy: 0.9730\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0761 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0789 - accuracy: 0.9710\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9730\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0520 - accuracy: 0.9820\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0610 - accuracy: 0.9750\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0815 - accuracy: 0.9750\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9820\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9750\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0622 - accuracy: 0.9820\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0758 - accuracy: 0.9730\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0425 - accuracy: 0.9860\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0743 - accuracy: 0.9800\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9820\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0513 - accuracy: 0.9850\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0434 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0027675035\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.0027675034943968058\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13579658\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.13579657673835754\n",
            "tf.Tensor(0.0027675035, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.13579658, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5758 - accuracy: 0.4700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4870 - accuracy: 0.8460\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3368 - accuracy: 0.8890\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2393 - accuracy: 0.9140\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2030 - accuracy: 0.9330\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1917 - accuracy: 0.9470\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1449 - accuracy: 0.9570\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1245 - accuracy: 0.9630\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1155 - accuracy: 0.9650\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1266 - accuracy: 0.9500\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0715 - accuracy: 0.9750\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0807 - accuracy: 0.9680\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0724 - accuracy: 0.9800\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9800\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0540 - accuracy: 0.9820\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0691 - accuracy: 0.9780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0465 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0594 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9830\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0476 - accuracy: 0.9820\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9850\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9840\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0509 - accuracy: 0.9800\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9840\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0382 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0288 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0325 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014273449\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.800% | global_loss: 0.0014273450942710042\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12692146\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.12692147493362427\n",
            "tf.Tensor(0.0014273451, shape=(), dtype=float32) 0.108\n",
            "tf.Tensor(0.12692147, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.4482 - accuracy: 0.4900\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5220 - accuracy: 0.8370\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3230 - accuracy: 0.8960\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2230 - accuracy: 0.9230\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1793 - accuracy: 0.9380\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1702 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1249 - accuracy: 0.9600\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1077 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1029 - accuracy: 0.9670\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1038 - accuracy: 0.9700\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0782 - accuracy: 0.9700\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0850 - accuracy: 0.9730\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0746 - accuracy: 0.9710\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0500 - accuracy: 0.9820\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0495 - accuracy: 0.9860\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0442 - accuracy: 0.9870\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0642 - accuracy: 0.9750\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0650 - accuracy: 0.9740\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0620 - accuracy: 0.9740\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0323 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0341 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0357 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9870\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0294 - accuracy: 0.9890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0347 - accuracy: 0.9890\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0266 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0294 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0022222158\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.900% | global_loss: 0.0022222157567739487\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13056625\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.13056623935699463\n",
            "tf.Tensor(0.0022222158, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.13056624, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4316 - accuracy: 0.4990\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4576 - accuracy: 0.8400\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2867 - accuracy: 0.9070\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2023 - accuracy: 0.9370\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1718 - accuracy: 0.9350\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9590\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1018 - accuracy: 0.9660\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1138 - accuracy: 0.9580\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1150 - accuracy: 0.9590\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0616 - accuracy: 0.9790\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0776 - accuracy: 0.9760\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9810\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0492 - accuracy: 0.9840\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0595 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0629 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0422 - accuracy: 0.9860\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9860\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0485 - accuracy: 0.9840\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0382 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0398 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0361 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0358 - accuracy: 0.9860\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0233 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0227 - accuracy: 0.9890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0345 - accuracy: 0.9900\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0294 - accuracy: 0.9870\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0290 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0010788013\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.0010788013460114598\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.105678596\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.10567858815193176\n",
            "tf.Tensor(0.0010788013, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.10567859, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.6123 - accuracy: 0.4360\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5653 - accuracy: 0.8100\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3224 - accuracy: 0.9000\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2186 - accuracy: 0.9250\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2339 - accuracy: 0.9280\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1662 - accuracy: 0.9420\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1591 - accuracy: 0.9440\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1286 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1175 - accuracy: 0.9610\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0946 - accuracy: 0.9650\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0959 - accuracy: 0.9670\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9800\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0827 - accuracy: 0.9640\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0833 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0622 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0651 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0640 - accuracy: 0.9770\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0648 - accuracy: 0.9810\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 0.9790\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0631 - accuracy: 0.9780\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9890\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9820\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0357 - accuracy: 0.9880\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 0.9890\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0309 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0278 - accuracy: 0.9890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9880\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0386 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0305 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014995652\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.100% | global_loss: 0.0014995653182268143\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.16661687\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.16661687195301056\n",
            "tf.Tensor(0.0014995653, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.16661687, shape=(), dtype=float32) 0.094\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.5607 - accuracy: 0.4620\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4836 - accuracy: 0.8400\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2950 - accuracy: 0.9040\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2503 - accuracy: 0.9130\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1711 - accuracy: 0.9450\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1649 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1606 - accuracy: 0.9470\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1295 - accuracy: 0.9530\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0944 - accuracy: 0.9710\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0871 - accuracy: 0.9690\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0930 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0700 - accuracy: 0.9730\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0612 - accuracy: 0.9790\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0565 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0794 - accuracy: 0.9730\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9820\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0540 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9850\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0430 - accuracy: 0.9850\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0289 - accuracy: 0.9890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9850\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0239 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0144 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0307 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0011152949\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.600% | global_loss: 0.0011152948718518019\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10019553\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.10019552707672119\n",
            "tf.Tensor(0.0011152949, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.10019553, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 2.3486 - accuracy: 0.1140\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.2997 - accuracy: 0.1200\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2987 - accuracy: 0.1200\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.3016 - accuracy: 0.1200\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2991 - accuracy: 0.1200\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2992 - accuracy: 0.1200\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2999 - accuracy: 0.1200\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2994 - accuracy: 0.1200\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2976 - accuracy: 0.1210\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2985 - accuracy: 0.1220\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2988 - accuracy: 0.1220\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2985 - accuracy: 0.1200\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2986 - accuracy: 0.1200\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2981 - accuracy: 0.1200\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2984 - accuracy: 0.1200\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2981 - accuracy: 0.1200\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2973 - accuracy: 0.1200\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2987 - accuracy: 0.1200\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2982 - accuracy: 0.1210\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.2991 - accuracy: 0.1200\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2982 - accuracy: 0.1210\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2950 - accuracy: 0.1210\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.2017 - accuracy: 0.1750\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.0957 - accuracy: 0.2560\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.0445 - accuracy: 0.2550\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.0306 - accuracy: 0.2540\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.9642 - accuracy: 0.3000\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.9303 - accuracy: 0.3290\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.9618 - accuracy: 0.2990\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.9355 - accuracy: 0.3030\n",
            "Accuracy:  0.30300000309944153\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "1.7523403\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 1.752340316772461\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "1.8612232\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 13.800% | global_loss: 1.8612233400344849\n",
            "tf.Tensor(1.7523403, shape=(), dtype=float32) 0.092\n",
            "tf.Tensor(1.8612233, shape=(), dtype=float32) 0.138\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5199 - accuracy: 0.4780\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5155 - accuracy: 0.8370\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3541 - accuracy: 0.8880\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2615 - accuracy: 0.9130\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2354 - accuracy: 0.9290\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1719 - accuracy: 0.9370\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1511 - accuracy: 0.9550\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1549 - accuracy: 0.9550\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1243 - accuracy: 0.9570\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1159 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1070 - accuracy: 0.9650\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0840 - accuracy: 0.9730\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0843 - accuracy: 0.9690\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9810\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0687 - accuracy: 0.9790\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0657 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0754 - accuracy: 0.9790\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0427 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0447 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0454 - accuracy: 0.9810\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0306 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0375 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0379 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0404 - accuracy: 0.9830\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0282 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0385 - accuracy: 0.9880\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0316 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9870\n",
            "Accuracy:  0.9869999885559082\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.002628529\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 12.000% | global_loss: 0.002628528978675604\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12821451\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.1282145082950592\n",
            "tf.Tensor(0.002628529, shape=(), dtype=float32) 0.12\n",
            "tf.Tensor(0.12821451, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5265 - accuracy: 0.4770\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5484 - accuracy: 0.8210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3493 - accuracy: 0.8940\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2493 - accuracy: 0.9220\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2337 - accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1579 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1707 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1200 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1141 - accuracy: 0.9680\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1270 - accuracy: 0.9540\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1326 - accuracy: 0.9580\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0924 - accuracy: 0.9720\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0748 - accuracy: 0.9780\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0906 - accuracy: 0.9670\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0586 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0595 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0600 - accuracy: 0.9790\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0527 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0589 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0372 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0423 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0287 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0502 - accuracy: 0.9830\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0279 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0450 - accuracy: 0.9820\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 0.9870\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00450521\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.100% | global_loss: 0.004505210090428591\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.06464174\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.06464173644781113\n",
            "tf.Tensor(0.00450521, shape=(), dtype=float32) 0.081\n",
            "tf.Tensor(0.06464174, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 1.5905 - accuracy: 0.4530\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5660 - accuracy: 0.8140\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3388 - accuracy: 0.8880\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2888 - accuracy: 0.9090\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2022 - accuracy: 0.9380\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1804 - accuracy: 0.9400\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1500 - accuracy: 0.9540\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1207 - accuracy: 0.9630\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1191 - accuracy: 0.9650\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1161 - accuracy: 0.9590\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0902 - accuracy: 0.9710\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0917 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0802 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 0.9830\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0608 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0698 - accuracy: 0.9780\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0638 - accuracy: 0.9750\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0511 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0500 - accuracy: 0.9810\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0554 - accuracy: 0.9790\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0483 - accuracy: 0.9850\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9880\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0400 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 0.9890\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0323 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0039412645\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.200% | global_loss: 0.003941264469176531\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12015815\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.12015814334154129\n",
            "tf.Tensor(0.0039412645, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.12015814, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4702 - accuracy: 0.5010\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5303 - accuracy: 0.8240\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3466 - accuracy: 0.8830\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2770 - accuracy: 0.9120\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2139 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1560 - accuracy: 0.9480\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1342 - accuracy: 0.9580\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1314 - accuracy: 0.9520\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1358 - accuracy: 0.9530\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1029 - accuracy: 0.9680\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1090 - accuracy: 0.9650\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0781 - accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0583 - accuracy: 0.9780\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0714 - accuracy: 0.9760\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0639 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0498 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0634 - accuracy: 0.9750\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0389 - accuracy: 0.9870\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0255 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0376 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0288 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0375 - accuracy: 0.9840\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0266 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0279 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0294 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013153526\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.0013153527397662401\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.07902217\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.07902216911315918\n",
            "tf.Tensor(0.0013153527, shape=(), dtype=float32) 0.088\n",
            "tf.Tensor(0.07902217, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4908 - accuracy: 0.4860\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5252 - accuracy: 0.8320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3466 - accuracy: 0.8940\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2532 - accuracy: 0.9230\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2183 - accuracy: 0.9260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1846 - accuracy: 0.9420\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1585 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1488 - accuracy: 0.9520\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1374 - accuracy: 0.9530\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0877 - accuracy: 0.9720\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0937 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1021 - accuracy: 0.9630\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9740\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0711 - accuracy: 0.9790\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0722 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0702 - accuracy: 0.9790\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0573 - accuracy: 0.9830\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0517 - accuracy: 0.9820\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9850\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0366 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0369 - accuracy: 0.9850\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0372 - accuracy: 0.9840\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0544 - accuracy: 0.9790\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0226 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 0.9870\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0311 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0243 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0019541327\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.100% | global_loss: 0.0019541329238563776\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1468117\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.14681167900562286\n",
            "tf.Tensor(0.001954133, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.14681168, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5112 - accuracy: 0.4760\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5401 - accuracy: 0.8110\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3183 - accuracy: 0.8900\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2310 - accuracy: 0.9280\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1910 - accuracy: 0.9390\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1644 - accuracy: 0.9460\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1608 - accuracy: 0.9380\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1000 - accuracy: 0.9650\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1084 - accuracy: 0.9640\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1051 - accuracy: 0.9640\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1037 - accuracy: 0.9660\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0722 - accuracy: 0.9750\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0693 - accuracy: 0.9780\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0649 - accuracy: 0.9830\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0641 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0492 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0529 - accuracy: 0.9840\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 0.9830\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 0.9800\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0483 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9890\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0293 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0333 - accuracy: 0.9890\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0388 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0305 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0231 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0206 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0264 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0360 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014124217\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 11.200% | global_loss: 0.0014124218141660094\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08705411\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.08705411851406097\n",
            "tf.Tensor(0.0014124218, shape=(), dtype=float32) 0.112\n",
            "tf.Tensor(0.08705412, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5290 - accuracy: 0.4620\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5108 - accuracy: 0.8310\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3598 - accuracy: 0.8840\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2993 - accuracy: 0.9030\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2664 - accuracy: 0.9180\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1939 - accuracy: 0.9360\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.1604 - accuracy: 0.9480\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1550 - accuracy: 0.9460\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1436 - accuracy: 0.9590\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1190 - accuracy: 0.9600\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1052 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0946 - accuracy: 0.9700\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0790 - accuracy: 0.9690\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0894 - accuracy: 0.9670\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0704 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0852 - accuracy: 0.9730\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0718 - accuracy: 0.9770\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0595 - accuracy: 0.9840\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0645 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9820\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0436 - accuracy: 0.9810\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0464 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0365 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0452 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0463 - accuracy: 0.9830\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0402 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0021359278\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.00213592778891325\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11174672\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.11174672096967697\n",
            "tf.Tensor(0.0021359278, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.11174672, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5767 - accuracy: 0.4530\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5073 - accuracy: 0.8330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3390 - accuracy: 0.9020\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2284 - accuracy: 0.9240\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2335 - accuracy: 0.9290\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1475 - accuracy: 0.9520\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1528 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1276 - accuracy: 0.9560\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1039 - accuracy: 0.9640\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0965 - accuracy: 0.9750\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9720\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0820 - accuracy: 0.9770\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9790\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0848 - accuracy: 0.9720\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0665 - accuracy: 0.9740\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0560 - accuracy: 0.9810\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0387 - accuracy: 0.9860\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0459 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0502 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0743 - accuracy: 0.9780\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0453 - accuracy: 0.9840\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9890\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0393 - accuracy: 0.9830\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0389 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9830\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9880\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0393 - accuracy: 0.9830\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0242 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0023606978\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.500% | global_loss: 0.0023606978356838226\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13533431\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.135334312915802\n",
            "tf.Tensor(0.0023606978, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.13533431, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 1.5028 - accuracy: 0.4790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5001 - accuracy: 0.8350\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3227 - accuracy: 0.8960\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2550 - accuracy: 0.9120\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2060 - accuracy: 0.9360\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1655 - accuracy: 0.9480\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1312 - accuracy: 0.9550\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1246 - accuracy: 0.9560\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1136 - accuracy: 0.9570\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0964 - accuracy: 0.9650\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0865 - accuracy: 0.9730\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9770\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0763 - accuracy: 0.9750\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0650 - accuracy: 0.9820\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0708 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0737 - accuracy: 0.9740\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0570 - accuracy: 0.9810\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0533 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0505 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0306 - accuracy: 0.9880\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9900\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0306 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0399 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0237 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0261 - accuracy: 0.9880\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0209 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0321 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0204 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0209 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0010322724\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.900% | global_loss: 0.0010322722373530269\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08530658\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.08530658483505249\n",
            "tf.Tensor(0.0010322722, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.085306585, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4832 - accuracy: 0.4760\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5171 - accuracy: 0.8280\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3467 - accuracy: 0.8830\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2518 - accuracy: 0.9200\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2037 - accuracy: 0.9430\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1561 - accuracy: 0.9450\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1254 - accuracy: 0.9580\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1334 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9630\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0902 - accuracy: 0.9720\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1034 - accuracy: 0.9610\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0688 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0730 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0652 - accuracy: 0.9760\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0610 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0448 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0601 - accuracy: 0.9800\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0472 - accuracy: 0.9820\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0409 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0395 - accuracy: 0.9830\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0388 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0305 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0286 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0297 - accuracy: 0.9880\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0457 - accuracy: 0.9810\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0430 - accuracy: 0.9840\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0325 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0015865904\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.300% | global_loss: 0.0015865903114899993\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.18512884\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.1851288229227066\n",
            "tf.Tensor(0.0015865903, shape=(), dtype=float32) 0.103\n",
            "tf.Tensor(0.18512882, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5522 - accuracy: 0.4630\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5613 - accuracy: 0.8170\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3242 - accuracy: 0.8890\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2902 - accuracy: 0.9020\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1855 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1494 - accuracy: 0.9500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1410 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1188 - accuracy: 0.9600\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1136 - accuracy: 0.9590\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9540\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0841 - accuracy: 0.9770\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0833 - accuracy: 0.9700\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0765 - accuracy: 0.9740\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0969 - accuracy: 0.9690\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0590 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0690 - accuracy: 0.9810\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0428 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 0.9870\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0402 - accuracy: 0.9910\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0497 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0530 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0386 - accuracy: 0.9830\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0267 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0395 - accuracy: 0.9860\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0374 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0383 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0291 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0469 - accuracy: 0.9850\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0016529702\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.300% | global_loss: 0.0016529702115803957\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.06654723\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.0665472149848938\n",
            "tf.Tensor(0.0016529702, shape=(), dtype=float32) 0.103\n",
            "tf.Tensor(0.066547215, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5631 - accuracy: 0.4610\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6233 - accuracy: 0.8050\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3884 - accuracy: 0.8790\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3053 - accuracy: 0.8960\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2300 - accuracy: 0.9230\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1962 - accuracy: 0.9370\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1567 - accuracy: 0.9450\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1609 - accuracy: 0.9440\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1415 - accuracy: 0.9550\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0967 - accuracy: 0.9700\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1025 - accuracy: 0.9660\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0936 - accuracy: 0.9680\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0856 - accuracy: 0.9670\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0613 - accuracy: 0.9830\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0710 - accuracy: 0.9760\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0546 - accuracy: 0.9820\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0641 - accuracy: 0.9790\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0637 - accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0651 - accuracy: 0.9790\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0587 - accuracy: 0.9810\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0632 - accuracy: 0.9780\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0456 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0379 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0453 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0437 - accuracy: 0.9840\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0477 - accuracy: 0.9820\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0329 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0409 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0026441913\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.900% | global_loss: 0.002644191263243556\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10226683\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.1022668331861496\n",
            "tf.Tensor(0.0026441913, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.10226683, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4816 - accuracy: 0.4830\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5217 - accuracy: 0.8260\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3974 - accuracy: 0.8710\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2460 - accuracy: 0.9110\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1767 - accuracy: 0.9410\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1675 - accuracy: 0.9400\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1307 - accuracy: 0.9630\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1331 - accuracy: 0.9580\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9620\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0933 - accuracy: 0.9710\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1148 - accuracy: 0.9590\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0984 - accuracy: 0.9640\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0876 - accuracy: 0.9710\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0687 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0627 - accuracy: 0.9780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0548 - accuracy: 0.9810\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0926 - accuracy: 0.9660\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0687 - accuracy: 0.9780\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9810\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0646 - accuracy: 0.9790\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0462 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0385 - accuracy: 0.9890\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0503 - accuracy: 0.9880\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0407 - accuracy: 0.9830\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0431 - accuracy: 0.9810\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0446 - accuracy: 0.9840\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0016610582\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.200% | global_loss: 0.0016610582824796438\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12776649\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.200% | global_loss: 0.12776648998260498\n",
            "tf.Tensor(0.0016610583, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.12776649, shape=(), dtype=float32) 0.092\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4963 - accuracy: 0.4890\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5461 - accuracy: 0.8240\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3653 - accuracy: 0.8830\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3114 - accuracy: 0.8940\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2199 - accuracy: 0.9300\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1820 - accuracy: 0.9490\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1796 - accuracy: 0.9400\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1388 - accuracy: 0.9540\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9560\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1271 - accuracy: 0.9570\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0975 - accuracy: 0.9710\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1012 - accuracy: 0.9720\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0839 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0699 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0765 - accuracy: 0.9710\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0601 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0536 - accuracy: 0.9810\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0590 - accuracy: 0.9790\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0500 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 0.9880\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9850\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0545 - accuracy: 0.9810\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0591 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0483 - accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0378 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0280 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0397 - accuracy: 0.9870\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0371 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0279 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0012560366\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.500% | global_loss: 0.001256036339327693\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1059607\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.10596068203449249\n",
            "tf.Tensor(0.0012560363, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.10596068, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4988 - accuracy: 0.4810\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5138 - accuracy: 0.8300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3408 - accuracy: 0.8890\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3023 - accuracy: 0.8960\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2237 - accuracy: 0.9260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2129 - accuracy: 0.9250\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1439 - accuracy: 0.9550\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1451 - accuracy: 0.9560\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1460 - accuracy: 0.9520\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1023 - accuracy: 0.9660\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1039 - accuracy: 0.9670\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0838 - accuracy: 0.9720\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0828 - accuracy: 0.9690\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0771 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0658 - accuracy: 0.9800\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0656 - accuracy: 0.9740\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0560 - accuracy: 0.9840\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0838 - accuracy: 0.9710\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0500 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0649 - accuracy: 0.9800\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0371 - accuracy: 0.9870\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0441 - accuracy: 0.9850\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0351 - accuracy: 0.9870\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0361 - accuracy: 0.9880\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0308 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0406 - accuracy: 0.9840\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0299 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9950\n",
            "Accuracy:  0.9950000047683716\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0018995418\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.400% | global_loss: 0.0018995419377461076\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11460927\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.1146092563867569\n",
            "tf.Tensor(0.0018995419, shape=(), dtype=float32) 0.104\n",
            "tf.Tensor(0.11460926, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5479 - accuracy: 0.4540\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5823 - accuracy: 0.8220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3544 - accuracy: 0.8740\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2838 - accuracy: 0.9130\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2176 - accuracy: 0.9240\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1844 - accuracy: 0.9420\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1632 - accuracy: 0.9520\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1204 - accuracy: 0.9610\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1221 - accuracy: 0.9600\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1005 - accuracy: 0.9670\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1005 - accuracy: 0.9670\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0733 - accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0891 - accuracy: 0.9710\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0661 - accuracy: 0.9780\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0659 - accuracy: 0.9770\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 0.9820\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0468 - accuracy: 0.9820\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9840\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0504 - accuracy: 0.9810\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0487 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0502 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0408 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0468 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9870\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0281 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0371 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0519 - accuracy: 0.9800\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0337 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0185 - accuracy: 0.9950\n",
            "Accuracy:  0.9950000047683716\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014340468\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.0014340467751026154\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.092830144\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.09283013641834259\n",
            "tf.Tensor(0.0014340468, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.09283014, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5053 - accuracy: 0.4730\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5141 - accuracy: 0.8190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2957 - accuracy: 0.8980\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2119 - accuracy: 0.9350\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1775 - accuracy: 0.9400\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1345 - accuracy: 0.9550\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1486 - accuracy: 0.9460\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1215 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1029 - accuracy: 0.9550\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0807 - accuracy: 0.9730\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1107 - accuracy: 0.9730\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0955 - accuracy: 0.9660\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0776 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0641 - accuracy: 0.9760\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0581 - accuracy: 0.9860\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0643 - accuracy: 0.9830\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9860\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9880\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0439 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0509 - accuracy: 0.9830\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0474 - accuracy: 0.9880\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9850\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0442 - accuracy: 0.9860\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0587 - accuracy: 0.9820\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0446 - accuracy: 0.9850\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0238 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0207 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0417 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0323 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0014861961\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.900% | global_loss: 0.0014861960662528872\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08829654\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.08829653263092041\n",
            "tf.Tensor(0.0014861961, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.08829653, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.6003 - accuracy: 0.4620\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5737 - accuracy: 0.8090\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3223 - accuracy: 0.9040\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2964 - accuracy: 0.9120\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1867 - accuracy: 0.9350\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1534 - accuracy: 0.9550\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1471 - accuracy: 0.9520\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1302 - accuracy: 0.9550\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1381 - accuracy: 0.9500\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1166 - accuracy: 0.9610\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1043 - accuracy: 0.9600\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0831 - accuracy: 0.9710\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0793 - accuracy: 0.9750\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0674 - accuracy: 0.9770\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0722 - accuracy: 0.9770\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0639 - accuracy: 0.9780\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0582 - accuracy: 0.9800\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0416 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0451 - accuracy: 0.9850\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9810\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0487 - accuracy: 0.9840\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0432 - accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0278 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0452 - accuracy: 0.9790\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0231 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0375 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0176 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.000998668\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.800% | global_loss: 0.0009986680233851075\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.095069274\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.0950692817568779\n",
            "tf.Tensor(0.000998668, shape=(), dtype=float32) 0.098\n",
            "tf.Tensor(0.09506928, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5545 - accuracy: 0.4730\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5217 - accuracy: 0.8400\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3527 - accuracy: 0.8920\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2734 - accuracy: 0.9180\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1971 - accuracy: 0.9490\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1731 - accuracy: 0.9380\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1374 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1426 - accuracy: 0.9500\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1302 - accuracy: 0.9580\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1220 - accuracy: 0.9630\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1050 - accuracy: 0.9650\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0747 - accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9820\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0828 - accuracy: 0.9660\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0657 - accuracy: 0.9750\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0644 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0525 - accuracy: 0.9840\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0348 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0326 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9820\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0366 - accuracy: 0.9840\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0412 - accuracy: 0.9870\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0409 - accuracy: 0.9840\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0229 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0225 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0408 - accuracy: 0.9870\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0433 - accuracy: 0.9890\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0295 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0401 - accuracy: 0.9860\n",
            "Accuracy:  0.9860000014305115\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0015814764\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.000% | global_loss: 0.0015814764192327857\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.113532394\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.000% | global_loss: 0.11353239417076111\n",
            "tf.Tensor(0.0015814764, shape=(), dtype=float32) 0.1\n",
            "tf.Tensor(0.113532394, shape=(), dtype=float32) 0.09\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5516 - accuracy: 0.4710\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5853 - accuracy: 0.8070\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3831 - accuracy: 0.8870\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2859 - accuracy: 0.9020\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2327 - accuracy: 0.9270\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1798 - accuracy: 0.9430\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1733 - accuracy: 0.9490\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1682 - accuracy: 0.9370\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1549 - accuracy: 0.9500\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1386 - accuracy: 0.9530\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1153 - accuracy: 0.9650\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0881 - accuracy: 0.9700\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0833 - accuracy: 0.9750\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0880 - accuracy: 0.9690\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0725 - accuracy: 0.9760\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0721 - accuracy: 0.9730\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0607 - accuracy: 0.9810\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0680 - accuracy: 0.9720\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0772 - accuracy: 0.9750\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0636 - accuracy: 0.9790\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0610 - accuracy: 0.9770\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9880\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0484 - accuracy: 0.9850\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0545 - accuracy: 0.9780\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0401 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0325 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0274 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0414 - accuracy: 0.9840\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0423 - accuracy: 0.9860\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0468 - accuracy: 0.9820\n",
            "Accuracy:  0.9819999933242798\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0021480403\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.002148040570318699\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.102023475\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.600% | global_loss: 0.10202346742153168\n",
            "tf.Tensor(0.0021480406, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.10202347, shape=(), dtype=float32) 0.086\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 1.4736 - accuracy: 0.4840\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4721 - accuracy: 0.8440\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3043 - accuracy: 0.8940\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2203 - accuracy: 0.9370\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1780 - accuracy: 0.9400\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1673 - accuracy: 0.9440\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1398 - accuracy: 0.9490\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9590\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1189 - accuracy: 0.9600\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0922 - accuracy: 0.9710\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0633 - accuracy: 0.9800\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0848 - accuracy: 0.9700\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0800 - accuracy: 0.9740\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0704 - accuracy: 0.9790\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0616 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0560 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0447 - accuracy: 0.9840\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0431 - accuracy: 0.9880\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0414 - accuracy: 0.9880\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0595 - accuracy: 0.9780\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0423 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0361 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0392 - accuracy: 0.9860\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0458 - accuracy: 0.9840\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0235 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0240 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0266 - accuracy: 0.9900\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0250 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0229 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013502645\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.300% | global_loss: 0.0013502645306289196\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09647093\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.0964709222316742\n",
            "tf.Tensor(0.0013502645, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.09647092, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.4071 - accuracy: 0.5080\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4682 - accuracy: 0.8530\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2911 - accuracy: 0.8960\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2314 - accuracy: 0.9270\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1723 - accuracy: 0.9390\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1440 - accuracy: 0.9530\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1353 - accuracy: 0.9530\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1176 - accuracy: 0.9600\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0923 - accuracy: 0.9680\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1069 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0819 - accuracy: 0.9740\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1096 - accuracy: 0.9610\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0886 - accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0660 - accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0569 - accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0468 - accuracy: 0.9840\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0435 - accuracy: 0.9850\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0519 - accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0561 - accuracy: 0.9810\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0352 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9870\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0304 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0321 - accuracy: 0.9890\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0260 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0466 - accuracy: 0.9810\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0226 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0232 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0255 - accuracy: 0.9900\n",
            "Accuracy:  0.9900000095367432\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0012084183\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.900% | global_loss: 0.0012084183981642127\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.097271934\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 8.800% | global_loss: 0.09727193415164948\n",
            "tf.Tensor(0.0012084184, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.097271934, shape=(), dtype=float32) 0.088\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.5329 - accuracy: 0.4790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.5280 - accuracy: 0.8290\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3405 - accuracy: 0.8920\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2856 - accuracy: 0.9170\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2293 - accuracy: 0.9320\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2467 - accuracy: 0.9130\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1589 - accuracy: 0.9470\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9560\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9710\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1006 - accuracy: 0.9620\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1006 - accuracy: 0.9690\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0819 - accuracy: 0.9740\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0716 - accuracy: 0.9760\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0750 - accuracy: 0.9670\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0576 - accuracy: 0.9820\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0720 - accuracy: 0.9760\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0480 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0608 - accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0532 - accuracy: 0.9830\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0434 - accuracy: 0.9860\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0410 - accuracy: 0.9860\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0559 - accuracy: 0.9830\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0461 - accuracy: 0.9870\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0466 - accuracy: 0.9810\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0333 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0320 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0281 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0232 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0229 - accuracy: 0.9900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0254 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0013904884\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.500% | global_loss: 0.0013904883526265621\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09155808\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.400% | global_loss: 0.0915580615401268\n",
            "tf.Tensor(0.0013904884, shape=(), dtype=float32) 0.105\n",
            "tf.Tensor(0.09155806, shape=(), dtype=float32) 0.094\n",
            "bestclientid 38\n",
            "worstclientid 36\n",
            "[2.30044991e-01 5.42404175e-01 9.71515081e-04 1.22951262e-01\n",
            " 1.66758359e-03 1.39869645e-01 2.32777791e-03 1.08340092e-01\n",
            " 1.46240834e-03 1.33339137e-01 5.81804156e-01 1.17267108e+00\n",
            " 1.84764492e-03 1.40975505e-01 1.46644318e-03 1.04663394e-01\n",
            " 2.03301292e-03 9.01773944e-02 2.30285549e-03 1.40530184e-01\n",
            " 1.33665849e-03 1.29717305e-01 9.88226500e-04 6.65849373e-02\n",
            " 1.49504666e-03 1.02301449e-01 3.71725997e-03 1.50160372e-01\n",
            " 1.72728172e-03 1.23024382e-01 1.51466089e-03 1.43655881e-01\n",
            " 3.85890040e-03 1.06256731e-01 1.44003623e-03 1.05209835e-01\n",
            " 1.31064339e-03 1.14319883e-01 1.17748254e-03 1.14851855e-01\n",
            " 1.30816340e-03 8.96050408e-02 1.98213314e-03 1.29131287e-01\n",
            " 2.83237034e-03 1.11988924e-01 2.13197991e-03 1.28067821e-01\n",
            " 2.56070565e-03 1.65614203e-01 1.13455510e+00 1.42817354e+00\n",
            " 1.84903992e-03 1.19819641e-01 2.18917662e-03 6.56836107e-02\n",
            " 1.20134489e-03 1.93165526e-01 3.15617840e-03 8.49198699e-02\n",
            " 2.76750349e-03 1.35796577e-01 1.42734486e-03 1.26921460e-01\n",
            " 2.22221576e-03 1.30566254e-01 1.07880135e-03 1.05678596e-01\n",
            " 1.49956520e-03 1.66616872e-01 1.11529487e-03 1.00195527e-01\n",
            " 1.75234032e+00 1.86122322e+00 2.62852898e-03 1.28214508e-01\n",
            " 4.50521009e-03 6.46417364e-02 3.94126447e-03 1.20158151e-01\n",
            " 1.31535262e-03 7.90221691e-02 1.95413269e-03 1.46811694e-01\n",
            " 1.41242170e-03 8.70541111e-02 2.13592779e-03 1.11746721e-01\n",
            " 2.36069784e-03 1.35334313e-01 1.03227235e-03 8.53065774e-02\n",
            " 1.58659043e-03 1.85128838e-01 1.65297021e-03 6.65472299e-02\n",
            " 2.64419126e-03 1.02266833e-01 1.66105817e-03 1.27766490e-01\n",
            " 1.25603657e-03 1.05960697e-01 1.89954182e-03 1.14609271e-01\n",
            " 1.43404678e-03 9.28301439e-02 1.48619607e-03 8.82965401e-02\n",
            " 9.98668023e-04 9.50692743e-02 1.58147642e-03 1.13532394e-01\n",
            " 2.14804034e-03 1.02023475e-01 1.35026453e-03 9.64709297e-02\n",
            " 1.20841828e-03 9.72719342e-02 1.39048835e-03 9.15580764e-02]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:142: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i 0.0011 \t 0.9313333333333333\n",
            "i 0.0012000000000000001 \t 0.898\n",
            "i 0.0013000000000000002 \t 0.848\n",
            "i 0.0014000000000000002 \t 0.748\n",
            "i 0.0015000000000000002 \t 0.598\n",
            "i 0.0016000000000000003 \t 0.548\n",
            "i 0.0017000000000000003 \t 0.498\n",
            "i 0.0018000000000000004 \t 0.48133333333333334\n",
            "i 0.0019000000000000004 \t 0.43133333333333335\n",
            "i 0.0020000000000000005 \t 0.398\n",
            "i 0.0021000000000000003 \t 0.38133333333333336\n",
            "i 0.0022 \t 0.31466666666666665\n",
            "i 0.0023 \t 0.298\n",
            "i 0.0024 \t 0.248\n",
            "i 0.0024999999999999996 \t 0.248\n",
            "i 0.0025999999999999994 \t 0.23133333333333334\n",
            "i 0.0026999999999999993 \t 0.198\n",
            "i 0.002799999999999999 \t 0.18133333333333335\n",
            "i 0.002899999999999999 \t 0.16466666666666666\n",
            "i 0.0029999999999999988 \t 0.16466666666666666\n",
            "i 0.0030999999999999986 \t 0.16466666666666666\n",
            "i 0.0031999999999999984 \t 0.148\n",
            "i 0.0032999999999999982 \t 0.148\n",
            "i 0.003399999999999998 \t 0.148\n",
            "i 0.003499999999999998 \t 0.148\n",
            "i 0.0035999999999999977 \t 0.148\n",
            "i 0.0036999999999999976 \t 0.148\n",
            "i 0.0037999999999999974 \t 0.13133333333333333\n",
            "i 0.0038999999999999972 \t 0.11466666666666667\n",
            "i 0.0039999999999999975 \t 0.11666666666666667\n",
            "starlambda 0.0038999999999999972\n",
            "clienttrainingloss [0.23004499, 0.0009715151, 0.0016675836, 0.002327778, 0.0014624083, 0.58180416, 0.0018476449, 0.0014664432, 0.002033013, 0.0023028555, 0.0013366585, 0.0009882265, 0.0014950467, 0.00371726, 0.0017272817, 0.0015146609, 0.0038589004, 0.0014400362, 0.0013106434, 0.0011774825, 0.0013081634, 0.0019821331, 0.0028323703, 0.00213198, 0.0025607056, 1.1345551, 0.0018490399, 0.0021891766, 0.0012013449, 0.0031561784, 0.0027675035, 0.0014273449, 0.0022222158, 0.0010788013, 0.0014995652, 0.0011152949, 1.7523403, 0.002628529, 0.00450521, 0.0039412645, 0.0013153526, 0.0019541327, 0.0014124217, 0.0021359278, 0.0023606978, 0.0010322724, 0.0015865904, 0.0016529702, 0.0026441913, 0.0016610582, 0.0012560366, 0.0018995418, 0.0014340468, 0.0014861961, 0.000998668, 0.0015814764, 0.0021480403, 0.0013502645, 0.0012084183, 0.0013904884]\n",
            "x = 0.0301537335723510\n",
            "y = 0.00914737497180574\n",
            "[0.0001567181360585049, 0.030153733572350978, 0.017567220022388873, 0.012584880700569782, 0.02003189339936448, 6.196625518087237e-05, 0.015855213829914845, 0.019976775521197847, 0.014409552602462846, 0.012721079391296896, 0.021916447828559067, 0.02964381906057827, 0.019594577330634286, 0.007880753034045498, 0.016960062454620936, 0.019340835949942105, 0.007591490663848435, 0.02034310352999562, 0.022351470539199437, 0.024879186101409104, 0.02239384374478383, 0.014779434777109073, 0.010342858964288102, 0.013740657691132648, 0.011440130161324369, 3.177652944286019e-05, 0.01584325278125671, 0.013381655487643748, 0.02438501056831397, 0.009281733319402654, 0.010585282730645157, 0.02052398585877144, 0.013182701281443806, 0.027154959382807396, 0.019535534143636794, 0.02626642331260181, 2.0573757306094863e-05, 0.011144943881829452, 0.008002340321716058, 0.009147374971805736, 0.022271447439206248, 0.014991206446645675, 0.02074083580674284, 0.013715260907050945, 0.012409383988969631, 0.02837895119542447, 0.018464000319520807, 0.01772252493817388, 0.011078928958131, 0.017636231049485238, 0.023323210934961685, 0.015422038746324897, 0.020428069212078948, 0.019711267200525982, 0.02933387962286994, 0.018523708508472243, 0.013637922078458342, 0.02169560607156142, 0.02424227418945854, 0.021067998795058364]\n",
            "weightage of clients [0.0001567181360585049, 0.030153733572350978, 0.017567220022388873, 0.012584880700569782, 0.02003189339936448, 6.196625518087237e-05, 0.015855213829914845, 0.019976775521197847, 0.014409552602462846, 0.012721079391296896, 0.021916447828559067, 0.02964381906057827, 0.019594577330634286, 0.007880753034045498, 0.016960062454620936, 0.019340835949942105, 0.007591490663848435, 0.02034310352999562, 0.022351470539199437, 0.024879186101409104, 0.02239384374478383, 0.014779434777109073, 0.010342858964288102, 0.013740657691132648, 0.011440130161324369, 3.177652944286019e-05, 0.01584325278125671, 0.013381655487643748, 0.02438501056831397, 0.009281733319402654, 0.010585282730645157, 0.02052398585877144, 0.013182701281443806, 0.027154959382807396, 0.019535534143636794, 0.02626642331260181, 2.0573757306094863e-05, 0.011144943881829452, 0.008002340321716058, 0.009147374971805736, 0.022271447439206248, 0.014991206446645675, 0.02074083580674284, 0.013715260907050945, 0.012409383988969631, 0.02837895119542447, 0.018464000319520807, 0.01772252493817388, 0.011078928958131, 0.017636231049485238, 0.023323210934961685, 0.015422038746324897, 0.020428069212078948, 0.019711267200525982, 0.02933387962286994, 0.018523708508472243, 0.013637922078458342, 0.02169560607156142, 0.02424227418945854, 0.021067998795058364]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(60000, 1) (60000, 10)\n",
            "lossindiv\n",
            "1.089535\n",
            "logits\n",
            "size (60000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 9.987% | global_loss: 1.0895349979400635\n",
            "sparse_categorical_crossentropy tf.Tensor(1.089535, shape=(), dtype=float32) 0.09986666666666667\n",
            "(10000, 1) (10000, 10)\n",
            "lossindiv\n",
            "1.0672373\n",
            "logits\n",
            "size (10000, 10)\n",
            "loss\n",
            "comm_round: 0 | global_acc: 10.100% | global_loss: 1.0672372579574585\n",
            "tf.Tensor(1.0672373, shape=(), dtype=float32) 0.101\n",
            "hello (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.8457 - accuracy: 0.3540\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.4220 - accuracy: 0.5050\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3183 - accuracy: 0.5560\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1852 - accuracy: 0.5940\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.0760 - accuracy: 0.6260\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0252 - accuracy: 0.6410\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9645 - accuracy: 0.6620\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.9488 - accuracy: 0.6660\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8921 - accuracy: 0.6960\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7554 - accuracy: 0.7370\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8099 - accuracy: 0.7120\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7785 - accuracy: 0.7320\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6754 - accuracy: 0.7670\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6844 - accuracy: 0.7600\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6101 - accuracy: 0.7780\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6462 - accuracy: 0.7800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6037 - accuracy: 0.7850\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.5922 - accuracy: 0.7880\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6024 - accuracy: 0.7720\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5597 - accuracy: 0.7950\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5361 - accuracy: 0.8100\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5343 - accuracy: 0.7970\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4797 - accuracy: 0.8340\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4854 - accuracy: 0.8280\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4877 - accuracy: 0.8320\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4574 - accuracy: 0.8340\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4550 - accuracy: 0.8440\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4109 - accuracy: 0.8440\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4468 - accuracy: 0.8270\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4458 - accuracy: 0.8370\n",
            "Accuracy:  0.8370000123977661\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.12017279\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.100% | global_loss: 0.12017279118299484\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.410258\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.600% | global_loss: 0.4102579653263092\n",
            "tf.Tensor(0.12017279, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.41025797, shape=(), dtype=float32) 0.096\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4212 - accuracy: 0.8830\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1964 - accuracy: 0.9360\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1298 - accuracy: 0.9550\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0897 - accuracy: 0.9660\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0844 - accuracy: 0.9770\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0736 - accuracy: 0.9810\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0438 - accuracy: 0.9870\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 0.9870\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0278 - accuracy: 0.9890\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0271 - accuracy: 0.9900\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0326 - accuracy: 0.9910\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0292 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0188 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0382 - accuracy: 0.9860\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0210 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0192 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0135 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0196 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 0.9890\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0211 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0086 - accuracy: 0.9980\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0094 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0137 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0127 - accuracy: 0.9970\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0111 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0076 - accuracy: 0.9990\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0096 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00017192101\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.900% | global_loss: 0.00017192101222462952\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12548038\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.12548038363456726\n",
            "tf.Tensor(0.00017192101, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.12548038, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4870 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2282 - accuracy: 0.9390\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1398 - accuracy: 0.9610\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1121 - accuracy: 0.9570\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0881 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9820\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0582 - accuracy: 0.9840\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0640 - accuracy: 0.9750\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0607 - accuracy: 0.9800\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0357 - accuracy: 0.9910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0454 - accuracy: 0.9840\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0248 - accuracy: 0.9930\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9820\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0261 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0218 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0179 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0099 - accuracy: 0.9990\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0248 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0171 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0133 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0091 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0217 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0097 - accuracy: 0.9970\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0091 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0144 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00037628182\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0003762818523682654\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12506233\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.12506231665611267\n",
            "tf.Tensor(0.00037628185, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.12506232, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5154 - accuracy: 0.8720\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2263 - accuracy: 0.9260\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1911 - accuracy: 0.9400\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1445 - accuracy: 0.9420\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1028 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0699 - accuracy: 0.9770\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0793 - accuracy: 0.9690\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0710 - accuracy: 0.9770\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0584 - accuracy: 0.9810\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 0.9890\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0262 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0323 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0289 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9910\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0271 - accuracy: 0.9910\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0211 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0320 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0252 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0144 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0193 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0243 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0095 - accuracy: 0.9990\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0104 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0139 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00049429154\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.000% | global_loss: 0.0004942914820276201\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11650343\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.11650341004133224\n",
            "tf.Tensor(0.0004942915, shape=(), dtype=float32) 0.11\n",
            "tf.Tensor(0.11650341, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5168 - accuracy: 0.8580\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2399 - accuracy: 0.9230\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1827 - accuracy: 0.9350\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1199 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0969 - accuracy: 0.9660\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0879 - accuracy: 0.9720\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0872 - accuracy: 0.9680\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0602 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0438 - accuracy: 0.9880\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0450 - accuracy: 0.9850\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0338 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0439 - accuracy: 0.9860\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9870\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 0.9890\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0386 - accuracy: 0.9870\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0223 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0197 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0260 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0301 - accuracy: 0.9900\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0210 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0178 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0142 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0005910607\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.600% | global_loss: 0.0005910608451813459\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13848855\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.1384885460138321\n",
            "tf.Tensor(0.00059106085, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.13848855, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.7346 - accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.3013 - accuracy: 0.5640\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0966 - accuracy: 0.6370\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.9859 - accuracy: 0.6720\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8853 - accuracy: 0.6800\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7837 - accuracy: 0.7380\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7474 - accuracy: 0.7500\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7317 - accuracy: 0.7340\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6823 - accuracy: 0.7650\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6078 - accuracy: 0.7920\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5992 - accuracy: 0.7900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5951 - accuracy: 0.7980\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5802 - accuracy: 0.7980\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5246 - accuracy: 0.8250\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4895 - accuracy: 0.8360\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4247 - accuracy: 0.8560\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4385 - accuracy: 0.8510\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4392 - accuracy: 0.8660\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4537 - accuracy: 0.8460\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4493 - accuracy: 0.8540\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4294 - accuracy: 0.8430\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3880 - accuracy: 0.8610\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3559 - accuracy: 0.8800\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3914 - accuracy: 0.8750\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3784 - accuracy: 0.8660\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3616 - accuracy: 0.8890\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3353 - accuracy: 0.8730\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3176 - accuracy: 0.8850\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3312 - accuracy: 0.8900\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3030 - accuracy: 0.9020\n",
            "Accuracy:  0.9020000100135803\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.07668616\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.500% | global_loss: 0.07668615877628326\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.38116476\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.800% | global_loss: 0.3811647295951843\n",
            "tf.Tensor(0.07668616, shape=(), dtype=float32) 0.115\n",
            "tf.Tensor(0.38116473, shape=(), dtype=float32) 0.108\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.5570 - accuracy: 0.8530\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2092 - accuracy: 0.9310\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1360 - accuracy: 0.9470\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0835 - accuracy: 0.9730\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9700\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0670 - accuracy: 0.9750\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0601 - accuracy: 0.9810\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 0.9900\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0358 - accuracy: 0.9900\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0601 - accuracy: 0.9790\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0321 - accuracy: 0.9920\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0243 - accuracy: 0.9930\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0273 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 0.9840\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0290 - accuracy: 0.9890\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0250 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0204 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0247 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0211 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0126 - accuracy: 0.9980\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0239 - accuracy: 0.9900\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0288 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0138 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0106 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0143 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0102 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0099 - accuracy: 0.9980\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00044590645\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.200% | global_loss: 0.0004459064803086221\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11050204\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.11050203442573547\n",
            "tf.Tensor(0.00044590648, shape=(), dtype=float32) 0.082\n",
            "tf.Tensor(0.110502034, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5102 - accuracy: 0.8550\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2389 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1607 - accuracy: 0.9460\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1484 - accuracy: 0.9520\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1077 - accuracy: 0.9660\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0951 - accuracy: 0.9680\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9850\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0706 - accuracy: 0.9790\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0619 - accuracy: 0.9770\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0541 - accuracy: 0.9850\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0412 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0315 - accuracy: 0.9940\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0227 - accuracy: 0.9920\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0236 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9870\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0273 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0259 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0216 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0234 - accuracy: 0.9900\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0188 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0079 - accuracy: 0.9990\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0112 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.003160494\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.100% | global_loss: 0.00316049437969923\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.19216107\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.19216106832027435\n",
            "tf.Tensor(0.0031604944, shape=(), dtype=float32) 0.091\n",
            "tf.Tensor(0.19216107, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4610 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2407 - accuracy: 0.9320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1363 - accuracy: 0.9590\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1187 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0910 - accuracy: 0.9720\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0682 - accuracy: 0.9790\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9840\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0437 - accuracy: 0.9850\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0684 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9830\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0587 - accuracy: 0.9810\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0278 - accuracy: 0.9910\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9920\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0220 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0351 - accuracy: 0.9890\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0305 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0289 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0278 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0285 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0223 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0241 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0237 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0294 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0194 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0285 - accuracy: 0.9900\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0138 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0107 - accuracy: 0.9980\n",
            "Accuracy:  0.9980000257492065\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00038196548\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.400% | global_loss: 0.00038196551031433046\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09523876\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.09523875266313553\n",
            "tf.Tensor(0.0003819655, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.09523875, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5072 - accuracy: 0.8730\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2578 - accuracy: 0.9190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1762 - accuracy: 0.9440\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1374 - accuracy: 0.9580\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0886 - accuracy: 0.9710\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0964 - accuracy: 0.9700\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0663 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0557 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0489 - accuracy: 0.9850\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0406 - accuracy: 0.9890\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0403 - accuracy: 0.9860\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0173 - accuracy: 0.9950\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9830\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0197 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9890\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0169 - accuracy: 0.9970\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0200 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0203 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0222 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0149 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0066 - accuracy: 0.9990\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0095 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0296 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00081810117\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0008181011653505266\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14267193\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.14267192780971527\n",
            "tf.Tensor(0.00081810117, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.14267193, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5173 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2206 - accuracy: 0.9310\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1454 - accuracy: 0.9570\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1217 - accuracy: 0.9580\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0940 - accuracy: 0.9670\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0835 - accuracy: 0.9680\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9850\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0567 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 0.9810\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9830\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9810\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0355 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0290 - accuracy: 0.9900\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0165 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0116 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0103 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0153 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0109 - accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0137 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0189 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0008329283\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.100% | global_loss: 0.0008329284028150141\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13182962\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.1318296194076538\n",
            "tf.Tensor(0.0008329284, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.13182962, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4408 - accuracy: 0.8740\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2556 - accuracy: 0.9190\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1742 - accuracy: 0.9480\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1558 - accuracy: 0.9530\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1011 - accuracy: 0.9630\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0957 - accuracy: 0.9680\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0665 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0590 - accuracy: 0.9810\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0478 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0645 - accuracy: 0.9810\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0555 - accuracy: 0.9810\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9840\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0285 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0505 - accuracy: 0.9850\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9960\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0276 - accuracy: 0.9930\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0252 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.9970\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0179 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0231 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 0.9890\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0229 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0172 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.9980\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0137 - accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0200 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00038756104\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.000% | global_loss: 0.00038756104186177254\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.051346477\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.05134647339582443\n",
            "tf.Tensor(0.00038756104, shape=(), dtype=float32) 0.08\n",
            "tf.Tensor(0.051346473, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5132 - accuracy: 0.8780\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2242 - accuracy: 0.9280\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1638 - accuracy: 0.9460\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1121 - accuracy: 0.9620\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1079 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0663 - accuracy: 0.9770\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0640 - accuracy: 0.9800\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0745 - accuracy: 0.9780\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0506 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0237 - accuracy: 0.9930\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0323 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0216 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9900\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0246 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0262 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0079 - accuracy: 0.9990\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0127 - accuracy: 0.9970\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0098 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0114 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0149 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0110 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0161 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00035530253\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.500% | global_loss: 0.00035530253080651164\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09829687\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.0982968807220459\n",
            "tf.Tensor(0.00035530253, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.09829688, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5828 - accuracy: 0.8480\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2487 - accuracy: 0.9270\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1947 - accuracy: 0.9360\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1714 - accuracy: 0.9470\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9630\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0963 - accuracy: 0.9760\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0893 - accuracy: 0.9700\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0700 - accuracy: 0.9720\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0563 - accuracy: 0.9840\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9790\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0467 - accuracy: 0.9850\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0300 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0312 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0322 - accuracy: 0.9880\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0205 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0260 - accuracy: 0.9900\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0203 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0253 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0164 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0408 - accuracy: 0.9840\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0084 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0091 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0211 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0005737948\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.400% | global_loss: 0.0005737948231399059\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.118643254\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.11864324659109116\n",
            "tf.Tensor(0.0005737948, shape=(), dtype=float32) 0.114\n",
            "tf.Tensor(0.11864325, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4563 - accuracy: 0.8820\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2645 - accuracy: 0.9160\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1551 - accuracy: 0.9510\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1408 - accuracy: 0.9630\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1106 - accuracy: 0.9620\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0787 - accuracy: 0.9740\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0703 - accuracy: 0.9760\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9840\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0365 - accuracy: 0.9880\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0413 - accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0558 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9830\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 0.9800\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0403 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0275 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0169 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0242 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0237 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0259 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0200 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0167 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0097 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0097 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0143 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0152 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0183 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0006505332\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.100% | global_loss: 0.0006505331839434803\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12157091\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.400% | global_loss: 0.1215709000825882\n",
            "tf.Tensor(0.0006505332, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.1215709, shape=(), dtype=float32) 0.094\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5036 - accuracy: 0.8710\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2340 - accuracy: 0.9310\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1821 - accuracy: 0.9370\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1562 - accuracy: 0.9530\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0934 - accuracy: 0.9660\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0820 - accuracy: 0.9770\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0724 - accuracy: 0.9800\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9840\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0463 - accuracy: 0.9830\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0349 - accuracy: 0.9900\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9850\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9840\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0237 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0401 - accuracy: 0.9860\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0326 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0505 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0225 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0184 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0180 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0306 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0232 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0227 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0166 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0126 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0088 - accuracy: 0.9990\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0092 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0238 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0126 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00040325982\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.900% | global_loss: 0.0004032598517369479\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14278857\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.1427885890007019\n",
            "tf.Tensor(0.00040325985, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.14278859, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4718 - accuracy: 0.8810\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2098 - accuracy: 0.9330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2172 - accuracy: 0.9380\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1170 - accuracy: 0.9620\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0955 - accuracy: 0.9660\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0920 - accuracy: 0.9660\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0661 - accuracy: 0.9790\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0780 - accuracy: 0.9700\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0769 - accuracy: 0.9790\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9850\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0416 - accuracy: 0.9860\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9900\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0418 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0336 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0287 - accuracy: 0.9910\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0225 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0204 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0181 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0188 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0128 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0111 - accuracy: 0.9980\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0182 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0085 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0114 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0177 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0153 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00024068441\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.200% | global_loss: 0.00024068441416602582\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08976214\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.08976215124130249\n",
            "tf.Tensor(0.00024068441, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.08976215, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4830 - accuracy: 0.8750\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2594 - accuracy: 0.9160\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1773 - accuracy: 0.9400\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9550\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0883 - accuracy: 0.9740\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0919 - accuracy: 0.9660\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0620 - accuracy: 0.9820\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0657 - accuracy: 0.9810\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0565 - accuracy: 0.9830\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9770\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0408 - accuracy: 0.9830\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0423 - accuracy: 0.9860\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0396 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0388 - accuracy: 0.9840\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0322 - accuracy: 0.9900\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0253 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0215 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0159 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0257 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0304 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0180 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0145 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0130 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0168 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0213 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9950\n",
            "Accuracy:  0.9950000047683716\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00055219163\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.900% | global_loss: 0.0005521916900761425\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11668665\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.11668665707111359\n",
            "tf.Tensor(0.0005521917, shape=(), dtype=float32) 0.109\n",
            "tf.Tensor(0.11668666, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5269 - accuracy: 0.8790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2425 - accuracy: 0.9250\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1477 - accuracy: 0.9520\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1094 - accuracy: 0.9640\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1469 - accuracy: 0.9500\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0959 - accuracy: 0.9690\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0633 - accuracy: 0.9770\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0599 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0506 - accuracy: 0.9810\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9810\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0301 - accuracy: 0.9910\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0435 - accuracy: 0.9840\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0356 - accuracy: 0.9900\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0206 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0310 - accuracy: 0.9910\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0224 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.9980\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0250 - accuracy: 0.9900\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0253 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0101 - accuracy: 0.9980\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0214 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0149 - accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0149 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0108 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00039725803\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 7.500% | global_loss: 0.0003972580307163298\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09954059\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.0995405986905098\n",
            "tf.Tensor(0.00039725803, shape=(), dtype=float32) 0.075\n",
            "tf.Tensor(0.0995406, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4531 - accuracy: 0.8820\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2254 - accuracy: 0.9350\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1883 - accuracy: 0.9310\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1323 - accuracy: 0.9530\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0861 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0905 - accuracy: 0.9670\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0621 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0720 - accuracy: 0.9700\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9880\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0500 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 0.9910\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0504 - accuracy: 0.9810\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0369 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0580 - accuracy: 0.9800\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0360 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0199 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0249 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0161 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0113 - accuracy: 0.9990\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0208 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0066 - accuracy: 0.9980\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0121 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0127 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0113 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0095 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00040287004\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.300% | global_loss: 0.00040287000592797995\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09258896\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.09258894622325897\n",
            "tf.Tensor(0.00040287, shape=(), dtype=float32) 0.083\n",
            "tf.Tensor(0.092588946, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 0.5437 - accuracy: 0.8680\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2358 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1669 - accuracy: 0.9460\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1173 - accuracy: 0.9630\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1141 - accuracy: 0.9610\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0994 - accuracy: 0.9710\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0727 - accuracy: 0.9770\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0691 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0433 - accuracy: 0.9900\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0618 - accuracy: 0.9770\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0467 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0277 - accuracy: 0.9940\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9870\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9940\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0268 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0248 - accuracy: 0.9940\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0193 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9980\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0168 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0138 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0194 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0169 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0172 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0199 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0003496115\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.000% | global_loss: 0.000349611509591341\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.080991805\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.08099179714918137\n",
            "tf.Tensor(0.0003496115, shape=(), dtype=float32) 0.11\n",
            "tf.Tensor(0.0809918, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4933 - accuracy: 0.8890\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2301 - accuracy: 0.9200\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1610 - accuracy: 0.9450\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1358 - accuracy: 0.9470\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1088 - accuracy: 0.9630\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0733 - accuracy: 0.9750\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0524 - accuracy: 0.9810\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0528 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0405 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0233 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9900\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0177 - accuracy: 0.9950\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0254 - accuracy: 0.9900\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0206 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0289 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0092 - accuracy: 0.9970\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0118 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0151 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0119 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0175 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0193 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0054 - accuracy: 0.9990\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00013680494\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.00013680494157597423\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10172205\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.10172205418348312\n",
            "tf.Tensor(0.00013680494, shape=(), dtype=float32) 0.09\n",
            "tf.Tensor(0.101722054, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4884 - accuracy: 0.8770\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2067 - accuracy: 0.9320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1391 - accuracy: 0.9610\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1197 - accuracy: 0.9570\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0873 - accuracy: 0.9750\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0735 - accuracy: 0.9780\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0910 - accuracy: 0.9690\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0539 - accuracy: 0.9780\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0464 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0589 - accuracy: 0.9770\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0422 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0607 - accuracy: 0.9770\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0312 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.9980\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9970\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 0.9970\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0232 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0216 - accuracy: 0.9950\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0101 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0177 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0162 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0090 - accuracy: 0.9980\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00058891583\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.000% | global_loss: 0.0005889158346690238\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09384183\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.09384182840585709\n",
            "tf.Tensor(0.00058891583, shape=(), dtype=float32) 0.1\n",
            "tf.Tensor(0.09384183, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4682 - accuracy: 0.8760\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2380 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1674 - accuracy: 0.9470\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9490\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1115 - accuracy: 0.9650\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1054 - accuracy: 0.9670\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0630 - accuracy: 0.9790\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0557 - accuracy: 0.9840\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0497 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0331 - accuracy: 0.9890\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0346 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0422 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0333 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0175 - accuracy: 0.9970\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0159 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0169 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0124 - accuracy: 0.9970\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9970\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0227 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0211 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0103 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0007931742\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.100% | global_loss: 0.0007931742584332824\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08631516\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.08631515502929688\n",
            "tf.Tensor(0.00079317426, shape=(), dtype=float32) 0.101\n",
            "tf.Tensor(0.086315155, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4673 - accuracy: 0.8780\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2340 - accuracy: 0.9230\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1437 - accuracy: 0.9520\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1273 - accuracy: 0.9520\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0883 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0847 - accuracy: 0.9690\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0588 - accuracy: 0.9860\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0554 - accuracy: 0.9830\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0642 - accuracy: 0.9790\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0552 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0575 - accuracy: 0.9820\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0426 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 0.9930\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9830\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0313 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0216 - accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0289 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9850\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0231 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9970\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0168 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0094 - accuracy: 0.9980\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0131 - accuracy: 0.9970\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0061 - accuracy: 0.9980\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0282 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0331 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0019414879\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.0019414880080148578\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12215137\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.12215137481689453\n",
            "tf.Tensor(0.001941488, shape=(), dtype=float32) 0.092\n",
            "tf.Tensor(0.122151375, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.6913 - accuracy: 0.4150\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.2819 - accuracy: 0.5770\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0864 - accuracy: 0.6450\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.9324 - accuracy: 0.6920\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8945 - accuracy: 0.7030\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7752 - accuracy: 0.7480\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7461 - accuracy: 0.7510\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7398 - accuracy: 0.7520\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6727 - accuracy: 0.7740\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6393 - accuracy: 0.7910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5997 - accuracy: 0.7920\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5759 - accuracy: 0.8040\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5516 - accuracy: 0.8330\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5306 - accuracy: 0.8150\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5023 - accuracy: 0.8160\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4677 - accuracy: 0.8320\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4531 - accuracy: 0.8430\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4332 - accuracy: 0.8590\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4281 - accuracy: 0.8560\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4199 - accuracy: 0.8630\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.4100 - accuracy: 0.8580\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.4095 - accuracy: 0.8540\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3679 - accuracy: 0.8530\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3970 - accuracy: 0.8630\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3501 - accuracy: 0.8740\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3492 - accuracy: 0.8820\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3441 - accuracy: 0.8840\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3016 - accuracy: 0.8910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3367 - accuracy: 0.8820\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3174 - accuracy: 0.8850\n",
            "Accuracy:  0.8849999904632568\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.07402208\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.900% | global_loss: 0.074022077023983\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.3096444\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.3096443712711334\n",
            "tf.Tensor(0.07402208, shape=(), dtype=float32) 0.109\n",
            "tf.Tensor(0.30964437, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4631 - accuracy: 0.8730\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2316 - accuracy: 0.9270\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1621 - accuracy: 0.9470\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1139 - accuracy: 0.9620\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1050 - accuracy: 0.9620\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0730 - accuracy: 0.9780\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0723 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0514 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0490 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0332 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0339 - accuracy: 0.9860\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0232 - accuracy: 0.9950\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0210 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0201 - accuracy: 0.9940\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0252 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0343 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0200 - accuracy: 0.9910\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0240 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0128 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 0.9960\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0125 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0145 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0165 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00043668284\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.600% | global_loss: 0.0004366828943602741\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14286593\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.14286592602729797\n",
            "tf.Tensor(0.0004366829, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.14286593, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4620 - accuracy: 0.8790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2830 - accuracy: 0.9030\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1822 - accuracy: 0.9420\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1291 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0906 - accuracy: 0.9700\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9740\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0910 - accuracy: 0.9760\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0520 - accuracy: 0.9810\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0620 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0578 - accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0314 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9770\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0351 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0322 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0228 - accuracy: 0.9950\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0293 - accuracy: 0.9910\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0184 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0234 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9840\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0299 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0223 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0185 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0174 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0125 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0123 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9870\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00041446215\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.400% | global_loss: 0.00041446209070272744\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.101425216\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.10142521560192108\n",
            "tf.Tensor(0.0004144621, shape=(), dtype=float32) 0.104\n",
            "tf.Tensor(0.101425216, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4956 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2202 - accuracy: 0.9300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1631 - accuracy: 0.9370\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1125 - accuracy: 0.9690\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0918 - accuracy: 0.9710\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0716 - accuracy: 0.9800\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0510 - accuracy: 0.9820\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0791 - accuracy: 0.9710\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0335 - accuracy: 0.9890\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0438 - accuracy: 0.9870\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0416 - accuracy: 0.9870\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9980\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0190 - accuracy: 0.9960\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0132 - accuracy: 0.9970\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0110 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0137 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0116 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0070 - accuracy: 0.9980\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0183 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0167 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0094 - accuracy: 0.9980\n",
            "Accuracy:  0.9980000257492065\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00025445988\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.400% | global_loss: 0.00025445991195738316\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.12410855\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.12410856038331985\n",
            "tf.Tensor(0.0002544599, shape=(), dtype=float32) 0.084\n",
            "tf.Tensor(0.12410856, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4725 - accuracy: 0.8730\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2458 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1521 - accuracy: 0.9540\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1272 - accuracy: 0.9580\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0742 - accuracy: 0.9780\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1181 - accuracy: 0.9640\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0556 - accuracy: 0.9850\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0408 - accuracy: 0.9860\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0403 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9920\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0392 - accuracy: 0.9830\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0262 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9940\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0259 - accuracy: 0.9900\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0510 - accuracy: 0.9790\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0242 - accuracy: 0.9910\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0201 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0204 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0115 - accuracy: 0.9970\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0151 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0171 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0102 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0229 - accuracy: 0.9910\n",
            "Accuracy:  0.9909999966621399\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.000263724\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.700% | global_loss: 0.00026372403954155743\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.04788578\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.047885775566101074\n",
            "tf.Tensor(0.00026372404, shape=(), dtype=float32) 0.097\n",
            "tf.Tensor(0.047885776, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4952 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2534 - accuracy: 0.9180\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1622 - accuracy: 0.9480\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1541 - accuracy: 0.9520\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1102 - accuracy: 0.9680\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0680 - accuracy: 0.9800\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0980 - accuracy: 0.9740\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0662 - accuracy: 0.9760\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0653 - accuracy: 0.9760\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0530 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0466 - accuracy: 0.9850\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0309 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0436 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0227 - accuracy: 0.9930\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0304 - accuracy: 0.9900\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0289 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0503 - accuracy: 0.9820\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0308 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0345 - accuracy: 0.9900\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0140 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0312 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0184 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0247 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9880\n",
            "Accuracy:  0.9879999756813049\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00055604894\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0005560489953495562\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11970756\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.11970754712820053\n",
            "tf.Tensor(0.000556049, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.11970755, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4417 - accuracy: 0.8790\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1877 - accuracy: 0.9340\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1413 - accuracy: 0.9530\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1144 - accuracy: 0.9610\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0976 - accuracy: 0.9610\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0789 - accuracy: 0.9670\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0760 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0753 - accuracy: 0.9760\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0262 - accuracy: 0.9940\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0243 - accuracy: 0.9910\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 0.9930\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0328 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9890\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.9970\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0240 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0230 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0116 - accuracy: 0.9970\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0177 - accuracy: 0.9970\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0144 - accuracy: 0.9980\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 0.9980\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0143 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0146 - accuracy: 0.9970\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0150 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0254 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0008189888\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.800% | global_loss: 0.000818988832179457\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1516461\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.15164610743522644\n",
            "tf.Tensor(0.00081898883, shape=(), dtype=float32) 0.108\n",
            "tf.Tensor(0.1516461, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 0.4681 - accuracy: 0.8750\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2583 - accuracy: 0.9170\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1586 - accuracy: 0.9530\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1119 - accuracy: 0.9630\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1104 - accuracy: 0.9640\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0885 - accuracy: 0.9740\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0658 - accuracy: 0.9750\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9850\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0476 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0477 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0322 - accuracy: 0.9900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0239 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 0.9880\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0291 - accuracy: 0.9890\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0196 - accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0305 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0257 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 0.9870\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0244 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 0.9980\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0146 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0137 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0162 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0092 - accuracy: 0.9980\n",
            "Accuracy:  0.9980000257492065\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00038561606\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.900% | global_loss: 0.0003856160619761795\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1566253\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.15662530064582825\n",
            "tf.Tensor(0.00038561606, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.1566253, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4704 - accuracy: 0.8760\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2477 - accuracy: 0.9150\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1468 - accuracy: 0.9520\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1120 - accuracy: 0.9690\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9840\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0939 - accuracy: 0.9750\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0619 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0398 - accuracy: 0.9880\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9870\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0402 - accuracy: 0.9890\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0370 - accuracy: 0.9900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9840\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0290 - accuracy: 0.9940\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0204 - accuracy: 0.9940\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0174 - accuracy: 0.9930\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 0.9970\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0146 - accuracy: 0.9950\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0396 - accuracy: 0.9900\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0162 - accuracy: 0.9970\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0235 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0186 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0135 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0152 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0183 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0201 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.001183159\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0011831589508801699\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11553862\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.11553861200809479\n",
            "tf.Tensor(0.001183159, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.11553861, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5242 - accuracy: 0.8610\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2139 - accuracy: 0.9360\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9590\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0829 - accuracy: 0.9670\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0834 - accuracy: 0.9720\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0864 - accuracy: 0.9620\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0758 - accuracy: 0.9700\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0611 - accuracy: 0.9740\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0291 - accuracy: 0.9920\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0232 - accuracy: 0.9930\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0342 - accuracy: 0.9920\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0289 - accuracy: 0.9920\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0306 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0251 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0234 - accuracy: 0.9930\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0124 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0098 - accuracy: 0.9980\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0088 - accuracy: 0.9980\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0248 - accuracy: 0.9910\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0190 - accuracy: 0.9910\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0316 - accuracy: 0.9890\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0199 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0193 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0082 - accuracy: 0.9990\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0045 - accuracy: 0.9990\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00029025276\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.100% | global_loss: 0.00029025276307947934\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10359931\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.10359930992126465\n",
            "tf.Tensor(0.00029025276, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.10359931, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.4829 - accuracy: 0.8650\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2296 - accuracy: 0.9240\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1350 - accuracy: 0.9570\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1095 - accuracy: 0.9680\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1331 - accuracy: 0.9600\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0857 - accuracy: 0.9700\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0652 - accuracy: 0.9830\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0656 - accuracy: 0.9750\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0469 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0393 - accuracy: 0.9840\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9830\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 0.9900\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9940\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0277 - accuracy: 0.9930\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0369 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0156 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9900\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0251 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0182 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 0.9880\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0163 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0202 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0209 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0181 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0123 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0086 - accuracy: 0.9990\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0090 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00017726523\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.600% | global_loss: 0.0001772652321960777\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10976433\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.10976433008909225\n",
            "tf.Tensor(0.00017726523, shape=(), dtype=float32) 0.106\n",
            "tf.Tensor(0.10976433, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.8090 - accuracy: 0.3680\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2642 - accuracy: 0.5660\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.1756 - accuracy: 0.5890\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0409 - accuracy: 0.6640\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.9478 - accuracy: 0.6560\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8412 - accuracy: 0.7010\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8005 - accuracy: 0.7220\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7360 - accuracy: 0.7390\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6937 - accuracy: 0.7610\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6998 - accuracy: 0.7550\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6861 - accuracy: 0.7600\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6211 - accuracy: 0.7910\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6031 - accuracy: 0.8070\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5765 - accuracy: 0.8000\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5426 - accuracy: 0.8190\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5083 - accuracy: 0.8230\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4980 - accuracy: 0.8230\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5483 - accuracy: 0.8170\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4488 - accuracy: 0.8430\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4514 - accuracy: 0.8390\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4592 - accuracy: 0.8280\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4345 - accuracy: 0.8500\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4787 - accuracy: 0.8130\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4290 - accuracy: 0.8360\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3759 - accuracy: 0.8700\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4040 - accuracy: 0.8560\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3988 - accuracy: 0.8550\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3947 - accuracy: 0.8660\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3605 - accuracy: 0.8740\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3854 - accuracy: 0.8680\n",
            "Accuracy:  0.8679999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.11259743\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.900% | global_loss: 0.11259742826223373\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.42804036\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.400% | global_loss: 0.42804035544395447\n",
            "tf.Tensor(0.11259743, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.42804036, shape=(), dtype=float32) 0.094\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4512 - accuracy: 0.8810\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2408 - accuracy: 0.9350\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1579 - accuracy: 0.9440\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1465 - accuracy: 0.9510\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1040 - accuracy: 0.9670\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1063 - accuracy: 0.9690\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0775 - accuracy: 0.9780\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0478 - accuracy: 0.9830\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0450 - accuracy: 0.9830\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0689 - accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0659 - accuracy: 0.9710\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0439 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0405 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0333 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0249 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0480 - accuracy: 0.9820\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0240 - accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0237 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0111 - accuracy: 0.9990\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0210 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0146 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0147 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0191 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.000984474\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 12.000% | global_loss: 0.0009844739688560367\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.10556456\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.10556453466415405\n",
            "tf.Tensor(0.000984474, shape=(), dtype=float32) 0.12\n",
            "tf.Tensor(0.105564535, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4710 - accuracy: 0.8760\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2222 - accuracy: 0.9330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1652 - accuracy: 0.9480\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1266 - accuracy: 0.9630\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1136 - accuracy: 0.9680\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0885 - accuracy: 0.9760\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0610 - accuracy: 0.9860\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0525 - accuracy: 0.9830\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9790\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0569 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9840\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0280 - accuracy: 0.9910\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0291 - accuracy: 0.9900\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0114 - accuracy: 0.9970\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0105 - accuracy: 0.9970\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0141 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0104 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 0.9890\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0006970496\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.100% | global_loss: 0.0006970494869165123\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1153153\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.11531531065702438\n",
            "tf.Tensor(0.0006970495, shape=(), dtype=float32) 0.081\n",
            "tf.Tensor(0.11531531, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8840\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2283 - accuracy: 0.9320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1793 - accuracy: 0.9400\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1132 - accuracy: 0.9620\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0860 - accuracy: 0.9750\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9620\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0739 - accuracy: 0.9760\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0628 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0419 - accuracy: 0.9870\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9830\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0300 - accuracy: 0.9900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9850\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0442 - accuracy: 0.9870\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0274 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0267 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 0.9910\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0275 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0175 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0157 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0197 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0106 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0111 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0077 - accuracy: 0.9990\n",
            "Accuracy:  0.9990000128746033\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00028236897\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.200% | global_loss: 0.0002823690010700375\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11430255\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.11430254578590393\n",
            "tf.Tensor(0.000282369, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.114302546, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5129 - accuracy: 0.8580\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2559 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2182 - accuracy: 0.9260\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1418 - accuracy: 0.9510\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0856 - accuracy: 0.9670\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1084 - accuracy: 0.9670\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0622 - accuracy: 0.9800\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0627 - accuracy: 0.9790\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0579 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0582 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0617 - accuracy: 0.9830\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0483 - accuracy: 0.9840\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0408 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0352 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0258 - accuracy: 0.9900\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9910\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0168 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0286 - accuracy: 0.9900\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0279 - accuracy: 0.9880\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0192 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0081 - accuracy: 0.9990\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0080 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0004939628\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.0004939627833664417\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.093639344\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.09363935887813568\n",
            "tf.Tensor(0.0004939628, shape=(), dtype=float32) 0.088\n",
            "tf.Tensor(0.09363936, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4939 - accuracy: 0.8750\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2596 - accuracy: 0.9180\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1703 - accuracy: 0.9420\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9560\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1139 - accuracy: 0.9630\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0687 - accuracy: 0.9790\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0790 - accuracy: 0.9750\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0514 - accuracy: 0.9810\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9850\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0601 - accuracy: 0.9770\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0390 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0300 - accuracy: 0.9900\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0437 - accuracy: 0.9840\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 0.9850\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0301 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0519 - accuracy: 0.9800\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0264 - accuracy: 0.9910\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0266 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0145 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0284 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 0.9980\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0260 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0196 - accuracy: 0.9940\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0146 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0220 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0003646664\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.100% | global_loss: 0.00036466639721766114\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.085762754\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.08576274663209915\n",
            "tf.Tensor(0.0003646664, shape=(), dtype=float32) 0.111\n",
            "tf.Tensor(0.08576275, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4475 - accuracy: 0.8780\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2153 - accuracy: 0.9220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1308 - accuracy: 0.9600\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1242 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0983 - accuracy: 0.9710\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0699 - accuracy: 0.9760\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0737 - accuracy: 0.9740\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9790\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0513 - accuracy: 0.9830\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0429 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 0.9860\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0284 - accuracy: 0.9940\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0277 - accuracy: 0.9930\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0231 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9960\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9930\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0166 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0244 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0253 - accuracy: 0.9930\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0279 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0332 - accuracy: 0.9870\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0128 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0118 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0126 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0124 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0104 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0146 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00049283134\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 11.200% | global_loss: 0.0004928313428536057\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.067713104\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.06771309673786163\n",
            "tf.Tensor(0.00049283134, shape=(), dtype=float32) 0.112\n",
            "tf.Tensor(0.0677131, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.5510 - accuracy: 0.8690\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2445 - accuracy: 0.9300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1731 - accuracy: 0.9430\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9460\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1045 - accuracy: 0.9690\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1130 - accuracy: 0.9610\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0733 - accuracy: 0.9750\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0623 - accuracy: 0.9790\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0798 - accuracy: 0.9700\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0624 - accuracy: 0.9790\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0398 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0466 - accuracy: 0.9830\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9860\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0208 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 0.9880\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0400 - accuracy: 0.9850\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0242 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0241 - accuracy: 0.9910\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0277 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9890\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0177 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0157 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0188 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0174 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0238 - accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0255 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0135 - accuracy: 0.9950\n",
            "Accuracy:  0.9950000047683716\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00051284605\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0005128461052663624\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.109481454\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.10948145389556885\n",
            "tf.Tensor(0.0005128461, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.109481454, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5484 - accuracy: 0.8720\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2148 - accuracy: 0.9280\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1509 - accuracy: 0.9550\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1143 - accuracy: 0.9670\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0774 - accuracy: 0.9760\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0782 - accuracy: 0.9760\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0681 - accuracy: 0.9830\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0553 - accuracy: 0.9850\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0491 - accuracy: 0.9860\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0428 - accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0465 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0289 - accuracy: 0.9920\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0430 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0283 - accuracy: 0.9940\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0230 - accuracy: 0.9930\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0298 - accuracy: 0.9890\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0397 - accuracy: 0.9870\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0272 - accuracy: 0.9870\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0203 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0118 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0092 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0117 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0185 - accuracy: 0.9920\n",
            "Accuracy:  0.9919999837875366\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00023323484\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.500% | global_loss: 0.00023323482309933752\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09425087\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.0942508652806282\n",
            "tf.Tensor(0.00023323482, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.094250865, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4104 - accuracy: 0.8870\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2376 - accuracy: 0.9180\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9470\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1331 - accuracy: 0.9590\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0791 - accuracy: 0.9750\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0483 - accuracy: 0.9900\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0710 - accuracy: 0.9800\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0539 - accuracy: 0.9810\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9900\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0312 - accuracy: 0.9910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 0.9900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0265 - accuracy: 0.9920\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9740\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9840\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9880\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9950\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9900\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9980\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0166 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9930\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0128 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0209 - accuracy: 0.9930\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0113 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0108 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0268 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0126 - accuracy: 0.9980\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0002638563\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.900% | global_loss: 0.00026385628734715283\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.07565185\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.07565184682607651\n",
            "tf.Tensor(0.0002638563, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.07565185, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4608 - accuracy: 0.8770\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2074 - accuracy: 0.9280\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1778 - accuracy: 0.9450\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0734 - accuracy: 0.9740\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0488 - accuracy: 0.9820\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0470 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0557 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0430 - accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9920\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0345 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0292 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0304 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0216 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0237 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0261 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0472 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0269 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0193 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0156 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0132 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0115 - accuracy: 0.9970\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0089 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00025191123\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.300% | global_loss: 0.0002519112022127956\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.14981624\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.14981622993946075\n",
            "tf.Tensor(0.0002519112, shape=(), dtype=float32) 0.103\n",
            "tf.Tensor(0.14981623, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4867 - accuracy: 0.8590\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2206 - accuracy: 0.9300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1633 - accuracy: 0.9450\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1278 - accuracy: 0.9570\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0638 - accuracy: 0.9790\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0640 - accuracy: 0.9790\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0809 - accuracy: 0.9730\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0869 - accuracy: 0.9700\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0722 - accuracy: 0.9720\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 0.9850\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0411 - accuracy: 0.9900\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9870\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0254 - accuracy: 0.9900\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9930\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0332 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 0.9940\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0217 - accuracy: 0.9940\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0082 - accuracy: 0.9980\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0137 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 0.9990\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0104 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0210 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0115 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0157 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0229 - accuracy: 0.9910\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0162 - accuracy: 0.9930\n",
            "Accuracy:  0.9929999709129333\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0004333297\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.300% | global_loss: 0.0004333297547418624\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.06165652\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.600% | global_loss: 0.061656516045331955\n",
            "tf.Tensor(0.00043332975, shape=(), dtype=float32) 0.103\n",
            "tf.Tensor(0.061656516, shape=(), dtype=float32) 0.086\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5363 - accuracy: 0.8580\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2258 - accuracy: 0.9310\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2075 - accuracy: 0.9260\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1663 - accuracy: 0.9390\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0926 - accuracy: 0.9630\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1087 - accuracy: 0.9600\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0868 - accuracy: 0.9720\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0729 - accuracy: 0.9770\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0588 - accuracy: 0.9810\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0636 - accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9910\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0460 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9900\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0275 - accuracy: 0.9940\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9940\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0355 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0206 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0230 - accuracy: 0.9940\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0275 - accuracy: 0.9910\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0157 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0174 - accuracy: 0.9910\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.9930\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0131 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0188 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0208 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0179 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0137 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00063052395\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.900% | global_loss: 0.0006305240676738322\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.089615725\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.08961571753025055\n",
            "tf.Tensor(0.00063052407, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.08961572, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4888 - accuracy: 0.8700\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2232 - accuracy: 0.9290\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1452 - accuracy: 0.9580\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0905 - accuracy: 0.9680\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0841 - accuracy: 0.9750\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0960 - accuracy: 0.9650\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0747 - accuracy: 0.9760\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0525 - accuracy: 0.9850\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0499 - accuracy: 0.9880\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9780\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9790\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0277 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0188 - accuracy: 0.9950\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0206 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0284 - accuracy: 0.9910\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0279 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0170 - accuracy: 0.9970\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0246 - accuracy: 0.9910\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.9970\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0095 - accuracy: 0.9970\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0101 - accuracy: 0.9960\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0164 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0204 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0102 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00013497766\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.200% | global_loss: 0.00013497767213266343\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.11234803\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.11234802007675171\n",
            "tf.Tensor(0.00013497767, shape=(), dtype=float32) 0.102\n",
            "tf.Tensor(0.11234802, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5337 - accuracy: 0.8520\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2707 - accuracy: 0.9210\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1762 - accuracy: 0.9520\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1570 - accuracy: 0.9460\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0921 - accuracy: 0.9760\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0993 - accuracy: 0.9710\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0841 - accuracy: 0.9770\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0526 - accuracy: 0.9830\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0434 - accuracy: 0.9830\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0400 - accuracy: 0.9870\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0389 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0372 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0556 - accuracy: 0.9800\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9860\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0389 - accuracy: 0.9850\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0269 - accuracy: 0.9910\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9940\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9980\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0358 - accuracy: 0.9860\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9870\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0213 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0323 - accuracy: 0.9860\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0282 - accuracy: 0.9920\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0275 - accuracy: 0.9880\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0166 - accuracy: 0.9950\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0127 - accuracy: 0.9980\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 0.9950\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0194 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.001327736\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.500% | global_loss: 0.0013277360703796148\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.13337971\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.1333797127008438\n",
            "tf.Tensor(0.0013277361, shape=(), dtype=float32) 0.095\n",
            "tf.Tensor(0.13337971, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5256 - accuracy: 0.8710\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2317 - accuracy: 0.9300\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2025 - accuracy: 0.9310\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.1258 - accuracy: 0.9640\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0922 - accuracy: 0.9700\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0502 - accuracy: 0.9870\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0774 - accuracy: 0.9690\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9840\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0559 - accuracy: 0.9770\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0618 - accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0360 - accuracy: 0.9870\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9840\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 0.9870\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0286 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0347 - accuracy: 0.9890\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0229 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0150 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0263 - accuracy: 0.9910\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0140 - accuracy: 0.9980\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0261 - accuracy: 0.9940\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0343 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0279 - accuracy: 0.9900\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9910\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0212 - accuracy: 0.9910\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0158 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0192 - accuracy: 0.9940\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0156 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0005350477\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.400% | global_loss: 0.0005350476130843163\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.1041523\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.400% | global_loss: 0.10415229201316833\n",
            "tf.Tensor(0.0005350476, shape=(), dtype=float32) 0.104\n",
            "tf.Tensor(0.10415229, shape=(), dtype=float32) 0.084\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5611 - accuracy: 0.8770\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2370 - accuracy: 0.9370\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1904 - accuracy: 0.9420\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9620\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0943 - accuracy: 0.9650\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0669 - accuracy: 0.9770\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0762 - accuracy: 0.9770\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0688 - accuracy: 0.9730\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0612 - accuracy: 0.9800\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0438 - accuracy: 0.9880\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0313 - accuracy: 0.9910\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0234 - accuracy: 0.9940\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0303 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9880\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0224 - accuracy: 0.9930\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0267 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0274 - accuracy: 0.9890\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0171 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0143 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0200 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.9930\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0196 - accuracy: 0.9960\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0007773986\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.400% | global_loss: 0.0007773985853418708\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08413009\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.000% | global_loss: 0.08413009345531464\n",
            "tf.Tensor(0.0007773986, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.08413009, shape=(), dtype=float32) 0.09\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4714 - accuracy: 0.8680\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2102 - accuracy: 0.9320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1430 - accuracy: 0.9420\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0851 - accuracy: 0.9660\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0808 - accuracy: 0.9680\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0982 - accuracy: 0.9650\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0835 - accuracy: 0.9730\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0788 - accuracy: 0.9690\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9770\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0352 - accuracy: 0.9910\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0461 - accuracy: 0.9840\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0342 - accuracy: 0.9900\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0344 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0338 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0356 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0368 - accuracy: 0.9880\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0270 - accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0403 - accuracy: 0.9880\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0183 - accuracy: 0.9950\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9960\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0155 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0130 - accuracy: 0.9950\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0186 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0184 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0174 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Accuracy:  1.0\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00044287305\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.900% | global_loss: 0.0004428730462677777\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09849591\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.09849589318037033\n",
            "tf.Tensor(0.00044287305, shape=(), dtype=float32) 0.089\n",
            "tf.Tensor(0.09849589, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4780 - accuracy: 0.8640\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2595 - accuracy: 0.9230\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1650 - accuracy: 0.9440\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1087 - accuracy: 0.9670\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0925 - accuracy: 0.9650\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0675 - accuracy: 0.9770\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0895 - accuracy: 0.9660\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0865 - accuracy: 0.9730\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0476 - accuracy: 0.9860\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0387 - accuracy: 0.9880\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9890\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0388 - accuracy: 0.9850\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0326 - accuracy: 0.9890\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0293 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0235 - accuracy: 0.9910\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0234 - accuracy: 0.9930\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0231 - accuracy: 0.9950\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0185 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0287 - accuracy: 0.9900\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9940\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9940\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9910\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0199 - accuracy: 0.9910\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0197 - accuracy: 0.9960\n",
            "Accuracy:  0.9959999918937683\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00041667226\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.800% | global_loss: 0.0004166722937952727\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08435798\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.08435796946287155\n",
            "tf.Tensor(0.0004166723, shape=(), dtype=float32) 0.098\n",
            "tf.Tensor(0.08435797, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4424 - accuracy: 0.8850\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2785 - accuracy: 0.9160\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1802 - accuracy: 0.9440\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1370 - accuracy: 0.9550\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0834 - accuracy: 0.9760\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0841 - accuracy: 0.9760\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0677 - accuracy: 0.9760\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0656 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9840\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9850\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0260 - accuracy: 0.9910\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9950\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9920\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0297 - accuracy: 0.9910\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9870\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0485 - accuracy: 0.9870\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0263 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0159 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0127 - accuracy: 0.9980\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0178 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0189 - accuracy: 0.9950\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0205 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0216 - accuracy: 0.9900\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 0.9920\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0180 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 0.9940\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0157 - accuracy: 0.9950\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 0.9940\n",
            "Accuracy:  0.9940000176429749\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00031109955\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.000% | global_loss: 0.0003110994293820113\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.099059135\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.09905914217233658\n",
            "tf.Tensor(0.00031109943, shape=(), dtype=float32) 0.1\n",
            "tf.Tensor(0.09905914, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.5683 - accuracy: 0.8610\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2196 - accuracy: 0.9320\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1514 - accuracy: 0.9530\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1230 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1083 - accuracy: 0.9710\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9780\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0735 - accuracy: 0.9750\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0556 - accuracy: 0.9800\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0620 - accuracy: 0.9790\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0426 - accuracy: 0.9810\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0649 - accuracy: 0.9770\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0525 - accuracy: 0.9830\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0333 - accuracy: 0.9930\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0290 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0194 - accuracy: 0.9940\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0422 - accuracy: 0.9870\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0207 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0128 - accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9930\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0155 - accuracy: 0.9940\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0060 - accuracy: 0.9990\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0219 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0103 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0078 - accuracy: 0.9970\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9930\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.9960\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0205 - accuracy: 0.9920\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0053 - accuracy: 0.9990\n",
            "Accuracy:  0.9990000128746033\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00024804677\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.400% | global_loss: 0.00024804676650092006\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.08444682\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.400% | global_loss: 0.08444682508707047\n",
            "tf.Tensor(0.00024804677, shape=(), dtype=float32) 0.094\n",
            "tf.Tensor(0.084446825, shape=(), dtype=float32) 0.084\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 0.4187 - accuracy: 0.8910\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2607 - accuracy: 0.9160\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1691 - accuracy: 0.9440\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1399 - accuracy: 0.9580\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0907 - accuracy: 0.9720\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0741 - accuracy: 0.9790\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0614 - accuracy: 0.9840\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0659 - accuracy: 0.9780\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0609 - accuracy: 0.9820\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0592 - accuracy: 0.9800\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0382 - accuracy: 0.9880\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9810\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0305 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0247 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0313 - accuracy: 0.9880\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0223 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0266 - accuracy: 0.9920\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0307 - accuracy: 0.9920\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0410 - accuracy: 0.9840\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9920\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9930\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0166 - accuracy: 0.9930\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9860\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9940\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0071 - accuracy: 0.9990\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0057 - accuracy: 0.9990\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0093 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0135 - accuracy: 0.9970\n",
            "Accuracy:  0.996999979019165\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.0003700279\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.300% | global_loss: 0.0003700279048644006\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.086059734\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.08605973422527313\n",
            "tf.Tensor(0.0003700279, shape=(), dtype=float32) 0.093\n",
            "tf.Tensor(0.086059734, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4938 - accuracy: 0.8650\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2116 - accuracy: 0.9330\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1449 - accuracy: 0.9490\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1080 - accuracy: 0.9670\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.1021 - accuracy: 0.9620\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0788 - accuracy: 0.9800\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9850\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0617 - accuracy: 0.9820\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0540 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9900\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9890\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0382 - accuracy: 0.9830\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0366 - accuracy: 0.9880\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0269 - accuracy: 0.9880\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0266 - accuracy: 0.9910\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0227 - accuracy: 0.9940\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9900\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0247 - accuracy: 0.9920\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0140 - accuracy: 0.9950\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0095 - accuracy: 0.9980\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0108 - accuracy: 0.9970\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0219 - accuracy: 0.9920\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.9970\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9950\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0089 - accuracy: 0.9980\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9930\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0263 - accuracy: 0.9890\n",
            "Accuracy:  0.9890000224113464\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.004568445\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.900% | global_loss: 0.0045684450305998325\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.09778331\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 8.800% | global_loss: 0.09778331220149994\n",
            "tf.Tensor(0.004568445, shape=(), dtype=float32) 0.099\n",
            "tf.Tensor(0.09778331, shape=(), dtype=float32) 0.088\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.4172 - accuracy: 0.8910\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.2307 - accuracy: 0.9220\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.1537 - accuracy: 0.9450\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1345 - accuracy: 0.9600\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.1098 - accuracy: 0.9620\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0833 - accuracy: 0.9710\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0689 - accuracy: 0.9770\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0469 - accuracy: 0.9850\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9820\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0629 - accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0594 - accuracy: 0.9800\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0539 - accuracy: 0.9820\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 0.9890\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 0.9900\n",
            "Epoch 15/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0224 - accuracy: 0.9920\n",
            "Epoch 16/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0249 - accuracy: 0.9920\n",
            "Epoch 17/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0257 - accuracy: 0.9890\n",
            "Epoch 18/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 19/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9970\n",
            "Epoch 20/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9980\n",
            "Epoch 21/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9920\n",
            "Epoch 23/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0190 - accuracy: 0.9940\n",
            "Epoch 24/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0114 - accuracy: 0.9970\n",
            "Epoch 25/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 0.9960\n",
            "Epoch 26/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0115 - accuracy: 0.9980\n",
            "Epoch 27/30\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0191 - accuracy: 0.9920\n",
            "Epoch 28/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9960\n",
            "Epoch 29/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 0.9970\n",
            "Epoch 30/30\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0121 - accuracy: 0.9950\n",
            "Accuracy:  0.9950000047683716\n",
            "(1000, 1) (1000, 10)\n",
            "lossindiv\n",
            "0.00020814223\n",
            "logits\n",
            "size (1000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 10.500% | global_loss: 0.00020814222807530314\n",
            "(500, 1) (500, 10)\n",
            "lossindiv\n",
            "0.085902914\n",
            "logits\n",
            "size (500, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.200% | global_loss: 0.08590291440486908\n",
            "tf.Tensor(0.00020814223, shape=(), dtype=float32) 0.105\n",
            "tf.Tensor(0.085902914, shape=(), dtype=float32) 0.092\n",
            "<class 'numpy.ndarray'>\n",
            "(60000, 1) (60000, 10)\n",
            "lossindiv\n",
            "0.091368064\n",
            "logits\n",
            "size (60000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.907% | global_loss: 0.09136800467967987\n",
            "sparse_categorical_crossentropy tf.Tensor(0.091368005, shape=(), dtype=float32) 0.09906666666666666\n",
            "(10000, 1) (10000, 10)\n",
            "lossindiv\n",
            "0.07782108\n",
            "logits\n",
            "size (10000, 10)\n",
            "loss\n",
            "comm_round: 1 | global_acc: 9.950% | global_loss: 0.07782106101512909\n",
            "tf.Tensor(0.07782106, shape=(), dtype=float32) 0.0995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxDbbCiQt0nI",
        "outputId": "2017fe87-6f94-4e8e-81cc-793affc15992"
      },
      "source": [
        "!nvidia-smi\n",
        "abcd=samplecdf[117].copy()\n",
        "abcd=np.array(abcd)\n",
        "print(max(abcd))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  5 09:29:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "6.080212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOw5knjCeRzg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e0XoEjtwTeb",
        "outputId": "897fe17c-f7db-4ea8-c244-f87463d9e578"
      },
      "source": [
        "ind=0.24925\n",
        "#ecdf2 = ECDF(samplecdf[13])\n",
        "#pyplot.plot(ecdf2.x,ecdf2.y)\n",
        "lambdastar=1000\n",
        "distmax=1000#infinity\n",
        "#abc1=ecdfbest.copy()\n",
        "#ecdfbest=np.array(ecdfbest)\n",
        "#ecdfworst=np.array(ecdfworst)clienttrainingloss\n",
        "ecdf2 = ECDF(samplecdf[103])\n",
        "#abc1.sort()\n",
        "F1=ecdf2(samplecdf[103])\n",
        "clienttrainingloss=np.array(clienttrainingloss)\n",
        "for ind1 in range(0,10000):\n",
        "  #abc2=ecdfworst.copy()\n",
        "  #print(abc2[13])\n",
        "  #abc2.pop(9)\n",
        "  ind=ind+0.000000001\n",
        "  ecdf4 = ECDF(clienttrainingloss)\n",
        "  #abc2.sort()\n",
        "  F2=ecdf4(clienttrainingloss)/ecdf4(ind)\n",
        "  for indd in range(len(F2)):\n",
        "    if F2[indd]>1:\n",
        "      F2[indd]=1\n",
        "  ksdist,ksvalue = ks_2samp(F1,F2)\n",
        "  print(\"i\",ind,\"\\t\",ksdist)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "i 0.24925500099999737 \t 0.16666666666666666\n",
            "i 0.24925500199999737 \t 0.16666666666666666\n",
            "i 0.24925500299999737 \t 0.16666666666666666\n",
            "i 0.24925500399999737 \t 0.16666666666666666\n",
            "i 0.24925500499999736 \t 0.16666666666666666\n",
            "i 0.24925500599999736 \t 0.16666666666666666\n",
            "i 0.24925500699999736 \t 0.16666666666666666\n",
            "i 0.24925500799999736 \t 0.16666666666666666\n",
            "i 0.24925500899999736 \t 0.16666666666666666\n",
            "i 0.24925500999999736 \t 0.16666666666666666\n",
            "i 0.24925501099999736 \t 0.16666666666666666\n",
            "i 0.24925501199999736 \t 0.16666666666666666\n",
            "i 0.24925501299999736 \t 0.16666666666666666\n",
            "i 0.24925501399999736 \t 0.16666666666666666\n",
            "i 0.24925501499999736 \t 0.16666666666666666\n",
            "i 0.24925501599999736 \t 0.16666666666666666\n",
            "i 0.24925501699999736 \t 0.16666666666666666\n",
            "i 0.24925501799999736 \t 0.16666666666666666\n",
            "i 0.24925501899999736 \t 0.16666666666666666\n",
            "i 0.24925501999999736 \t 0.16666666666666666\n",
            "i 0.24925502099999736 \t 0.16666666666666666\n",
            "i 0.24925502199999736 \t 0.16666666666666666\n",
            "i 0.24925502299999736 \t 0.16666666666666666\n",
            "i 0.24925502399999735 \t 0.16666666666666666\n",
            "i 0.24925502499999735 \t 0.16666666666666666\n",
            "i 0.24925502599999735 \t 0.16666666666666666\n",
            "i 0.24925502699999735 \t 0.16666666666666666\n",
            "i 0.24925502799999735 \t 0.16666666666666666\n",
            "i 0.24925502899999735 \t 0.16666666666666666\n",
            "i 0.24925502999999735 \t 0.16666666666666666\n",
            "i 0.24925503099999735 \t 0.16666666666666666\n",
            "i 0.24925503199999735 \t 0.16666666666666666\n",
            "i 0.24925503299999735 \t 0.16666666666666666\n",
            "i 0.24925503399999735 \t 0.16666666666666666\n",
            "i 0.24925503499999735 \t 0.16666666666666666\n",
            "i 0.24925503599999735 \t 0.16666666666666666\n",
            "i 0.24925503699999735 \t 0.16666666666666666\n",
            "i 0.24925503799999735 \t 0.16666666666666666\n",
            "i 0.24925503899999735 \t 0.16666666666666666\n",
            "i 0.24925503999999735 \t 0.16666666666666666\n",
            "i 0.24925504099999735 \t 0.16666666666666666\n",
            "i 0.24925504199999735 \t 0.16666666666666666\n",
            "i 0.24925504299999734 \t 0.16666666666666666\n",
            "i 0.24925504399999734 \t 0.16666666666666666\n",
            "i 0.24925504499999734 \t 0.16666666666666666\n",
            "i 0.24925504599999734 \t 0.16666666666666666\n",
            "i 0.24925504699999734 \t 0.16666666666666666\n",
            "i 0.24925504799999734 \t 0.16666666666666666\n",
            "i 0.24925504899999734 \t 0.16666666666666666\n",
            "i 0.24925504999999734 \t 0.16666666666666666\n",
            "i 0.24925505099999734 \t 0.16666666666666666\n",
            "i 0.24925505199999734 \t 0.16666666666666666\n",
            "i 0.24925505299999734 \t 0.16666666666666666\n",
            "i 0.24925505399999734 \t 0.16666666666666666\n",
            "i 0.24925505499999734 \t 0.16666666666666666\n",
            "i 0.24925505599999734 \t 0.16666666666666666\n",
            "i 0.24925505699999734 \t 0.16666666666666666\n",
            "i 0.24925505799999734 \t 0.16666666666666666\n",
            "i 0.24925505899999734 \t 0.16666666666666666\n",
            "i 0.24925505999999734 \t 0.16666666666666666\n",
            "i 0.24925506099999734 \t 0.16666666666666666\n",
            "i 0.24925506199999733 \t 0.16666666666666666\n",
            "i 0.24925506299999733 \t 0.16666666666666666\n",
            "i 0.24925506399999733 \t 0.16666666666666666\n",
            "i 0.24925506499999733 \t 0.16666666666666666\n",
            "i 0.24925506599999733 \t 0.16666666666666666\n",
            "i 0.24925506699999733 \t 0.16666666666666666\n",
            "i 0.24925506799999733 \t 0.16666666666666666\n",
            "i 0.24925506899999733 \t 0.16666666666666666\n",
            "i 0.24925506999999733 \t 0.16666666666666666\n",
            "i 0.24925507099999733 \t 0.16666666666666666\n",
            "i 0.24925507199999733 \t 0.16666666666666666\n",
            "i 0.24925507299999733 \t 0.16666666666666666\n",
            "i 0.24925507399999733 \t 0.16666666666666666\n",
            "i 0.24925507499999733 \t 0.16666666666666666\n",
            "i 0.24925507599999733 \t 0.16666666666666666\n",
            "i 0.24925507699999733 \t 0.16666666666666666\n",
            "i 0.24925507799999733 \t 0.16666666666666666\n",
            "i 0.24925507899999733 \t 0.16666666666666666\n",
            "i 0.24925507999999733 \t 0.16666666666666666\n",
            "i 0.24925508099999732 \t 0.16666666666666666\n",
            "i 0.24925508199999732 \t 0.16666666666666666\n",
            "i 0.24925508299999732 \t 0.16666666666666666\n",
            "i 0.24925508399999732 \t 0.16666666666666666\n",
            "i 0.24925508499999732 \t 0.16666666666666666\n",
            "i 0.24925508599999732 \t 0.16666666666666666\n",
            "i 0.24925508699999732 \t 0.16666666666666666\n",
            "i 0.24925508799999732 \t 0.16666666666666666\n",
            "i 0.24925508899999732 \t 0.16666666666666666\n",
            "i 0.24925508999999732 \t 0.16666666666666666\n",
            "i 0.24925509099999732 \t 0.16666666666666666\n",
            "i 0.24925509199999732 \t 0.16666666666666666\n",
            "i 0.24925509299999732 \t 0.16666666666666666\n",
            "i 0.24925509399999732 \t 0.16666666666666666\n",
            "i 0.24925509499999732 \t 0.16666666666666666\n",
            "i 0.24925509599999732 \t 0.16666666666666666\n",
            "i 0.24925509699999732 \t 0.16666666666666666\n",
            "i 0.24925509799999732 \t 0.16666666666666666\n",
            "i 0.24925509899999732 \t 0.16666666666666666\n",
            "i 0.24925509999999731 \t 0.16666666666666666\n",
            "i 0.24925510099999731 \t 0.16666666666666666\n",
            "i 0.2492551019999973 \t 0.16666666666666666\n",
            "i 0.2492551029999973 \t 0.16666666666666666\n",
            "i 0.2492551039999973 \t 0.16666666666666666\n",
            "i 0.2492551049999973 \t 0.16666666666666666\n",
            "i 0.2492551059999973 \t 0.16666666666666666\n",
            "i 0.2492551069999973 \t 0.16666666666666666\n",
            "i 0.2492551079999973 \t 0.16666666666666666\n",
            "i 0.2492551089999973 \t 0.16666666666666666\n",
            "i 0.2492551099999973 \t 0.16666666666666666\n",
            "i 0.2492551109999973 \t 0.16666666666666666\n",
            "i 0.2492551119999973 \t 0.16666666666666666\n",
            "i 0.2492551129999973 \t 0.16666666666666666\n",
            "i 0.2492551139999973 \t 0.16666666666666666\n",
            "i 0.2492551149999973 \t 0.16666666666666666\n",
            "i 0.2492551159999973 \t 0.16666666666666666\n",
            "i 0.2492551169999973 \t 0.16666666666666666\n",
            "i 0.2492551179999973 \t 0.16666666666666666\n",
            "i 0.2492551189999973 \t 0.16666666666666666\n",
            "i 0.2492551199999973 \t 0.16666666666666666\n",
            "i 0.2492551209999973 \t 0.16666666666666666\n",
            "i 0.2492551219999973 \t 0.16666666666666666\n",
            "i 0.2492551229999973 \t 0.16666666666666666\n",
            "i 0.2492551239999973 \t 0.16666666666666666\n",
            "i 0.2492551249999973 \t 0.16666666666666666\n",
            "i 0.2492551259999973 \t 0.16666666666666666\n",
            "i 0.2492551269999973 \t 0.16666666666666666\n",
            "i 0.2492551279999973 \t 0.16666666666666666\n",
            "i 0.2492551289999973 \t 0.16666666666666666\n",
            "i 0.2492551299999973 \t 0.16666666666666666\n",
            "i 0.2492551309999973 \t 0.16666666666666666\n",
            "i 0.2492551319999973 \t 0.16666666666666666\n",
            "i 0.2492551329999973 \t 0.16666666666666666\n",
            "i 0.2492551339999973 \t 0.16666666666666666\n",
            "i 0.2492551349999973 \t 0.16666666666666666\n",
            "i 0.2492551359999973 \t 0.16666666666666666\n",
            "i 0.2492551369999973 \t 0.16666666666666666\n",
            "i 0.2492551379999973 \t 0.16666666666666666\n",
            "i 0.2492551389999973 \t 0.16666666666666666\n",
            "i 0.2492551399999973 \t 0.16666666666666666\n",
            "i 0.2492551409999973 \t 0.16666666666666666\n",
            "i 0.2492551419999973 \t 0.16666666666666666\n",
            "i 0.2492551429999973 \t 0.16666666666666666\n",
            "i 0.2492551439999973 \t 0.16666666666666666\n",
            "i 0.2492551449999973 \t 0.16666666666666666\n",
            "i 0.2492551459999973 \t 0.16666666666666666\n",
            "i 0.2492551469999973 \t 0.16666666666666666\n",
            "i 0.2492551479999973 \t 0.16666666666666666\n",
            "i 0.2492551489999973 \t 0.16666666666666666\n",
            "i 0.2492551499999973 \t 0.16666666666666666\n",
            "i 0.2492551509999973 \t 0.16666666666666666\n",
            "i 0.2492551519999973 \t 0.16666666666666666\n",
            "i 0.2492551529999973 \t 0.16666666666666666\n",
            "i 0.2492551539999973 \t 0.16666666666666666\n",
            "i 0.24925515499999729 \t 0.16666666666666666\n",
            "i 0.24925515599999729 \t 0.16666666666666666\n",
            "i 0.24925515699999728 \t 0.16666666666666666\n",
            "i 0.24925515799999728 \t 0.16666666666666666\n",
            "i 0.24925515899999728 \t 0.16666666666666666\n",
            "i 0.24925515999999728 \t 0.16666666666666666\n",
            "i 0.24925516099999728 \t 0.16666666666666666\n",
            "i 0.24925516199999728 \t 0.16666666666666666\n",
            "i 0.24925516299999728 \t 0.16666666666666666\n",
            "i 0.24925516399999728 \t 0.16666666666666666\n",
            "i 0.24925516499999728 \t 0.16666666666666666\n",
            "i 0.24925516599999728 \t 0.16666666666666666\n",
            "i 0.24925516699999728 \t 0.16666666666666666\n",
            "i 0.24925516799999728 \t 0.16666666666666666\n",
            "i 0.24925516899999728 \t 0.16666666666666666\n",
            "i 0.24925516999999728 \t 0.16666666666666666\n",
            "i 0.24925517099999728 \t 0.16666666666666666\n",
            "i 0.24925517199999728 \t 0.16666666666666666\n",
            "i 0.24925517299999728 \t 0.16666666666666666\n",
            "i 0.24925517399999728 \t 0.16666666666666666\n",
            "i 0.24925517499999728 \t 0.16666666666666666\n",
            "i 0.24925517599999727 \t 0.16666666666666666\n",
            "i 0.24925517699999727 \t 0.16666666666666666\n",
            "i 0.24925517799999727 \t 0.16666666666666666\n",
            "i 0.24925517899999727 \t 0.16666666666666666\n",
            "i 0.24925517999999727 \t 0.16666666666666666\n",
            "i 0.24925518099999727 \t 0.16666666666666666\n",
            "i 0.24925518199999727 \t 0.16666666666666666\n",
            "i 0.24925518299999727 \t 0.16666666666666666\n",
            "i 0.24925518399999727 \t 0.16666666666666666\n",
            "i 0.24925518499999727 \t 0.16666666666666666\n",
            "i 0.24925518599999727 \t 0.16666666666666666\n",
            "i 0.24925518699999727 \t 0.16666666666666666\n",
            "i 0.24925518799999727 \t 0.16666666666666666\n",
            "i 0.24925518899999727 \t 0.16666666666666666\n",
            "i 0.24925518999999727 \t 0.16666666666666666\n",
            "i 0.24925519099999727 \t 0.16666666666666666\n",
            "i 0.24925519199999727 \t 0.16666666666666666\n",
            "i 0.24925519299999727 \t 0.16666666666666666\n",
            "i 0.24925519399999727 \t 0.16666666666666666\n",
            "i 0.24925519499999726 \t 0.16666666666666666\n",
            "i 0.24925519599999726 \t 0.16666666666666666\n",
            "i 0.24925519699999726 \t 0.16666666666666666\n",
            "i 0.24925519799999726 \t 0.16666666666666666\n",
            "i 0.24925519899999726 \t 0.16666666666666666\n",
            "i 0.24925519999999726 \t 0.16666666666666666\n",
            "i 0.24925520099999726 \t 0.16666666666666666\n",
            "i 0.24925520199999726 \t 0.16666666666666666\n",
            "i 0.24925520299999726 \t 0.16666666666666666\n",
            "i 0.24925520399999726 \t 0.16666666666666666\n",
            "i 0.24925520499999726 \t 0.16666666666666666\n",
            "i 0.24925520599999726 \t 0.16666666666666666\n",
            "i 0.24925520699999726 \t 0.16666666666666666\n",
            "i 0.24925520799999726 \t 0.16666666666666666\n",
            "i 0.24925520899999726 \t 0.16666666666666666\n",
            "i 0.24925520999999726 \t 0.16666666666666666\n",
            "i 0.24925521099999726 \t 0.16666666666666666\n",
            "i 0.24925521199999726 \t 0.16666666666666666\n",
            "i 0.24925521299999726 \t 0.16666666666666666\n",
            "i 0.24925521399999725 \t 0.16666666666666666\n",
            "i 0.24925521499999725 \t 0.16666666666666666\n",
            "i 0.24925521599999725 \t 0.16666666666666666\n",
            "i 0.24925521699999725 \t 0.16666666666666666\n",
            "i 0.24925521799999725 \t 0.16666666666666666\n",
            "i 0.24925521899999725 \t 0.16666666666666666\n",
            "i 0.24925521999999725 \t 0.16666666666666666\n",
            "i 0.24925522099999725 \t 0.16666666666666666\n",
            "i 0.24925522199999725 \t 0.16666666666666666\n",
            "i 0.24925522299999725 \t 0.16666666666666666\n",
            "i 0.24925522399999725 \t 0.16666666666666666\n",
            "i 0.24925522499999725 \t 0.16666666666666666\n",
            "i 0.24925522599999725 \t 0.16666666666666666\n",
            "i 0.24925522699999725 \t 0.16666666666666666\n",
            "i 0.24925522799999725 \t 0.16666666666666666\n",
            "i 0.24925522899999725 \t 0.16666666666666666\n",
            "i 0.24925522999999725 \t 0.16666666666666666\n",
            "i 0.24925523099999725 \t 0.16666666666666666\n",
            "i 0.24925523199999725 \t 0.16666666666666666\n",
            "i 0.24925523299999724 \t 0.16666666666666666\n",
            "i 0.24925523399999724 \t 0.16666666666666666\n",
            "i 0.24925523499999724 \t 0.16666666666666666\n",
            "i 0.24925523599999724 \t 0.16666666666666666\n",
            "i 0.24925523699999724 \t 0.16666666666666666\n",
            "i 0.24925523799999724 \t 0.16666666666666666\n",
            "i 0.24925523899999724 \t 0.16666666666666666\n",
            "i 0.24925523999999724 \t 0.16666666666666666\n",
            "i 0.24925524099999724 \t 0.16666666666666666\n",
            "i 0.24925524199999724 \t 0.16666666666666666\n",
            "i 0.24925524299999724 \t 0.16666666666666666\n",
            "i 0.24925524399999724 \t 0.16666666666666666\n",
            "i 0.24925524499999724 \t 0.16666666666666666\n",
            "i 0.24925524599999724 \t 0.16666666666666666\n",
            "i 0.24925524699999724 \t 0.16666666666666666\n",
            "i 0.24925524799999724 \t 0.16666666666666666\n",
            "i 0.24925524899999724 \t 0.16666666666666666\n",
            "i 0.24925524999999724 \t 0.16666666666666666\n",
            "i 0.24925525099999724 \t 0.16666666666666666\n",
            "i 0.24925525199999723 \t 0.16666666666666666\n",
            "i 0.24925525299999723 \t 0.16666666666666666\n",
            "i 0.24925525399999723 \t 0.16666666666666666\n",
            "i 0.24925525499999723 \t 0.16666666666666666\n",
            "i 0.24925525599999723 \t 0.16666666666666666\n",
            "i 0.24925525699999723 \t 0.16666666666666666\n",
            "i 0.24925525799999723 \t 0.16666666666666666\n",
            "i 0.24925525899999723 \t 0.16666666666666666\n",
            "i 0.24925525999999723 \t 0.16666666666666666\n",
            "i 0.24925526099999723 \t 0.16666666666666666\n",
            "i 0.24925526199999723 \t 0.16666666666666666\n",
            "i 0.24925526299999723 \t 0.16666666666666666\n",
            "i 0.24925526399999723 \t 0.16666666666666666\n",
            "i 0.24925526499999723 \t 0.16666666666666666\n",
            "i 0.24925526599999723 \t 0.16666666666666666\n",
            "i 0.24925526699999723 \t 0.16666666666666666\n",
            "i 0.24925526799999723 \t 0.16666666666666666\n",
            "i 0.24925526899999723 \t 0.16666666666666666\n",
            "i 0.24925526999999723 \t 0.16666666666666666\n",
            "i 0.24925527099999722 \t 0.16666666666666666\n",
            "i 0.24925527199999722 \t 0.16666666666666666\n",
            "i 0.24925527299999722 \t 0.16666666666666666\n",
            "i 0.24925527399999722 \t 0.16666666666666666\n",
            "i 0.24925527499999722 \t 0.16666666666666666\n",
            "i 0.24925527599999722 \t 0.16666666666666666\n",
            "i 0.24925527699999722 \t 0.16666666666666666\n",
            "i 0.24925527799999722 \t 0.16666666666666666\n",
            "i 0.24925527899999722 \t 0.16666666666666666\n",
            "i 0.24925527999999722 \t 0.16666666666666666\n",
            "i 0.24925528099999722 \t 0.16666666666666666\n",
            "i 0.24925528199999722 \t 0.16666666666666666\n",
            "i 0.24925528299999722 \t 0.16666666666666666\n",
            "i 0.24925528399999722 \t 0.16666666666666666\n",
            "i 0.24925528499999722 \t 0.16666666666666666\n",
            "i 0.24925528599999722 \t 0.16666666666666666\n",
            "i 0.24925528699999722 \t 0.16666666666666666\n",
            "i 0.24925528799999722 \t 0.16666666666666666\n",
            "i 0.24925528899999722 \t 0.16666666666666666\n",
            "i 0.24925528999999721 \t 0.16666666666666666\n",
            "i 0.24925529099999721 \t 0.16666666666666666\n",
            "i 0.2492552919999972 \t 0.16666666666666666\n",
            "i 0.2492552929999972 \t 0.16666666666666666\n",
            "i 0.2492552939999972 \t 0.16666666666666666\n",
            "i 0.2492552949999972 \t 0.16666666666666666\n",
            "i 0.2492552959999972 \t 0.16666666666666666\n",
            "i 0.2492552969999972 \t 0.16666666666666666\n",
            "i 0.2492552979999972 \t 0.16666666666666666\n",
            "i 0.2492552989999972 \t 0.16666666666666666\n",
            "i 0.2492552999999972 \t 0.16666666666666666\n",
            "i 0.2492553009999972 \t 0.16666666666666666\n",
            "i 0.2492553019999972 \t 0.16666666666666666\n",
            "i 0.2492553029999972 \t 0.16666666666666666\n",
            "i 0.2492553039999972 \t 0.16666666666666666\n",
            "i 0.2492553049999972 \t 0.16666666666666666\n",
            "i 0.2492553059999972 \t 0.16666666666666666\n",
            "i 0.2492553069999972 \t 0.16666666666666666\n",
            "i 0.2492553079999972 \t 0.16666666666666666\n",
            "i 0.2492553089999972 \t 0.16666666666666666\n",
            "i 0.2492553099999972 \t 0.16666666666666666\n",
            "i 0.2492553109999972 \t 0.16666666666666666\n",
            "i 0.2492553119999972 \t 0.16666666666666666\n",
            "i 0.2492553129999972 \t 0.16666666666666666\n",
            "i 0.2492553139999972 \t 0.16666666666666666\n",
            "i 0.2492553149999972 \t 0.16666666666666666\n",
            "i 0.2492553159999972 \t 0.16666666666666666\n",
            "i 0.2492553169999972 \t 0.16666666666666666\n",
            "i 0.2492553179999972 \t 0.16666666666666666\n",
            "i 0.2492553189999972 \t 0.16666666666666666\n",
            "i 0.2492553199999972 \t 0.16666666666666666\n",
            "i 0.2492553209999972 \t 0.16666666666666666\n",
            "i 0.2492553219999972 \t 0.16666666666666666\n",
            "i 0.2492553229999972 \t 0.16666666666666666\n",
            "i 0.2492553239999972 \t 0.16666666666666666\n",
            "i 0.2492553249999972 \t 0.16666666666666666\n",
            "i 0.2492553259999972 \t 0.16666666666666666\n",
            "i 0.2492553269999972 \t 0.16666666666666666\n",
            "i 0.2492553279999972 \t 0.16666666666666666\n",
            "i 0.2492553289999972 \t 0.16666666666666666\n",
            "i 0.2492553299999972 \t 0.16666666666666666\n",
            "i 0.2492553309999972 \t 0.16666666666666666\n",
            "i 0.2492553319999972 \t 0.16666666666666666\n",
            "i 0.2492553329999972 \t 0.16666666666666666\n",
            "i 0.2492553339999972 \t 0.16666666666666666\n",
            "i 0.2492553349999972 \t 0.16666666666666666\n",
            "i 0.2492553359999972 \t 0.16666666666666666\n",
            "i 0.2492553369999972 \t 0.16666666666666666\n",
            "i 0.2492553379999972 \t 0.16666666666666666\n",
            "i 0.2492553389999972 \t 0.16666666666666666\n",
            "i 0.2492553399999972 \t 0.16666666666666666\n",
            "i 0.2492553409999972 \t 0.16666666666666666\n",
            "i 0.2492553419999972 \t 0.16666666666666666\n",
            "i 0.2492553429999972 \t 0.16666666666666666\n",
            "i 0.2492553439999972 \t 0.16666666666666666\n",
            "i 0.24925534499999719 \t 0.16666666666666666\n",
            "i 0.24925534599999719 \t 0.16666666666666666\n",
            "i 0.24925534699999718 \t 0.16666666666666666\n",
            "i 0.24925534799999718 \t 0.16666666666666666\n",
            "i 0.24925534899999718 \t 0.16666666666666666\n",
            "i 0.24925534999999718 \t 0.16666666666666666\n",
            "i 0.24925535099999718 \t 0.16666666666666666\n",
            "i 0.24925535199999718 \t 0.16666666666666666\n",
            "i 0.24925535299999718 \t 0.16666666666666666\n",
            "i 0.24925535399999718 \t 0.16666666666666666\n",
            "i 0.24925535499999718 \t 0.16666666666666666\n",
            "i 0.24925535599999718 \t 0.16666666666666666\n",
            "i 0.24925535699999718 \t 0.16666666666666666\n",
            "i 0.24925535799999718 \t 0.16666666666666666\n",
            "i 0.24925535899999718 \t 0.16666666666666666\n",
            "i 0.24925535999999718 \t 0.16666666666666666\n",
            "i 0.24925536099999718 \t 0.16666666666666666\n",
            "i 0.24925536199999718 \t 0.16666666666666666\n",
            "i 0.24925536299999718 \t 0.16666666666666666\n",
            "i 0.24925536399999718 \t 0.16666666666666666\n",
            "i 0.24925536499999718 \t 0.16666666666666666\n",
            "i 0.24925536599999717 \t 0.16666666666666666\n",
            "i 0.24925536699999717 \t 0.16666666666666666\n",
            "i 0.24925536799999717 \t 0.16666666666666666\n",
            "i 0.24925536899999717 \t 0.16666666666666666\n",
            "i 0.24925536999999717 \t 0.16666666666666666\n",
            "i 0.24925537099999717 \t 0.16666666666666666\n",
            "i 0.24925537199999717 \t 0.16666666666666666\n",
            "i 0.24925537299999717 \t 0.16666666666666666\n",
            "i 0.24925537399999717 \t 0.16666666666666666\n",
            "i 0.24925537499999717 \t 0.16666666666666666\n",
            "i 0.24925537599999717 \t 0.16666666666666666\n",
            "i 0.24925537699999717 \t 0.16666666666666666\n",
            "i 0.24925537799999717 \t 0.16666666666666666\n",
            "i 0.24925537899999717 \t 0.16666666666666666\n",
            "i 0.24925537999999717 \t 0.16666666666666666\n",
            "i 0.24925538099999717 \t 0.16666666666666666\n",
            "i 0.24925538199999717 \t 0.16666666666666666\n",
            "i 0.24925538299999717 \t 0.16666666666666666\n",
            "i 0.24925538399999717 \t 0.16666666666666666\n",
            "i 0.24925538499999716 \t 0.16666666666666666\n",
            "i 0.24925538599999716 \t 0.16666666666666666\n",
            "i 0.24925538699999716 \t 0.16666666666666666\n",
            "i 0.24925538799999716 \t 0.16666666666666666\n",
            "i 0.24925538899999716 \t 0.16666666666666666\n",
            "i 0.24925538999999716 \t 0.16666666666666666\n",
            "i 0.24925539099999716 \t 0.16666666666666666\n",
            "i 0.24925539199999716 \t 0.16666666666666666\n",
            "i 0.24925539299999716 \t 0.16666666666666666\n",
            "i 0.24925539399999716 \t 0.16666666666666666\n",
            "i 0.24925539499999716 \t 0.16666666666666666\n",
            "i 0.24925539599999716 \t 0.16666666666666666\n",
            "i 0.24925539699999716 \t 0.16666666666666666\n",
            "i 0.24925539799999716 \t 0.16666666666666666\n",
            "i 0.24925539899999716 \t 0.16666666666666666\n",
            "i 0.24925539999999716 \t 0.16666666666666666\n",
            "i 0.24925540099999716 \t 0.16666666666666666\n",
            "i 0.24925540199999716 \t 0.16666666666666666\n",
            "i 0.24925540299999716 \t 0.16666666666666666\n",
            "i 0.24925540399999715 \t 0.16666666666666666\n",
            "i 0.24925540499999715 \t 0.16666666666666666\n",
            "i 0.24925540599999715 \t 0.16666666666666666\n",
            "i 0.24925540699999715 \t 0.16666666666666666\n",
            "i 0.24925540799999715 \t 0.16666666666666666\n",
            "i 0.24925540899999715 \t 0.16666666666666666\n",
            "i 0.24925540999999715 \t 0.16666666666666666\n",
            "i 0.24925541099999715 \t 0.16666666666666666\n",
            "i 0.24925541199999715 \t 0.16666666666666666\n",
            "i 0.24925541299999715 \t 0.16666666666666666\n",
            "i 0.24925541399999715 \t 0.16666666666666666\n",
            "i 0.24925541499999715 \t 0.16666666666666666\n",
            "i 0.24925541599999715 \t 0.16666666666666666\n",
            "i 0.24925541699999715 \t 0.16666666666666666\n",
            "i 0.24925541799999715 \t 0.16666666666666666\n",
            "i 0.24925541899999715 \t 0.16666666666666666\n",
            "i 0.24925541999999715 \t 0.16666666666666666\n",
            "i 0.24925542099999715 \t 0.16666666666666666\n",
            "i 0.24925542199999715 \t 0.16666666666666666\n",
            "i 0.24925542299999714 \t 0.16666666666666666\n",
            "i 0.24925542399999714 \t 0.16666666666666666\n",
            "i 0.24925542499999714 \t 0.16666666666666666\n",
            "i 0.24925542599999714 \t 0.16666666666666666\n",
            "i 0.24925542699999714 \t 0.16666666666666666\n",
            "i 0.24925542799999714 \t 0.16666666666666666\n",
            "i 0.24925542899999714 \t 0.16666666666666666\n",
            "i 0.24925542999999714 \t 0.16666666666666666\n",
            "i 0.24925543099999714 \t 0.16666666666666666\n",
            "i 0.24925543199999714 \t 0.16666666666666666\n",
            "i 0.24925543299999714 \t 0.16666666666666666\n",
            "i 0.24925543399999714 \t 0.16666666666666666\n",
            "i 0.24925543499999714 \t 0.16666666666666666\n",
            "i 0.24925543599999714 \t 0.16666666666666666\n",
            "i 0.24925543699999714 \t 0.16666666666666666\n",
            "i 0.24925543799999714 \t 0.16666666666666666\n",
            "i 0.24925543899999714 \t 0.16666666666666666\n",
            "i 0.24925543999999714 \t 0.16666666666666666\n",
            "i 0.24925544099999714 \t 0.16666666666666666\n",
            "i 0.24925544199999713 \t 0.16666666666666666\n",
            "i 0.24925544299999713 \t 0.16666666666666666\n",
            "i 0.24925544399999713 \t 0.16666666666666666\n",
            "i 0.24925544499999713 \t 0.16666666666666666\n",
            "i 0.24925544599999713 \t 0.16666666666666666\n",
            "i 0.24925544699999713 \t 0.16666666666666666\n",
            "i 0.24925544799999713 \t 0.16666666666666666\n",
            "i 0.24925544899999713 \t 0.16666666666666666\n",
            "i 0.24925544999999713 \t 0.16666666666666666\n",
            "i 0.24925545099999713 \t 0.16666666666666666\n",
            "i 0.24925545199999713 \t 0.16666666666666666\n",
            "i 0.24925545299999713 \t 0.16666666666666666\n",
            "i 0.24925545399999713 \t 0.16666666666666666\n",
            "i 0.24925545499999713 \t 0.16666666666666666\n",
            "i 0.24925545599999713 \t 0.16666666666666666\n",
            "i 0.24925545699999713 \t 0.16666666666666666\n",
            "i 0.24925545799999713 \t 0.16666666666666666\n",
            "i 0.24925545899999713 \t 0.16666666666666666\n",
            "i 0.24925545999999713 \t 0.16666666666666666\n",
            "i 0.24925546099999712 \t 0.16666666666666666\n",
            "i 0.24925546199999712 \t 0.16666666666666666\n",
            "i 0.24925546299999712 \t 0.16666666666666666\n",
            "i 0.24925546399999712 \t 0.16666666666666666\n",
            "i 0.24925546499999712 \t 0.16666666666666666\n",
            "i 0.24925546599999712 \t 0.16666666666666666\n",
            "i 0.24925546699999712 \t 0.16666666666666666\n",
            "i 0.24925546799999712 \t 0.16666666666666666\n",
            "i 0.24925546899999712 \t 0.16666666666666666\n",
            "i 0.24925546999999712 \t 0.16666666666666666\n",
            "i 0.24925547099999712 \t 0.16666666666666666\n",
            "i 0.24925547199999712 \t 0.16666666666666666\n",
            "i 0.24925547299999712 \t 0.16666666666666666\n",
            "i 0.24925547399999712 \t 0.16666666666666666\n",
            "i 0.24925547499999712 \t 0.16666666666666666\n",
            "i 0.24925547599999712 \t 0.16666666666666666\n",
            "i 0.24925547699999712 \t 0.16666666666666666\n",
            "i 0.24925547799999712 \t 0.16666666666666666\n",
            "i 0.24925547899999712 \t 0.16666666666666666\n",
            "i 0.24925547999999711 \t 0.16666666666666666\n",
            "i 0.24925548099999711 \t 0.16666666666666666\n",
            "i 0.2492554819999971 \t 0.16666666666666666\n",
            "i 0.2492554829999971 \t 0.16666666666666666\n",
            "i 0.2492554839999971 \t 0.16666666666666666\n",
            "i 0.2492554849999971 \t 0.16666666666666666\n",
            "i 0.2492554859999971 \t 0.16666666666666666\n",
            "i 0.2492554869999971 \t 0.16666666666666666\n",
            "i 0.2492554879999971 \t 0.16666666666666666\n",
            "i 0.2492554889999971 \t 0.16666666666666666\n",
            "i 0.2492554899999971 \t 0.16666666666666666\n",
            "i 0.2492554909999971 \t 0.16666666666666666\n",
            "i 0.2492554919999971 \t 0.16666666666666666\n",
            "i 0.2492554929999971 \t 0.16666666666666666\n",
            "i 0.2492554939999971 \t 0.16666666666666666\n",
            "i 0.2492554949999971 \t 0.16666666666666666\n",
            "i 0.2492554959999971 \t 0.16666666666666666\n",
            "i 0.2492554969999971 \t 0.16666666666666666\n",
            "i 0.2492554979999971 \t 0.16666666666666666\n",
            "i 0.2492554989999971 \t 0.16666666666666666\n",
            "i 0.2492554999999971 \t 0.16666666666666666\n",
            "i 0.2492555009999971 \t 0.16666666666666666\n",
            "i 0.2492555019999971 \t 0.16666666666666666\n",
            "i 0.2492555029999971 \t 0.16666666666666666\n",
            "i 0.2492555039999971 \t 0.16666666666666666\n",
            "i 0.2492555049999971 \t 0.16666666666666666\n",
            "i 0.2492555059999971 \t 0.16666666666666666\n",
            "i 0.2492555069999971 \t 0.16666666666666666\n",
            "i 0.2492555079999971 \t 0.16666666666666666\n",
            "i 0.2492555089999971 \t 0.16666666666666666\n",
            "i 0.2492555099999971 \t 0.16666666666666666\n",
            "i 0.2492555109999971 \t 0.16666666666666666\n",
            "i 0.2492555119999971 \t 0.16666666666666666\n",
            "i 0.2492555129999971 \t 0.16666666666666666\n",
            "i 0.2492555139999971 \t 0.16666666666666666\n",
            "i 0.2492555149999971 \t 0.16666666666666666\n",
            "i 0.2492555159999971 \t 0.16666666666666666\n",
            "i 0.2492555169999971 \t 0.16666666666666666\n",
            "i 0.2492555179999971 \t 0.16666666666666666\n",
            "i 0.2492555189999971 \t 0.16666666666666666\n",
            "i 0.2492555199999971 \t 0.16666666666666666\n",
            "i 0.2492555209999971 \t 0.16666666666666666\n",
            "i 0.2492555219999971 \t 0.16666666666666666\n",
            "i 0.2492555229999971 \t 0.16666666666666666\n",
            "i 0.2492555239999971 \t 0.16666666666666666\n",
            "i 0.2492555249999971 \t 0.16666666666666666\n",
            "i 0.2492555259999971 \t 0.16666666666666666\n",
            "i 0.2492555269999971 \t 0.16666666666666666\n",
            "i 0.2492555279999971 \t 0.16666666666666666\n",
            "i 0.2492555289999971 \t 0.16666666666666666\n",
            "i 0.2492555299999971 \t 0.16666666666666666\n",
            "i 0.2492555309999971 \t 0.16666666666666666\n",
            "i 0.2492555319999971 \t 0.16666666666666666\n",
            "i 0.2492555329999971 \t 0.16666666666666666\n",
            "i 0.2492555339999971 \t 0.16666666666666666\n",
            "i 0.24925553499999709 \t 0.16666666666666666\n",
            "i 0.24925553599999709 \t 0.16666666666666666\n",
            "i 0.24925553699999708 \t 0.16666666666666666\n",
            "i 0.24925553799999708 \t 0.16666666666666666\n",
            "i 0.24925553899999708 \t 0.16666666666666666\n",
            "i 0.24925553999999708 \t 0.16666666666666666\n",
            "i 0.24925554099999708 \t 0.16666666666666666\n",
            "i 0.24925554199999708 \t 0.16666666666666666\n",
            "i 0.24925554299999708 \t 0.16666666666666666\n",
            "i 0.24925554399999708 \t 0.16666666666666666\n",
            "i 0.24925554499999708 \t 0.16666666666666666\n",
            "i 0.24925554599999708 \t 0.16666666666666666\n",
            "i 0.24925554699999708 \t 0.16666666666666666\n",
            "i 0.24925554799999708 \t 0.16666666666666666\n",
            "i 0.24925554899999708 \t 0.16666666666666666\n",
            "i 0.24925554999999708 \t 0.16666666666666666\n",
            "i 0.24925555099999708 \t 0.16666666666666666\n",
            "i 0.24925555199999708 \t 0.16666666666666666\n",
            "i 0.24925555299999708 \t 0.16666666666666666\n",
            "i 0.24925555399999708 \t 0.16666666666666666\n",
            "i 0.24925555499999708 \t 0.16666666666666666\n",
            "i 0.24925555599999707 \t 0.16666666666666666\n",
            "i 0.24925555699999707 \t 0.16666666666666666\n",
            "i 0.24925555799999707 \t 0.16666666666666666\n",
            "i 0.24925555899999707 \t 0.16666666666666666\n",
            "i 0.24925555999999707 \t 0.16666666666666666\n",
            "i 0.24925556099999707 \t 0.16666666666666666\n",
            "i 0.24925556199999707 \t 0.16666666666666666\n",
            "i 0.24925556299999707 \t 0.16666666666666666\n",
            "i 0.24925556399999707 \t 0.16666666666666666\n",
            "i 0.24925556499999707 \t 0.16666666666666666\n",
            "i 0.24925556599999707 \t 0.16666666666666666\n",
            "i 0.24925556699999707 \t 0.16666666666666666\n",
            "i 0.24925556799999707 \t 0.16666666666666666\n",
            "i 0.24925556899999707 \t 0.16666666666666666\n",
            "i 0.24925556999999707 \t 0.16666666666666666\n",
            "i 0.24925557099999707 \t 0.16666666666666666\n",
            "i 0.24925557199999707 \t 0.16666666666666666\n",
            "i 0.24925557299999707 \t 0.16666666666666666\n",
            "i 0.24925557399999707 \t 0.16666666666666666\n",
            "i 0.24925557499999706 \t 0.16666666666666666\n",
            "i 0.24925557599999706 \t 0.16666666666666666\n",
            "i 0.24925557699999706 \t 0.16666666666666666\n",
            "i 0.24925557799999706 \t 0.16666666666666666\n",
            "i 0.24925557899999706 \t 0.16666666666666666\n",
            "i 0.24925557999999706 \t 0.16666666666666666\n",
            "i 0.24925558099999706 \t 0.16666666666666666\n",
            "i 0.24925558199999706 \t 0.16666666666666666\n",
            "i 0.24925558299999706 \t 0.16666666666666666\n",
            "i 0.24925558399999706 \t 0.16666666666666666\n",
            "i 0.24925558499999706 \t 0.16666666666666666\n",
            "i 0.24925558599999706 \t 0.16666666666666666\n",
            "i 0.24925558699999706 \t 0.16666666666666666\n",
            "i 0.24925558799999706 \t 0.16666666666666666\n",
            "i 0.24925558899999706 \t 0.16666666666666666\n",
            "i 0.24925558999999706 \t 0.16666666666666666\n",
            "i 0.24925559099999706 \t 0.16666666666666666\n",
            "i 0.24925559199999706 \t 0.16666666666666666\n",
            "i 0.24925559299999706 \t 0.16666666666666666\n",
            "i 0.24925559399999705 \t 0.16666666666666666\n",
            "i 0.24925559499999705 \t 0.16666666666666666\n",
            "i 0.24925559599999705 \t 0.16666666666666666\n",
            "i 0.24925559699999705 \t 0.16666666666666666\n",
            "i 0.24925559799999705 \t 0.16666666666666666\n",
            "i 0.24925559899999705 \t 0.16666666666666666\n",
            "i 0.24925559999999705 \t 0.16666666666666666\n",
            "i 0.24925560099999705 \t 0.16666666666666666\n",
            "i 0.24925560199999705 \t 0.16666666666666666\n",
            "i 0.24925560299999705 \t 0.16666666666666666\n",
            "i 0.24925560399999705 \t 0.16666666666666666\n",
            "i 0.24925560499999705 \t 0.16666666666666666\n",
            "i 0.24925560599999705 \t 0.16666666666666666\n",
            "i 0.24925560699999705 \t 0.16666666666666666\n",
            "i 0.24925560799999705 \t 0.16666666666666666\n",
            "i 0.24925560899999705 \t 0.16666666666666666\n",
            "i 0.24925560999999705 \t 0.16666666666666666\n",
            "i 0.24925561099999705 \t 0.16666666666666666\n",
            "i 0.24925561199999705 \t 0.16666666666666666\n",
            "i 0.24925561299999704 \t 0.16666666666666666\n",
            "i 0.24925561399999704 \t 0.16666666666666666\n",
            "i 0.24925561499999704 \t 0.16666666666666666\n",
            "i 0.24925561599999704 \t 0.16666666666666666\n",
            "i 0.24925561699999704 \t 0.16666666666666666\n",
            "i 0.24925561799999704 \t 0.16666666666666666\n",
            "i 0.24925561899999704 \t 0.16666666666666666\n",
            "i 0.24925561999999704 \t 0.16666666666666666\n",
            "i 0.24925562099999704 \t 0.16666666666666666\n",
            "i 0.24925562199999704 \t 0.16666666666666666\n",
            "i 0.24925562299999704 \t 0.16666666666666666\n",
            "i 0.24925562399999704 \t 0.16666666666666666\n",
            "i 0.24925562499999704 \t 0.16666666666666666\n",
            "i 0.24925562599999704 \t 0.16666666666666666\n",
            "i 0.24925562699999704 \t 0.16666666666666666\n",
            "i 0.24925562799999704 \t 0.16666666666666666\n",
            "i 0.24925562899999704 \t 0.16666666666666666\n",
            "i 0.24925562999999704 \t 0.16666666666666666\n",
            "i 0.24925563099999704 \t 0.16666666666666666\n",
            "i 0.24925563199999703 \t 0.16666666666666666\n",
            "i 0.24925563299999703 \t 0.16666666666666666\n",
            "i 0.24925563399999703 \t 0.16666666666666666\n",
            "i 0.24925563499999703 \t 0.16666666666666666\n",
            "i 0.24925563599999703 \t 0.16666666666666666\n",
            "i 0.24925563699999703 \t 0.16666666666666666\n",
            "i 0.24925563799999703 \t 0.16666666666666666\n",
            "i 0.24925563899999703 \t 0.16666666666666666\n",
            "i 0.24925563999999703 \t 0.16666666666666666\n",
            "i 0.24925564099999703 \t 0.16666666666666666\n",
            "i 0.24925564199999703 \t 0.16666666666666666\n",
            "i 0.24925564299999703 \t 0.16666666666666666\n",
            "i 0.24925564399999703 \t 0.16666666666666666\n",
            "i 0.24925564499999703 \t 0.16666666666666666\n",
            "i 0.24925564599999703 \t 0.16666666666666666\n",
            "i 0.24925564699999703 \t 0.16666666666666666\n",
            "i 0.24925564799999703 \t 0.16666666666666666\n",
            "i 0.24925564899999703 \t 0.16666666666666666\n",
            "i 0.24925564999999703 \t 0.16666666666666666\n",
            "i 0.24925565099999702 \t 0.16666666666666666\n",
            "i 0.24925565199999702 \t 0.16666666666666666\n",
            "i 0.24925565299999702 \t 0.16666666666666666\n",
            "i 0.24925565399999702 \t 0.16666666666666666\n",
            "i 0.24925565499999702 \t 0.16666666666666666\n",
            "i 0.24925565599999702 \t 0.16666666666666666\n",
            "i 0.24925565699999702 \t 0.16666666666666666\n",
            "i 0.24925565799999702 \t 0.16666666666666666\n",
            "i 0.24925565899999702 \t 0.16666666666666666\n",
            "i 0.24925565999999702 \t 0.16666666666666666\n",
            "i 0.24925566099999702 \t 0.16666666666666666\n",
            "i 0.24925566199999702 \t 0.16666666666666666\n",
            "i 0.24925566299999702 \t 0.16666666666666666\n",
            "i 0.24925566399999702 \t 0.16666666666666666\n",
            "i 0.24925566499999702 \t 0.16666666666666666\n",
            "i 0.24925566599999702 \t 0.16666666666666666\n",
            "i 0.24925566699999702 \t 0.16666666666666666\n",
            "i 0.24925566799999702 \t 0.16666666666666666\n",
            "i 0.24925566899999702 \t 0.16666666666666666\n",
            "i 0.24925566999999701 \t 0.16666666666666666\n",
            "i 0.24925567099999701 \t 0.16666666666666666\n",
            "i 0.249255671999997 \t 0.16666666666666666\n",
            "i 0.249255672999997 \t 0.16666666666666666\n",
            "i 0.249255673999997 \t 0.16666666666666666\n",
            "i 0.249255674999997 \t 0.16666666666666666\n",
            "i 0.249255675999997 \t 0.16666666666666666\n",
            "i 0.249255676999997 \t 0.16666666666666666\n",
            "i 0.249255677999997 \t 0.16666666666666666\n",
            "i 0.249255678999997 \t 0.16666666666666666\n",
            "i 0.249255679999997 \t 0.16666666666666666\n",
            "i 0.249255680999997 \t 0.16666666666666666\n",
            "i 0.249255681999997 \t 0.16666666666666666\n",
            "i 0.249255682999997 \t 0.16666666666666666\n",
            "i 0.249255683999997 \t 0.16666666666666666\n",
            "i 0.249255684999997 \t 0.16666666666666666\n",
            "i 0.249255685999997 \t 0.16666666666666666\n",
            "i 0.249255686999997 \t 0.16666666666666666\n",
            "i 0.249255687999997 \t 0.16666666666666666\n",
            "i 0.249255688999997 \t 0.16666666666666666\n",
            "i 0.249255689999997 \t 0.16666666666666666\n",
            "i 0.249255690999997 \t 0.16666666666666666\n",
            "i 0.249255691999997 \t 0.16666666666666666\n",
            "i 0.249255692999997 \t 0.16666666666666666\n",
            "i 0.249255693999997 \t 0.16666666666666666\n",
            "i 0.249255694999997 \t 0.16666666666666666\n",
            "i 0.249255695999997 \t 0.16666666666666666\n",
            "i 0.249255696999997 \t 0.16666666666666666\n",
            "i 0.249255697999997 \t 0.16666666666666666\n",
            "i 0.249255698999997 \t 0.16666666666666666\n",
            "i 0.249255699999997 \t 0.16666666666666666\n",
            "i 0.249255700999997 \t 0.16666666666666666\n",
            "i 0.249255701999997 \t 0.16666666666666666\n",
            "i 0.249255702999997 \t 0.16666666666666666\n",
            "i 0.249255703999997 \t 0.16666666666666666\n",
            "i 0.249255704999997 \t 0.16666666666666666\n",
            "i 0.249255705999997 \t 0.16666666666666666\n",
            "i 0.249255706999997 \t 0.16666666666666666\n",
            "i 0.249255707999997 \t 0.16666666666666666\n",
            "i 0.249255708999997 \t 0.16666666666666666\n",
            "i 0.249255709999997 \t 0.16666666666666666\n",
            "i 0.249255710999997 \t 0.16666666666666666\n",
            "i 0.249255711999997 \t 0.16666666666666666\n",
            "i 0.249255712999997 \t 0.16666666666666666\n",
            "i 0.249255713999997 \t 0.16666666666666666\n",
            "i 0.249255714999997 \t 0.16666666666666666\n",
            "i 0.249255715999997 \t 0.16666666666666666\n",
            "i 0.249255716999997 \t 0.16666666666666666\n",
            "i 0.249255717999997 \t 0.16666666666666666\n",
            "i 0.249255718999997 \t 0.16666666666666666\n",
            "i 0.249255719999997 \t 0.16666666666666666\n",
            "i 0.249255720999997 \t 0.16666666666666666\n",
            "i 0.249255721999997 \t 0.16666666666666666\n",
            "i 0.249255722999997 \t 0.16666666666666666\n",
            "i 0.249255723999997 \t 0.16666666666666666\n",
            "i 0.24925572499999699 \t 0.16666666666666666\n",
            "i 0.24925572599999699 \t 0.16666666666666666\n",
            "i 0.24925572699999698 \t 0.16666666666666666\n",
            "i 0.24925572799999698 \t 0.16666666666666666\n",
            "i 0.24925572899999698 \t 0.16666666666666666\n",
            "i 0.24925572999999698 \t 0.16666666666666666\n",
            "i 0.24925573099999698 \t 0.16666666666666666\n",
            "i 0.24925573199999698 \t 0.16666666666666666\n",
            "i 0.24925573299999698 \t 0.16666666666666666\n",
            "i 0.24925573399999698 \t 0.16666666666666666\n",
            "i 0.24925573499999698 \t 0.16666666666666666\n",
            "i 0.24925573599999698 \t 0.16666666666666666\n",
            "i 0.24925573699999698 \t 0.16666666666666666\n",
            "i 0.24925573799999698 \t 0.16666666666666666\n",
            "i 0.24925573899999698 \t 0.16666666666666666\n",
            "i 0.24925573999999698 \t 0.16666666666666666\n",
            "i 0.24925574099999698 \t 0.16666666666666666\n",
            "i 0.24925574199999698 \t 0.16666666666666666\n",
            "i 0.24925574299999698 \t 0.16666666666666666\n",
            "i 0.24925574399999698 \t 0.16666666666666666\n",
            "i 0.24925574499999698 \t 0.16666666666666666\n",
            "i 0.24925574599999697 \t 0.16666666666666666\n",
            "i 0.24925574699999697 \t 0.16666666666666666\n",
            "i 0.24925574799999697 \t 0.16666666666666666\n",
            "i 0.24925574899999697 \t 0.16666666666666666\n",
            "i 0.24925574999999697 \t 0.16666666666666666\n",
            "i 0.24925575099999697 \t 0.16666666666666666\n",
            "i 0.24925575199999697 \t 0.16666666666666666\n",
            "i 0.24925575299999697 \t 0.16666666666666666\n",
            "i 0.24925575399999697 \t 0.16666666666666666\n",
            "i 0.24925575499999697 \t 0.16666666666666666\n",
            "i 0.24925575599999697 \t 0.16666666666666666\n",
            "i 0.24925575699999697 \t 0.16666666666666666\n",
            "i 0.24925575799999697 \t 0.16666666666666666\n",
            "i 0.24925575899999697 \t 0.16666666666666666\n",
            "i 0.24925575999999697 \t 0.16666666666666666\n",
            "i 0.24925576099999697 \t 0.16666666666666666\n",
            "i 0.24925576199999697 \t 0.16666666666666666\n",
            "i 0.24925576299999697 \t 0.16666666666666666\n",
            "i 0.24925576399999697 \t 0.16666666666666666\n",
            "i 0.24925576499999696 \t 0.16666666666666666\n",
            "i 0.24925576599999696 \t 0.16666666666666666\n",
            "i 0.24925576699999696 \t 0.16666666666666666\n",
            "i 0.24925576799999696 \t 0.16666666666666666\n",
            "i 0.24925576899999696 \t 0.16666666666666666\n",
            "i 0.24925576999999696 \t 0.16666666666666666\n",
            "i 0.24925577099999696 \t 0.16666666666666666\n",
            "i 0.24925577199999696 \t 0.16666666666666666\n",
            "i 0.24925577299999696 \t 0.16666666666666666\n",
            "i 0.24925577399999696 \t 0.16666666666666666\n",
            "i 0.24925577499999696 \t 0.16666666666666666\n",
            "i 0.24925577599999696 \t 0.16666666666666666\n",
            "i 0.24925577699999696 \t 0.16666666666666666\n",
            "i 0.24925577799999696 \t 0.16666666666666666\n",
            "i 0.24925577899999696 \t 0.16666666666666666\n",
            "i 0.24925577999999696 \t 0.16666666666666666\n",
            "i 0.24925578099999696 \t 0.16666666666666666\n",
            "i 0.24925578199999696 \t 0.16666666666666666\n",
            "i 0.24925578299999696 \t 0.16666666666666666\n",
            "i 0.24925578399999695 \t 0.16666666666666666\n",
            "i 0.24925578499999695 \t 0.16666666666666666\n",
            "i 0.24925578599999695 \t 0.16666666666666666\n",
            "i 0.24925578699999695 \t 0.16666666666666666\n",
            "i 0.24925578799999695 \t 0.16666666666666666\n",
            "i 0.24925578899999695 \t 0.16666666666666666\n",
            "i 0.24925578999999695 \t 0.16666666666666666\n",
            "i 0.24925579099999695 \t 0.16666666666666666\n",
            "i 0.24925579199999695 \t 0.16666666666666666\n",
            "i 0.24925579299999695 \t 0.16666666666666666\n",
            "i 0.24925579399999695 \t 0.16666666666666666\n",
            "i 0.24925579499999695 \t 0.16666666666666666\n",
            "i 0.24925579599999695 \t 0.16666666666666666\n",
            "i 0.24925579699999695 \t 0.16666666666666666\n",
            "i 0.24925579799999695 \t 0.16666666666666666\n",
            "i 0.24925579899999695 \t 0.16666666666666666\n",
            "i 0.24925579999999695 \t 0.16666666666666666\n",
            "i 0.24925580099999695 \t 0.16666666666666666\n",
            "i 0.24925580199999695 \t 0.16666666666666666\n",
            "i 0.24925580299999694 \t 0.16666666666666666\n",
            "i 0.24925580399999694 \t 0.16666666666666666\n",
            "i 0.24925580499999694 \t 0.16666666666666666\n",
            "i 0.24925580599999694 \t 0.16666666666666666\n",
            "i 0.24925580699999694 \t 0.16666666666666666\n",
            "i 0.24925580799999694 \t 0.16666666666666666\n",
            "i 0.24925580899999694 \t 0.16666666666666666\n",
            "i 0.24925580999999694 \t 0.16666666666666666\n",
            "i 0.24925581099999694 \t 0.16666666666666666\n",
            "i 0.24925581199999694 \t 0.16666666666666666\n",
            "i 0.24925581299999694 \t 0.16666666666666666\n",
            "i 0.24925581399999694 \t 0.16666666666666666\n",
            "i 0.24925581499999694 \t 0.16666666666666666\n",
            "i 0.24925581599999694 \t 0.16666666666666666\n",
            "i 0.24925581699999694 \t 0.16666666666666666\n",
            "i 0.24925581799999694 \t 0.16666666666666666\n",
            "i 0.24925581899999694 \t 0.16666666666666666\n",
            "i 0.24925581999999694 \t 0.16666666666666666\n",
            "i 0.24925582099999694 \t 0.16666666666666666\n",
            "i 0.24925582199999693 \t 0.16666666666666666\n",
            "i 0.24925582299999693 \t 0.16666666666666666\n",
            "i 0.24925582399999693 \t 0.16666666666666666\n",
            "i 0.24925582499999693 \t 0.16666666666666666\n",
            "i 0.24925582599999693 \t 0.16666666666666666\n",
            "i 0.24925582699999693 \t 0.16666666666666666\n",
            "i 0.24925582799999693 \t 0.16666666666666666\n",
            "i 0.24925582899999693 \t 0.16666666666666666\n",
            "i 0.24925582999999693 \t 0.16666666666666666\n",
            "i 0.24925583099999693 \t 0.16666666666666666\n",
            "i 0.24925583199999693 \t 0.16666666666666666\n",
            "i 0.24925583299999693 \t 0.16666666666666666\n",
            "i 0.24925583399999693 \t 0.16666666666666666\n",
            "i 0.24925583499999693 \t 0.16666666666666666\n",
            "i 0.24925583599999693 \t 0.16666666666666666\n",
            "i 0.24925583699999693 \t 0.16666666666666666\n",
            "i 0.24925583799999693 \t 0.16666666666666666\n",
            "i 0.24925583899999693 \t 0.16666666666666666\n",
            "i 0.24925583999999693 \t 0.16666666666666666\n",
            "i 0.24925584099999692 \t 0.16666666666666666\n",
            "i 0.24925584199999692 \t 0.16666666666666666\n",
            "i 0.24925584299999692 \t 0.16666666666666666\n",
            "i 0.24925584399999692 \t 0.16666666666666666\n",
            "i 0.24925584499999692 \t 0.16666666666666666\n",
            "i 0.24925584599999692 \t 0.16666666666666666\n",
            "i 0.24925584699999692 \t 0.16666666666666666\n",
            "i 0.24925584799999692 \t 0.16666666666666666\n",
            "i 0.24925584899999692 \t 0.16666666666666666\n",
            "i 0.24925584999999692 \t 0.16666666666666666\n",
            "i 0.24925585099999692 \t 0.16666666666666666\n",
            "i 0.24925585199999692 \t 0.16666666666666666\n",
            "i 0.24925585299999692 \t 0.16666666666666666\n",
            "i 0.24925585399999692 \t 0.16666666666666666\n",
            "i 0.24925585499999692 \t 0.16666666666666666\n",
            "i 0.24925585599999692 \t 0.16666666666666666\n",
            "i 0.24925585699999692 \t 0.16666666666666666\n",
            "i 0.24925585799999692 \t 0.16666666666666666\n",
            "i 0.24925585899999692 \t 0.16666666666666666\n",
            "i 0.24925585999999691 \t 0.16666666666666666\n",
            "i 0.24925586099999691 \t 0.16666666666666666\n",
            "i 0.2492558619999969 \t 0.16666666666666666\n",
            "i 0.2492558629999969 \t 0.16666666666666666\n",
            "i 0.2492558639999969 \t 0.16666666666666666\n",
            "i 0.2492558649999969 \t 0.16666666666666666\n",
            "i 0.2492558659999969 \t 0.16666666666666666\n",
            "i 0.2492558669999969 \t 0.16666666666666666\n",
            "i 0.2492558679999969 \t 0.16666666666666666\n",
            "i 0.2492558689999969 \t 0.16666666666666666\n",
            "i 0.2492558699999969 \t 0.16666666666666666\n",
            "i 0.2492558709999969 \t 0.16666666666666666\n",
            "i 0.2492558719999969 \t 0.16666666666666666\n",
            "i 0.2492558729999969 \t 0.16666666666666666\n",
            "i 0.2492558739999969 \t 0.16666666666666666\n",
            "i 0.2492558749999969 \t 0.16666666666666666\n",
            "i 0.2492558759999969 \t 0.16666666666666666\n",
            "i 0.2492558769999969 \t 0.16666666666666666\n",
            "i 0.2492558779999969 \t 0.16666666666666666\n",
            "i 0.2492558789999969 \t 0.16666666666666666\n",
            "i 0.2492558799999969 \t 0.16666666666666666\n",
            "i 0.2492558809999969 \t 0.16666666666666666\n",
            "i 0.2492558819999969 \t 0.16666666666666666\n",
            "i 0.2492558829999969 \t 0.16666666666666666\n",
            "i 0.2492558839999969 \t 0.16666666666666666\n",
            "i 0.2492558849999969 \t 0.16666666666666666\n",
            "i 0.2492558859999969 \t 0.16666666666666666\n",
            "i 0.2492558869999969 \t 0.16666666666666666\n",
            "i 0.2492558879999969 \t 0.16666666666666666\n",
            "i 0.2492558889999969 \t 0.16666666666666666\n",
            "i 0.2492558899999969 \t 0.16666666666666666\n",
            "i 0.2492558909999969 \t 0.16666666666666666\n",
            "i 0.2492558919999969 \t 0.16666666666666666\n",
            "i 0.2492558929999969 \t 0.16666666666666666\n",
            "i 0.2492558939999969 \t 0.16666666666666666\n",
            "i 0.2492558949999969 \t 0.16666666666666666\n",
            "i 0.2492558959999969 \t 0.16666666666666666\n",
            "i 0.2492558969999969 \t 0.16666666666666666\n",
            "i 0.2492558979999969 \t 0.16666666666666666\n",
            "i 0.2492558989999969 \t 0.16666666666666666\n",
            "i 0.2492558999999969 \t 0.16666666666666666\n",
            "i 0.2492559009999969 \t 0.16666666666666666\n",
            "i 0.2492559019999969 \t 0.16666666666666666\n",
            "i 0.2492559029999969 \t 0.16666666666666666\n",
            "i 0.2492559039999969 \t 0.16666666666666666\n",
            "i 0.2492559049999969 \t 0.16666666666666666\n",
            "i 0.2492559059999969 \t 0.16666666666666666\n",
            "i 0.2492559069999969 \t 0.16666666666666666\n",
            "i 0.2492559079999969 \t 0.16666666666666666\n",
            "i 0.2492559089999969 \t 0.16666666666666666\n",
            "i 0.2492559099999969 \t 0.16666666666666666\n",
            "i 0.2492559109999969 \t 0.16666666666666666\n",
            "i 0.2492559119999969 \t 0.16666666666666666\n",
            "i 0.2492559129999969 \t 0.16666666666666666\n",
            "i 0.2492559139999969 \t 0.16666666666666666\n",
            "i 0.24925591499999689 \t 0.16666666666666666\n",
            "i 0.24925591599999689 \t 0.16666666666666666\n",
            "i 0.24925591699999688 \t 0.16666666666666666\n",
            "i 0.24925591799999688 \t 0.16666666666666666\n",
            "i 0.24925591899999688 \t 0.16666666666666666\n",
            "i 0.24925591999999688 \t 0.16666666666666666\n",
            "i 0.24925592099999688 \t 0.16666666666666666\n",
            "i 0.24925592199999688 \t 0.16666666666666666\n",
            "i 0.24925592299999688 \t 0.16666666666666666\n",
            "i 0.24925592399999688 \t 0.16666666666666666\n",
            "i 0.24925592499999688 \t 0.16666666666666666\n",
            "i 0.24925592599999688 \t 0.16666666666666666\n",
            "i 0.24925592699999688 \t 0.16666666666666666\n",
            "i 0.24925592799999688 \t 0.16666666666666666\n",
            "i 0.24925592899999688 \t 0.16666666666666666\n",
            "i 0.24925592999999688 \t 0.16666666666666666\n",
            "i 0.24925593099999688 \t 0.16666666666666666\n",
            "i 0.24925593199999688 \t 0.16666666666666666\n",
            "i 0.24925593299999688 \t 0.16666666666666666\n",
            "i 0.24925593399999688 \t 0.16666666666666666\n",
            "i 0.24925593499999688 \t 0.16666666666666666\n",
            "i 0.24925593599999687 \t 0.16666666666666666\n",
            "i 0.24925593699999687 \t 0.16666666666666666\n",
            "i 0.24925593799999687 \t 0.16666666666666666\n",
            "i 0.24925593899999687 \t 0.16666666666666666\n",
            "i 0.24925593999999687 \t 0.16666666666666666\n",
            "i 0.24925594099999687 \t 0.16666666666666666\n",
            "i 0.24925594199999687 \t 0.16666666666666666\n",
            "i 0.24925594299999687 \t 0.16666666666666666\n",
            "i 0.24925594399999687 \t 0.16666666666666666\n",
            "i 0.24925594499999687 \t 0.16666666666666666\n",
            "i 0.24925594599999687 \t 0.16666666666666666\n",
            "i 0.24925594699999687 \t 0.16666666666666666\n",
            "i 0.24925594799999687 \t 0.16666666666666666\n",
            "i 0.24925594899999687 \t 0.16666666666666666\n",
            "i 0.24925594999999687 \t 0.16666666666666666\n",
            "i 0.24925595099999687 \t 0.16666666666666666\n",
            "i 0.24925595199999687 \t 0.16666666666666666\n",
            "i 0.24925595299999687 \t 0.16666666666666666\n",
            "i 0.24925595399999687 \t 0.16666666666666666\n",
            "i 0.24925595499999686 \t 0.16666666666666666\n",
            "i 0.24925595599999686 \t 0.16666666666666666\n",
            "i 0.24925595699999686 \t 0.16666666666666666\n",
            "i 0.24925595799999686 \t 0.16666666666666666\n",
            "i 0.24925595899999686 \t 0.16666666666666666\n",
            "i 0.24925595999999686 \t 0.16666666666666666\n",
            "i 0.24925596099999686 \t 0.16666666666666666\n",
            "i 0.24925596199999686 \t 0.16666666666666666\n",
            "i 0.24925596299999686 \t 0.16666666666666666\n",
            "i 0.24925596399999686 \t 0.16666666666666666\n",
            "i 0.24925596499999686 \t 0.16666666666666666\n",
            "i 0.24925596599999686 \t 0.16666666666666666\n",
            "i 0.24925596699999686 \t 0.16666666666666666\n",
            "i 0.24925596799999686 \t 0.16666666666666666\n",
            "i 0.24925596899999686 \t 0.16666666666666666\n",
            "i 0.24925596999999686 \t 0.16666666666666666\n",
            "i 0.24925597099999686 \t 0.16666666666666666\n",
            "i 0.24925597199999686 \t 0.16666666666666666\n",
            "i 0.24925597299999686 \t 0.16666666666666666\n",
            "i 0.24925597399999685 \t 0.16666666666666666\n",
            "i 0.24925597499999685 \t 0.16666666666666666\n",
            "i 0.24925597599999685 \t 0.16666666666666666\n",
            "i 0.24925597699999685 \t 0.16666666666666666\n",
            "i 0.24925597799999685 \t 0.16666666666666666\n",
            "i 0.24925597899999685 \t 0.16666666666666666\n",
            "i 0.24925597999999685 \t 0.16666666666666666\n",
            "i 0.24925598099999685 \t 0.16666666666666666\n",
            "i 0.24925598199999685 \t 0.16666666666666666\n",
            "i 0.24925598299999685 \t 0.16666666666666666\n",
            "i 0.24925598399999685 \t 0.16666666666666666\n",
            "i 0.24925598499999685 \t 0.16666666666666666\n",
            "i 0.24925598599999685 \t 0.16666666666666666\n",
            "i 0.24925598699999685 \t 0.16666666666666666\n",
            "i 0.24925598799999685 \t 0.16666666666666666\n",
            "i 0.24925598899999685 \t 0.16666666666666666\n",
            "i 0.24925598999999685 \t 0.16666666666666666\n",
            "i 0.24925599099999685 \t 0.16666666666666666\n",
            "i 0.24925599199999685 \t 0.16666666666666666\n",
            "i 0.24925599299999684 \t 0.16666666666666666\n",
            "i 0.24925599399999684 \t 0.16666666666666666\n",
            "i 0.24925599499999684 \t 0.16666666666666666\n",
            "i 0.24925599599999684 \t 0.16666666666666666\n",
            "i 0.24925599699999684 \t 0.16666666666666666\n",
            "i 0.24925599799999684 \t 0.16666666666666666\n",
            "i 0.24925599899999684 \t 0.16666666666666666\n",
            "i 0.24925599999999684 \t 0.16666666666666666\n",
            "i 0.24925600099999684 \t 0.16666666666666666\n",
            "i 0.24925600199999684 \t 0.16666666666666666\n",
            "i 0.24925600299999684 \t 0.16666666666666666\n",
            "i 0.24925600399999684 \t 0.16666666666666666\n",
            "i 0.24925600499999684 \t 0.16666666666666666\n",
            "i 0.24925600599999684 \t 0.16666666666666666\n",
            "i 0.24925600699999684 \t 0.16666666666666666\n",
            "i 0.24925600799999684 \t 0.16666666666666666\n",
            "i 0.24925600899999684 \t 0.16666666666666666\n",
            "i 0.24925600999999684 \t 0.16666666666666666\n",
            "i 0.24925601099999684 \t 0.16666666666666666\n",
            "i 0.24925601199999683 \t 0.16666666666666666\n",
            "i 0.24925601299999683 \t 0.16666666666666666\n",
            "i 0.24925601399999683 \t 0.16666666666666666\n",
            "i 0.24925601499999683 \t 0.16666666666666666\n",
            "i 0.24925601599999683 \t 0.16666666666666666\n",
            "i 0.24925601699999683 \t 0.16666666666666666\n",
            "i 0.24925601799999683 \t 0.16666666666666666\n",
            "i 0.24925601899999683 \t 0.16666666666666666\n",
            "i 0.24925601999999683 \t 0.16666666666666666\n",
            "i 0.24925602099999683 \t 0.16666666666666666\n",
            "i 0.24925602199999683 \t 0.16666666666666666\n",
            "i 0.24925602299999683 \t 0.16666666666666666\n",
            "i 0.24925602399999683 \t 0.16666666666666666\n",
            "i 0.24925602499999683 \t 0.16666666666666666\n",
            "i 0.24925602599999683 \t 0.16666666666666666\n",
            "i 0.24925602699999683 \t 0.16666666666666666\n",
            "i 0.24925602799999683 \t 0.16666666666666666\n",
            "i 0.24925602899999683 \t 0.16666666666666666\n",
            "i 0.24925602999999683 \t 0.16666666666666666\n",
            "i 0.24925603099999682 \t 0.16666666666666666\n",
            "i 0.24925603199999682 \t 0.16666666666666666\n",
            "i 0.24925603299999682 \t 0.16666666666666666\n",
            "i 0.24925603399999682 \t 0.16666666666666666\n",
            "i 0.24925603499999682 \t 0.16666666666666666\n",
            "i 0.24925603599999682 \t 0.16666666666666666\n",
            "i 0.24925603699999682 \t 0.16666666666666666\n",
            "i 0.24925603799999682 \t 0.16666666666666666\n",
            "i 0.24925603899999682 \t 0.16666666666666666\n",
            "i 0.24925603999999682 \t 0.16666666666666666\n",
            "i 0.24925604099999682 \t 0.16666666666666666\n",
            "i 0.24925604199999682 \t 0.16666666666666666\n",
            "i 0.24925604299999682 \t 0.16666666666666666\n",
            "i 0.24925604399999682 \t 0.16666666666666666\n",
            "i 0.24925604499999682 \t 0.16666666666666666\n",
            "i 0.24925604599999682 \t 0.16666666666666666\n",
            "i 0.24925604699999682 \t 0.16666666666666666\n",
            "i 0.24925604799999682 \t 0.16666666666666666\n",
            "i 0.24925604899999682 \t 0.16666666666666666\n",
            "i 0.24925604999999681 \t 0.16666666666666666\n",
            "i 0.24925605099999681 \t 0.16666666666666666\n",
            "i 0.2492560519999968 \t 0.16666666666666666\n",
            "i 0.2492560529999968 \t 0.16666666666666666\n",
            "i 0.2492560539999968 \t 0.16666666666666666\n",
            "i 0.2492560549999968 \t 0.16666666666666666\n",
            "i 0.2492560559999968 \t 0.16666666666666666\n",
            "i 0.2492560569999968 \t 0.16666666666666666\n",
            "i 0.2492560579999968 \t 0.16666666666666666\n",
            "i 0.2492560589999968 \t 0.16666666666666666\n",
            "i 0.2492560599999968 \t 0.16666666666666666\n",
            "i 0.2492560609999968 \t 0.16666666666666666\n",
            "i 0.2492560619999968 \t 0.16666666666666666\n",
            "i 0.2492560629999968 \t 0.16666666666666666\n",
            "i 0.2492560639999968 \t 0.16666666666666666\n",
            "i 0.2492560649999968 \t 0.16666666666666666\n",
            "i 0.2492560659999968 \t 0.16666666666666666\n",
            "i 0.2492560669999968 \t 0.16666666666666666\n",
            "i 0.2492560679999968 \t 0.16666666666666666\n",
            "i 0.2492560689999968 \t 0.16666666666666666\n",
            "i 0.2492560699999968 \t 0.16666666666666666\n",
            "i 0.2492560709999968 \t 0.16666666666666666\n",
            "i 0.2492560719999968 \t 0.16666666666666666\n",
            "i 0.2492560729999968 \t 0.16666666666666666\n",
            "i 0.2492560739999968 \t 0.16666666666666666\n",
            "i 0.2492560749999968 \t 0.16666666666666666\n",
            "i 0.2492560759999968 \t 0.16666666666666666\n",
            "i 0.2492560769999968 \t 0.16666666666666666\n",
            "i 0.2492560779999968 \t 0.16666666666666666\n",
            "i 0.2492560789999968 \t 0.16666666666666666\n",
            "i 0.2492560799999968 \t 0.16666666666666666\n",
            "i 0.2492560809999968 \t 0.16666666666666666\n",
            "i 0.2492560819999968 \t 0.16666666666666666\n",
            "i 0.2492560829999968 \t 0.16666666666666666\n",
            "i 0.2492560839999968 \t 0.16666666666666666\n",
            "i 0.2492560849999968 \t 0.16666666666666666\n",
            "i 0.2492560859999968 \t 0.16666666666666666\n",
            "i 0.2492560869999968 \t 0.16666666666666666\n",
            "i 0.2492560879999968 \t 0.16666666666666666\n",
            "i 0.2492560889999968 \t 0.16666666666666666\n",
            "i 0.2492560899999968 \t 0.16666666666666666\n",
            "i 0.2492560909999968 \t 0.16666666666666666\n",
            "i 0.2492560919999968 \t 0.16666666666666666\n",
            "i 0.2492560929999968 \t 0.16666666666666666\n",
            "i 0.2492560939999968 \t 0.16666666666666666\n",
            "i 0.2492560949999968 \t 0.16666666666666666\n",
            "i 0.2492560959999968 \t 0.16666666666666666\n",
            "i 0.2492560969999968 \t 0.16666666666666666\n",
            "i 0.2492560979999968 \t 0.16666666666666666\n",
            "i 0.2492560989999968 \t 0.16666666666666666\n",
            "i 0.2492560999999968 \t 0.16666666666666666\n",
            "i 0.2492561009999968 \t 0.16666666666666666\n",
            "i 0.2492561019999968 \t 0.16666666666666666\n",
            "i 0.2492561029999968 \t 0.16666666666666666\n",
            "i 0.2492561039999968 \t 0.16666666666666666\n",
            "i 0.24925610499999679 \t 0.16666666666666666\n",
            "i 0.24925610599999679 \t 0.16666666666666666\n",
            "i 0.24925610699999678 \t 0.16666666666666666\n",
            "i 0.24925610799999678 \t 0.16666666666666666\n",
            "i 0.24925610899999678 \t 0.16666666666666666\n",
            "i 0.24925610999999678 \t 0.16666666666666666\n",
            "i 0.24925611099999678 \t 0.16666666666666666\n",
            "i 0.24925611199999678 \t 0.16666666666666666\n",
            "i 0.24925611299999678 \t 0.16666666666666666\n",
            "i 0.24925611399999678 \t 0.16666666666666666\n",
            "i 0.24925611499999678 \t 0.16666666666666666\n",
            "i 0.24925611599999678 \t 0.16666666666666666\n",
            "i 0.24925611699999678 \t 0.16666666666666666\n",
            "i 0.24925611799999678 \t 0.16666666666666666\n",
            "i 0.24925611899999678 \t 0.16666666666666666\n",
            "i 0.24925611999999678 \t 0.16666666666666666\n",
            "i 0.24925612099999678 \t 0.16666666666666666\n",
            "i 0.24925612199999678 \t 0.16666666666666666\n",
            "i 0.24925612299999678 \t 0.16666666666666666\n",
            "i 0.24925612399999678 \t 0.16666666666666666\n",
            "i 0.24925612499999678 \t 0.16666666666666666\n",
            "i 0.24925612599999677 \t 0.16666666666666666\n",
            "i 0.24925612699999677 \t 0.16666666666666666\n",
            "i 0.24925612799999677 \t 0.16666666666666666\n",
            "i 0.24925612899999677 \t 0.16666666666666666\n",
            "i 0.24925612999999677 \t 0.16666666666666666\n",
            "i 0.24925613099999677 \t 0.16666666666666666\n",
            "i 0.24925613199999677 \t 0.16666666666666666\n",
            "i 0.24925613299999677 \t 0.16666666666666666\n",
            "i 0.24925613399999677 \t 0.16666666666666666\n",
            "i 0.24925613499999677 \t 0.16666666666666666\n",
            "i 0.24925613599999677 \t 0.16666666666666666\n",
            "i 0.24925613699999677 \t 0.16666666666666666\n",
            "i 0.24925613799999677 \t 0.16666666666666666\n",
            "i 0.24925613899999677 \t 0.16666666666666666\n",
            "i 0.24925613999999677 \t 0.16666666666666666\n",
            "i 0.24925614099999677 \t 0.16666666666666666\n",
            "i 0.24925614199999677 \t 0.16666666666666666\n",
            "i 0.24925614299999677 \t 0.16666666666666666\n",
            "i 0.24925614399999677 \t 0.16666666666666666\n",
            "i 0.24925614499999676 \t 0.16666666666666666\n",
            "i 0.24925614599999676 \t 0.16666666666666666\n",
            "i 0.24925614699999676 \t 0.16666666666666666\n",
            "i 0.24925614799999676 \t 0.16666666666666666\n",
            "i 0.24925614899999676 \t 0.16666666666666666\n",
            "i 0.24925614999999676 \t 0.16666666666666666\n",
            "i 0.24925615099999676 \t 0.16666666666666666\n",
            "i 0.24925615199999676 \t 0.16666666666666666\n",
            "i 0.24925615299999676 \t 0.16666666666666666\n",
            "i 0.24925615399999676 \t 0.16666666666666666\n",
            "i 0.24925615499999676 \t 0.16666666666666666\n",
            "i 0.24925615599999676 \t 0.16666666666666666\n",
            "i 0.24925615699999676 \t 0.16666666666666666\n",
            "i 0.24925615799999676 \t 0.16666666666666666\n",
            "i 0.24925615899999676 \t 0.16666666666666666\n",
            "i 0.24925615999999676 \t 0.16666666666666666\n",
            "i 0.24925616099999676 \t 0.16666666666666666\n",
            "i 0.24925616199999676 \t 0.16666666666666666\n",
            "i 0.24925616299999676 \t 0.16666666666666666\n",
            "i 0.24925616399999675 \t 0.16666666666666666\n",
            "i 0.24925616499999675 \t 0.16666666666666666\n",
            "i 0.24925616599999675 \t 0.16666666666666666\n",
            "i 0.24925616699999675 \t 0.16666666666666666\n",
            "i 0.24925616799999675 \t 0.16666666666666666\n",
            "i 0.24925616899999675 \t 0.16666666666666666\n",
            "i 0.24925616999999675 \t 0.16666666666666666\n",
            "i 0.24925617099999675 \t 0.16666666666666666\n",
            "i 0.24925617199999675 \t 0.16666666666666666\n",
            "i 0.24925617299999675 \t 0.16666666666666666\n",
            "i 0.24925617399999675 \t 0.16666666666666666\n",
            "i 0.24925617499999675 \t 0.16666666666666666\n",
            "i 0.24925617599999675 \t 0.16666666666666666\n",
            "i 0.24925617699999675 \t 0.16666666666666666\n",
            "i 0.24925617799999675 \t 0.16666666666666666\n",
            "i 0.24925617899999675 \t 0.16666666666666666\n",
            "i 0.24925617999999675 \t 0.16666666666666666\n",
            "i 0.24925618099999675 \t 0.16666666666666666\n",
            "i 0.24925618199999675 \t 0.16666666666666666\n",
            "i 0.24925618299999674 \t 0.16666666666666666\n",
            "i 0.24925618399999674 \t 0.16666666666666666\n",
            "i 0.24925618499999674 \t 0.16666666666666666\n",
            "i 0.24925618599999674 \t 0.16666666666666666\n",
            "i 0.24925618699999674 \t 0.16666666666666666\n",
            "i 0.24925618799999674 \t 0.16666666666666666\n",
            "i 0.24925618899999674 \t 0.16666666666666666\n",
            "i 0.24925618999999674 \t 0.16666666666666666\n",
            "i 0.24925619099999674 \t 0.16666666666666666\n",
            "i 0.24925619199999674 \t 0.16666666666666666\n",
            "i 0.24925619299999674 \t 0.16666666666666666\n",
            "i 0.24925619399999674 \t 0.16666666666666666\n",
            "i 0.24925619499999674 \t 0.16666666666666666\n",
            "i 0.24925619599999674 \t 0.16666666666666666\n",
            "i 0.24925619699999674 \t 0.16666666666666666\n",
            "i 0.24925619799999674 \t 0.16666666666666666\n",
            "i 0.24925619899999674 \t 0.16666666666666666\n",
            "i 0.24925619999999674 \t 0.16666666666666666\n",
            "i 0.24925620099999674 \t 0.16666666666666666\n",
            "i 0.24925620199999673 \t 0.16666666666666666\n",
            "i 0.24925620299999673 \t 0.16666666666666666\n",
            "i 0.24925620399999673 \t 0.16666666666666666\n",
            "i 0.24925620499999673 \t 0.16666666666666666\n",
            "i 0.24925620599999673 \t 0.16666666666666666\n",
            "i 0.24925620699999673 \t 0.16666666666666666\n",
            "i 0.24925620799999673 \t 0.16666666666666666\n",
            "i 0.24925620899999673 \t 0.16666666666666666\n",
            "i 0.24925620999999673 \t 0.16666666666666666\n",
            "i 0.24925621099999673 \t 0.16666666666666666\n",
            "i 0.24925621199999673 \t 0.16666666666666666\n",
            "i 0.24925621299999673 \t 0.16666666666666666\n",
            "i 0.24925621399999673 \t 0.16666666666666666\n",
            "i 0.24925621499999673 \t 0.16666666666666666\n",
            "i 0.24925621599999673 \t 0.16666666666666666\n",
            "i 0.24925621699999673 \t 0.16666666666666666\n",
            "i 0.24925621799999673 \t 0.16666666666666666\n",
            "i 0.24925621899999673 \t 0.16666666666666666\n",
            "i 0.24925621999999673 \t 0.16666666666666666\n",
            "i 0.24925622099999672 \t 0.16666666666666666\n",
            "i 0.24925622199999672 \t 0.16666666666666666\n",
            "i 0.24925622299999672 \t 0.16666666666666666\n",
            "i 0.24925622399999672 \t 0.16666666666666666\n",
            "i 0.24925622499999672 \t 0.16666666666666666\n",
            "i 0.24925622599999672 \t 0.16666666666666666\n",
            "i 0.24925622699999672 \t 0.16666666666666666\n",
            "i 0.24925622799999672 \t 0.16666666666666666\n",
            "i 0.24925622899999672 \t 0.16666666666666666\n",
            "i 0.24925622999999672 \t 0.16666666666666666\n",
            "i 0.24925623099999672 \t 0.16666666666666666\n",
            "i 0.24925623199999672 \t 0.16666666666666666\n",
            "i 0.24925623299999672 \t 0.16666666666666666\n",
            "i 0.24925623399999672 \t 0.16666666666666666\n",
            "i 0.24925623499999672 \t 0.16666666666666666\n",
            "i 0.24925623599999672 \t 0.16666666666666666\n",
            "i 0.24925623699999672 \t 0.16666666666666666\n",
            "i 0.24925623799999672 \t 0.16666666666666666\n",
            "i 0.24925623899999672 \t 0.16666666666666666\n",
            "i 0.24925623999999671 \t 0.16666666666666666\n",
            "i 0.24925624099999671 \t 0.16666666666666666\n",
            "i 0.2492562419999967 \t 0.16666666666666666\n",
            "i 0.2492562429999967 \t 0.16666666666666666\n",
            "i 0.2492562439999967 \t 0.16666666666666666\n",
            "i 0.2492562449999967 \t 0.16666666666666666\n",
            "i 0.2492562459999967 \t 0.16666666666666666\n",
            "i 0.2492562469999967 \t 0.16666666666666666\n",
            "i 0.2492562479999967 \t 0.16666666666666666\n",
            "i 0.2492562489999967 \t 0.16666666666666666\n",
            "i 0.2492562499999967 \t 0.16666666666666666\n",
            "i 0.2492562509999967 \t 0.16666666666666666\n",
            "i 0.2492562519999967 \t 0.16666666666666666\n",
            "i 0.2492562529999967 \t 0.16666666666666666\n",
            "i 0.2492562539999967 \t 0.16666666666666666\n",
            "i 0.2492562549999967 \t 0.16666666666666666\n",
            "i 0.2492562559999967 \t 0.16666666666666666\n",
            "i 0.2492562569999967 \t 0.16666666666666666\n",
            "i 0.2492562579999967 \t 0.16666666666666666\n",
            "i 0.2492562589999967 \t 0.16666666666666666\n",
            "i 0.2492562599999967 \t 0.16666666666666666\n",
            "i 0.2492562609999967 \t 0.16666666666666666\n",
            "i 0.2492562619999967 \t 0.16666666666666666\n",
            "i 0.2492562629999967 \t 0.16666666666666666\n",
            "i 0.2492562639999967 \t 0.16666666666666666\n",
            "i 0.2492562649999967 \t 0.16666666666666666\n",
            "i 0.2492562659999967 \t 0.16666666666666666\n",
            "i 0.2492562669999967 \t 0.16666666666666666\n",
            "i 0.2492562679999967 \t 0.16666666666666666\n",
            "i 0.2492562689999967 \t 0.16666666666666666\n",
            "i 0.2492562699999967 \t 0.16666666666666666\n",
            "i 0.2492562709999967 \t 0.16666666666666666\n",
            "i 0.2492562719999967 \t 0.16666666666666666\n",
            "i 0.2492562729999967 \t 0.16666666666666666\n",
            "i 0.2492562739999967 \t 0.16666666666666666\n",
            "i 0.2492562749999967 \t 0.16666666666666666\n",
            "i 0.2492562759999967 \t 0.16666666666666666\n",
            "i 0.2492562769999967 \t 0.16666666666666666\n",
            "i 0.2492562779999967 \t 0.16666666666666666\n",
            "i 0.2492562789999967 \t 0.16666666666666666\n",
            "i 0.2492562799999967 \t 0.16666666666666666\n",
            "i 0.2492562809999967 \t 0.16666666666666666\n",
            "i 0.2492562819999967 \t 0.16666666666666666\n",
            "i 0.2492562829999967 \t 0.16666666666666666\n",
            "i 0.2492562839999967 \t 0.16666666666666666\n",
            "i 0.2492562849999967 \t 0.16666666666666666\n",
            "i 0.2492562859999967 \t 0.16666666666666666\n",
            "i 0.2492562869999967 \t 0.16666666666666666\n",
            "i 0.2492562879999967 \t 0.16666666666666666\n",
            "i 0.2492562889999967 \t 0.16666666666666666\n",
            "i 0.2492562899999967 \t 0.16666666666666666\n",
            "i 0.2492562909999967 \t 0.16666666666666666\n",
            "i 0.2492562919999967 \t 0.16666666666666666\n",
            "i 0.2492562929999967 \t 0.16666666666666666\n",
            "i 0.2492562939999967 \t 0.16666666666666666\n",
            "i 0.24925629499999669 \t 0.16666666666666666\n",
            "i 0.24925629599999669 \t 0.16666666666666666\n",
            "i 0.24925629699999668 \t 0.16666666666666666\n",
            "i 0.24925629799999668 \t 0.16666666666666666\n",
            "i 0.24925629899999668 \t 0.16666666666666666\n",
            "i 0.24925629999999668 \t 0.16666666666666666\n",
            "i 0.24925630099999668 \t 0.16666666666666666\n",
            "i 0.24925630199999668 \t 0.16666666666666666\n",
            "i 0.24925630299999668 \t 0.16666666666666666\n",
            "i 0.24925630399999668 \t 0.16666666666666666\n",
            "i 0.24925630499999668 \t 0.16666666666666666\n",
            "i 0.24925630599999668 \t 0.16666666666666666\n",
            "i 0.24925630699999668 \t 0.16666666666666666\n",
            "i 0.24925630799999668 \t 0.16666666666666666\n",
            "i 0.24925630899999668 \t 0.16666666666666666\n",
            "i 0.24925630999999668 \t 0.16666666666666666\n",
            "i 0.24925631099999668 \t 0.16666666666666666\n",
            "i 0.24925631199999668 \t 0.16666666666666666\n",
            "i 0.24925631299999668 \t 0.16666666666666666\n",
            "i 0.24925631399999668 \t 0.16666666666666666\n",
            "i 0.24925631499999668 \t 0.16666666666666666\n",
            "i 0.24925631599999667 \t 0.16666666666666666\n",
            "i 0.24925631699999667 \t 0.16666666666666666\n",
            "i 0.24925631799999667 \t 0.16666666666666666\n",
            "i 0.24925631899999667 \t 0.16666666666666666\n",
            "i 0.24925631999999667 \t 0.16666666666666666\n",
            "i 0.24925632099999667 \t 0.16666666666666666\n",
            "i 0.24925632199999667 \t 0.16666666666666666\n",
            "i 0.24925632299999667 \t 0.16666666666666666\n",
            "i 0.24925632399999667 \t 0.16666666666666666\n",
            "i 0.24925632499999667 \t 0.16666666666666666\n",
            "i 0.24925632599999667 \t 0.16666666666666666\n",
            "i 0.24925632699999667 \t 0.16666666666666666\n",
            "i 0.24925632799999667 \t 0.16666666666666666\n",
            "i 0.24925632899999667 \t 0.16666666666666666\n",
            "i 0.24925632999999667 \t 0.16666666666666666\n",
            "i 0.24925633099999667 \t 0.16666666666666666\n",
            "i 0.24925633199999667 \t 0.16666666666666666\n",
            "i 0.24925633299999667 \t 0.16666666666666666\n",
            "i 0.24925633399999667 \t 0.16666666666666666\n",
            "i 0.24925633499999666 \t 0.16666666666666666\n",
            "i 0.24925633599999666 \t 0.16666666666666666\n",
            "i 0.24925633699999666 \t 0.16666666666666666\n",
            "i 0.24925633799999666 \t 0.16666666666666666\n",
            "i 0.24925633899999666 \t 0.16666666666666666\n",
            "i 0.24925633999999666 \t 0.16666666666666666\n",
            "i 0.24925634099999666 \t 0.16666666666666666\n",
            "i 0.24925634199999666 \t 0.16666666666666666\n",
            "i 0.24925634299999666 \t 0.16666666666666666\n",
            "i 0.24925634399999666 \t 0.16666666666666666\n",
            "i 0.24925634499999666 \t 0.16666666666666666\n",
            "i 0.24925634599999666 \t 0.16666666666666666\n",
            "i 0.24925634699999666 \t 0.16666666666666666\n",
            "i 0.24925634799999666 \t 0.16666666666666666\n",
            "i 0.24925634899999666 \t 0.16666666666666666\n",
            "i 0.24925634999999666 \t 0.16666666666666666\n",
            "i 0.24925635099999666 \t 0.16666666666666666\n",
            "i 0.24925635199999666 \t 0.16666666666666666\n",
            "i 0.24925635299999666 \t 0.16666666666666666\n",
            "i 0.24925635399999665 \t 0.16666666666666666\n",
            "i 0.24925635499999665 \t 0.16666666666666666\n",
            "i 0.24925635599999665 \t 0.16666666666666666\n",
            "i 0.24925635699999665 \t 0.16666666666666666\n",
            "i 0.24925635799999665 \t 0.16666666666666666\n",
            "i 0.24925635899999665 \t 0.16666666666666666\n",
            "i 0.24925635999999665 \t 0.16666666666666666\n",
            "i 0.24925636099999665 \t 0.16666666666666666\n",
            "i 0.24925636199999665 \t 0.16666666666666666\n",
            "i 0.24925636299999665 \t 0.16666666666666666\n",
            "i 0.24925636399999665 \t 0.16666666666666666\n",
            "i 0.24925636499999665 \t 0.16666666666666666\n",
            "i 0.24925636599999665 \t 0.16666666666666666\n",
            "i 0.24925636699999665 \t 0.16666666666666666\n",
            "i 0.24925636799999665 \t 0.16666666666666666\n",
            "i 0.24925636899999665 \t 0.16666666666666666\n",
            "i 0.24925636999999665 \t 0.16666666666666666\n",
            "i 0.24925637099999665 \t 0.16666666666666666\n",
            "i 0.24925637199999665 \t 0.16666666666666666\n",
            "i 0.24925637299999664 \t 0.16666666666666666\n",
            "i 0.24925637399999664 \t 0.16666666666666666\n",
            "i 0.24925637499999664 \t 0.16666666666666666\n",
            "i 0.24925637599999664 \t 0.16666666666666666\n",
            "i 0.24925637699999664 \t 0.16666666666666666\n",
            "i 0.24925637799999664 \t 0.16666666666666666\n",
            "i 0.24925637899999664 \t 0.16666666666666666\n",
            "i 0.24925637999999664 \t 0.16666666666666666\n",
            "i 0.24925638099999664 \t 0.16666666666666666\n",
            "i 0.24925638199999664 \t 0.16666666666666666\n",
            "i 0.24925638299999664 \t 0.16666666666666666\n",
            "i 0.24925638399999664 \t 0.16666666666666666\n",
            "i 0.24925638499999664 \t 0.16666666666666666\n",
            "i 0.24925638599999664 \t 0.16666666666666666\n",
            "i 0.24925638699999664 \t 0.16666666666666666\n",
            "i 0.24925638799999664 \t 0.16666666666666666\n",
            "i 0.24925638899999664 \t 0.16666666666666666\n",
            "i 0.24925638999999664 \t 0.16666666666666666\n",
            "i 0.24925639099999664 \t 0.16666666666666666\n",
            "i 0.24925639199999663 \t 0.16666666666666666\n",
            "i 0.24925639299999663 \t 0.16666666666666666\n",
            "i 0.24925639399999663 \t 0.16666666666666666\n",
            "i 0.24925639499999663 \t 0.16666666666666666\n",
            "i 0.24925639599999663 \t 0.16666666666666666\n",
            "i 0.24925639699999663 \t 0.16666666666666666\n",
            "i 0.24925639799999663 \t 0.16666666666666666\n",
            "i 0.24925639899999663 \t 0.16666666666666666\n",
            "i 0.24925639999999663 \t 0.16666666666666666\n",
            "i 0.24925640099999663 \t 0.16666666666666666\n",
            "i 0.24925640199999663 \t 0.16666666666666666\n",
            "i 0.24925640299999663 \t 0.16666666666666666\n",
            "i 0.24925640399999663 \t 0.16666666666666666\n",
            "i 0.24925640499999663 \t 0.16666666666666666\n",
            "i 0.24925640599999663 \t 0.16666666666666666\n",
            "i 0.24925640699999663 \t 0.16666666666666666\n",
            "i 0.24925640799999663 \t 0.16666666666666666\n",
            "i 0.24925640899999663 \t 0.16666666666666666\n",
            "i 0.24925640999999663 \t 0.16666666666666666\n",
            "i 0.24925641099999662 \t 0.16666666666666666\n",
            "i 0.24925641199999662 \t 0.16666666666666666\n",
            "i 0.24925641299999662 \t 0.16666666666666666\n",
            "i 0.24925641399999662 \t 0.16666666666666666\n",
            "i 0.24925641499999662 \t 0.16666666666666666\n",
            "i 0.24925641599999662 \t 0.16666666666666666\n",
            "i 0.24925641699999662 \t 0.16666666666666666\n",
            "i 0.24925641799999662 \t 0.16666666666666666\n",
            "i 0.24925641899999662 \t 0.16666666666666666\n",
            "i 0.24925641999999662 \t 0.16666666666666666\n",
            "i 0.24925642099999662 \t 0.16666666666666666\n",
            "i 0.24925642199999662 \t 0.16666666666666666\n",
            "i 0.24925642299999662 \t 0.16666666666666666\n",
            "i 0.24925642399999662 \t 0.16666666666666666\n",
            "i 0.24925642499999662 \t 0.16666666666666666\n",
            "i 0.24925642599999662 \t 0.16666666666666666\n",
            "i 0.24925642699999662 \t 0.16666666666666666\n",
            "i 0.24925642799999662 \t 0.16666666666666666\n",
            "i 0.24925642899999662 \t 0.16666666666666666\n",
            "i 0.24925642999999661 \t 0.16666666666666666\n",
            "i 0.24925643099999661 \t 0.16666666666666666\n",
            "i 0.2492564319999966 \t 0.16666666666666666\n",
            "i 0.2492564329999966 \t 0.16666666666666666\n",
            "i 0.2492564339999966 \t 0.16666666666666666\n",
            "i 0.2492564349999966 \t 0.16666666666666666\n",
            "i 0.2492564359999966 \t 0.16666666666666666\n",
            "i 0.2492564369999966 \t 0.16666666666666666\n",
            "i 0.2492564379999966 \t 0.16666666666666666\n",
            "i 0.2492564389999966 \t 0.16666666666666666\n",
            "i 0.2492564399999966 \t 0.16666666666666666\n",
            "i 0.2492564409999966 \t 0.16666666666666666\n",
            "i 0.2492564419999966 \t 0.16666666666666666\n",
            "i 0.2492564429999966 \t 0.16666666666666666\n",
            "i 0.2492564439999966 \t 0.16666666666666666\n",
            "i 0.2492564449999966 \t 0.16666666666666666\n",
            "i 0.2492564459999966 \t 0.16666666666666666\n",
            "i 0.2492564469999966 \t 0.16666666666666666\n",
            "i 0.2492564479999966 \t 0.16666666666666666\n",
            "i 0.2492564489999966 \t 0.16666666666666666\n",
            "i 0.2492564499999966 \t 0.16666666666666666\n",
            "i 0.2492564509999966 \t 0.16666666666666666\n",
            "i 0.2492564519999966 \t 0.16666666666666666\n",
            "i 0.2492564529999966 \t 0.16666666666666666\n",
            "i 0.2492564539999966 \t 0.16666666666666666\n",
            "i 0.2492564549999966 \t 0.16666666666666666\n",
            "i 0.2492564559999966 \t 0.16666666666666666\n",
            "i 0.2492564569999966 \t 0.16666666666666666\n",
            "i 0.2492564579999966 \t 0.16666666666666666\n",
            "i 0.2492564589999966 \t 0.16666666666666666\n",
            "i 0.2492564599999966 \t 0.16666666666666666\n",
            "i 0.2492564609999966 \t 0.16666666666666666\n",
            "i 0.2492564619999966 \t 0.16666666666666666\n",
            "i 0.2492564629999966 \t 0.16666666666666666\n",
            "i 0.2492564639999966 \t 0.16666666666666666\n",
            "i 0.2492564649999966 \t 0.16666666666666666\n",
            "i 0.2492564659999966 \t 0.16666666666666666\n",
            "i 0.2492564669999966 \t 0.16666666666666666\n",
            "i 0.2492564679999966 \t 0.16666666666666666\n",
            "i 0.2492564689999966 \t 0.16666666666666666\n",
            "i 0.2492564699999966 \t 0.16666666666666666\n",
            "i 0.2492564709999966 \t 0.16666666666666666\n",
            "i 0.2492564719999966 \t 0.16666666666666666\n",
            "i 0.2492564729999966 \t 0.16666666666666666\n",
            "i 0.2492564739999966 \t 0.16666666666666666\n",
            "i 0.2492564749999966 \t 0.16666666666666666\n",
            "i 0.2492564759999966 \t 0.16666666666666666\n",
            "i 0.2492564769999966 \t 0.16666666666666666\n",
            "i 0.2492564779999966 \t 0.16666666666666666\n",
            "i 0.2492564789999966 \t 0.16666666666666666\n",
            "i 0.2492564799999966 \t 0.16666666666666666\n",
            "i 0.2492564809999966 \t 0.16666666666666666\n",
            "i 0.2492564819999966 \t 0.16666666666666666\n",
            "i 0.2492564829999966 \t 0.16666666666666666\n",
            "i 0.2492564839999966 \t 0.16666666666666666\n",
            "i 0.24925648499999659 \t 0.16666666666666666\n",
            "i 0.24925648599999659 \t 0.16666666666666666\n",
            "i 0.24925648699999658 \t 0.16666666666666666\n",
            "i 0.24925648799999658 \t 0.16666666666666666\n",
            "i 0.24925648899999658 \t 0.16666666666666666\n",
            "i 0.24925648999999658 \t 0.16666666666666666\n",
            "i 0.24925649099999658 \t 0.16666666666666666\n",
            "i 0.24925649199999658 \t 0.16666666666666666\n",
            "i 0.24925649299999658 \t 0.16666666666666666\n",
            "i 0.24925649399999658 \t 0.16666666666666666\n",
            "i 0.24925649499999658 \t 0.16666666666666666\n",
            "i 0.24925649599999658 \t 0.16666666666666666\n",
            "i 0.24925649699999658 \t 0.16666666666666666\n",
            "i 0.24925649799999658 \t 0.16666666666666666\n",
            "i 0.24925649899999658 \t 0.16666666666666666\n",
            "i 0.24925649999999658 \t 0.16666666666666666\n",
            "i 0.24925650099999658 \t 0.16666666666666666\n",
            "i 0.24925650199999658 \t 0.16666666666666666\n",
            "i 0.24925650299999658 \t 0.16666666666666666\n",
            "i 0.24925650399999658 \t 0.16666666666666666\n",
            "i 0.24925650499999658 \t 0.16666666666666666\n",
            "i 0.24925650599999657 \t 0.16666666666666666\n",
            "i 0.24925650699999657 \t 0.16666666666666666\n",
            "i 0.24925650799999657 \t 0.16666666666666666\n",
            "i 0.24925650899999657 \t 0.16666666666666666\n",
            "i 0.24925650999999657 \t 0.16666666666666666\n",
            "i 0.24925651099999657 \t 0.16666666666666666\n",
            "i 0.24925651199999657 \t 0.16666666666666666\n",
            "i 0.24925651299999657 \t 0.16666666666666666\n",
            "i 0.24925651399999657 \t 0.16666666666666666\n",
            "i 0.24925651499999657 \t 0.16666666666666666\n",
            "i 0.24925651599999657 \t 0.16666666666666666\n",
            "i 0.24925651699999657 \t 0.16666666666666666\n",
            "i 0.24925651799999657 \t 0.16666666666666666\n",
            "i 0.24925651899999657 \t 0.16666666666666666\n",
            "i 0.24925651999999657 \t 0.16666666666666666\n",
            "i 0.24925652099999657 \t 0.16666666666666666\n",
            "i 0.24925652199999657 \t 0.16666666666666666\n",
            "i 0.24925652299999657 \t 0.16666666666666666\n",
            "i 0.24925652399999657 \t 0.16666666666666666\n",
            "i 0.24925652499999656 \t 0.16666666666666666\n",
            "i 0.24925652599999656 \t 0.16666666666666666\n",
            "i 0.24925652699999656 \t 0.16666666666666666\n",
            "i 0.24925652799999656 \t 0.16666666666666666\n",
            "i 0.24925652899999656 \t 0.16666666666666666\n",
            "i 0.24925652999999656 \t 0.16666666666666666\n",
            "i 0.24925653099999656 \t 0.16666666666666666\n",
            "i 0.24925653199999656 \t 0.16666666666666666\n",
            "i 0.24925653299999656 \t 0.16666666666666666\n",
            "i 0.24925653399999656 \t 0.16666666666666666\n",
            "i 0.24925653499999656 \t 0.16666666666666666\n",
            "i 0.24925653599999656 \t 0.16666666666666666\n",
            "i 0.24925653699999656 \t 0.16666666666666666\n",
            "i 0.24925653799999656 \t 0.16666666666666666\n",
            "i 0.24925653899999656 \t 0.16666666666666666\n",
            "i 0.24925653999999656 \t 0.16666666666666666\n",
            "i 0.24925654099999656 \t 0.16666666666666666\n",
            "i 0.24925654199999656 \t 0.16666666666666666\n",
            "i 0.24925654299999656 \t 0.16666666666666666\n",
            "i 0.24925654399999655 \t 0.16666666666666666\n",
            "i 0.24925654499999655 \t 0.16666666666666666\n",
            "i 0.24925654599999655 \t 0.16666666666666666\n",
            "i 0.24925654699999655 \t 0.16666666666666666\n",
            "i 0.24925654799999655 \t 0.16666666666666666\n",
            "i 0.24925654899999655 \t 0.16666666666666666\n",
            "i 0.24925654999999655 \t 0.16666666666666666\n",
            "i 0.24925655099999655 \t 0.16666666666666666\n",
            "i 0.24925655199999655 \t 0.16666666666666666\n",
            "i 0.24925655299999655 \t 0.16666666666666666\n",
            "i 0.24925655399999655 \t 0.16666666666666666\n",
            "i 0.24925655499999655 \t 0.16666666666666666\n",
            "i 0.24925655599999655 \t 0.16666666666666666\n",
            "i 0.24925655699999655 \t 0.16666666666666666\n",
            "i 0.24925655799999655 \t 0.16666666666666666\n",
            "i 0.24925655899999655 \t 0.16666666666666666\n",
            "i 0.24925655999999655 \t 0.16666666666666666\n",
            "i 0.24925656099999655 \t 0.16666666666666666\n",
            "i 0.24925656199999655 \t 0.16666666666666666\n",
            "i 0.24925656299999654 \t 0.16666666666666666\n",
            "i 0.24925656399999654 \t 0.16666666666666666\n",
            "i 0.24925656499999654 \t 0.16666666666666666\n",
            "i 0.24925656599999654 \t 0.16666666666666666\n",
            "i 0.24925656699999654 \t 0.16666666666666666\n",
            "i 0.24925656799999654 \t 0.16666666666666666\n",
            "i 0.24925656899999654 \t 0.16666666666666666\n",
            "i 0.24925656999999654 \t 0.16666666666666666\n",
            "i 0.24925657099999654 \t 0.16666666666666666\n",
            "i 0.24925657199999654 \t 0.16666666666666666\n",
            "i 0.24925657299999654 \t 0.16666666666666666\n",
            "i 0.24925657399999654 \t 0.16666666666666666\n",
            "i 0.24925657499999654 \t 0.16666666666666666\n",
            "i 0.24925657599999654 \t 0.16666666666666666\n",
            "i 0.24925657699999654 \t 0.16666666666666666\n",
            "i 0.24925657799999654 \t 0.16666666666666666\n",
            "i 0.24925657899999654 \t 0.16666666666666666\n",
            "i 0.24925657999999654 \t 0.16666666666666666\n",
            "i 0.24925658099999654 \t 0.16666666666666666\n",
            "i 0.24925658199999653 \t 0.16666666666666666\n",
            "i 0.24925658299999653 \t 0.16666666666666666\n",
            "i 0.24925658399999653 \t 0.16666666666666666\n",
            "i 0.24925658499999653 \t 0.16666666666666666\n",
            "i 0.24925658599999653 \t 0.16666666666666666\n",
            "i 0.24925658699999653 \t 0.16666666666666666\n",
            "i 0.24925658799999653 \t 0.16666666666666666\n",
            "i 0.24925658899999653 \t 0.16666666666666666\n",
            "i 0.24925658999999653 \t 0.16666666666666666\n",
            "i 0.24925659099999653 \t 0.16666666666666666\n",
            "i 0.24925659199999653 \t 0.16666666666666666\n",
            "i 0.24925659299999653 \t 0.16666666666666666\n",
            "i 0.24925659399999653 \t 0.16666666666666666\n",
            "i 0.24925659499999653 \t 0.16666666666666666\n",
            "i 0.24925659599999653 \t 0.16666666666666666\n",
            "i 0.24925659699999653 \t 0.16666666666666666\n",
            "i 0.24925659799999653 \t 0.16666666666666666\n",
            "i 0.24925659899999653 \t 0.16666666666666666\n",
            "i 0.24925659999999653 \t 0.16666666666666666\n",
            "i 0.24925660099999652 \t 0.16666666666666666\n",
            "i 0.24925660199999652 \t 0.16666666666666666\n",
            "i 0.24925660299999652 \t 0.16666666666666666\n",
            "i 0.24925660399999652 \t 0.16666666666666666\n",
            "i 0.24925660499999652 \t 0.16666666666666666\n",
            "i 0.24925660599999652 \t 0.16666666666666666\n",
            "i 0.24925660699999652 \t 0.16666666666666666\n",
            "i 0.24925660799999652 \t 0.16666666666666666\n",
            "i 0.24925660899999652 \t 0.16666666666666666\n",
            "i 0.24925660999999652 \t 0.16666666666666666\n",
            "i 0.24925661099999652 \t 0.16666666666666666\n",
            "i 0.24925661199999652 \t 0.16666666666666666\n",
            "i 0.24925661299999652 \t 0.16666666666666666\n",
            "i 0.24925661399999652 \t 0.16666666666666666\n",
            "i 0.24925661499999652 \t 0.16666666666666666\n",
            "i 0.24925661599999652 \t 0.16666666666666666\n",
            "i 0.24925661699999652 \t 0.16666666666666666\n",
            "i 0.24925661799999652 \t 0.16666666666666666\n",
            "i 0.24925661899999652 \t 0.16666666666666666\n",
            "i 0.24925661999999651 \t 0.16666666666666666\n",
            "i 0.24925662099999651 \t 0.16666666666666666\n",
            "i 0.2492566219999965 \t 0.16666666666666666\n",
            "i 0.2492566229999965 \t 0.16666666666666666\n",
            "i 0.2492566239999965 \t 0.16666666666666666\n",
            "i 0.2492566249999965 \t 0.16666666666666666\n",
            "i 0.2492566259999965 \t 0.16666666666666666\n",
            "i 0.2492566269999965 \t 0.16666666666666666\n",
            "i 0.2492566279999965 \t 0.16666666666666666\n",
            "i 0.2492566289999965 \t 0.16666666666666666\n",
            "i 0.2492566299999965 \t 0.16666666666666666\n",
            "i 0.2492566309999965 \t 0.16666666666666666\n",
            "i 0.2492566319999965 \t 0.16666666666666666\n",
            "i 0.2492566329999965 \t 0.16666666666666666\n",
            "i 0.2492566339999965 \t 0.16666666666666666\n",
            "i 0.2492566349999965 \t 0.16666666666666666\n",
            "i 0.2492566359999965 \t 0.16666666666666666\n",
            "i 0.2492566369999965 \t 0.16666666666666666\n",
            "i 0.2492566379999965 \t 0.16666666666666666\n",
            "i 0.2492566389999965 \t 0.16666666666666666\n",
            "i 0.2492566399999965 \t 0.16666666666666666\n",
            "i 0.2492566409999965 \t 0.16666666666666666\n",
            "i 0.2492566419999965 \t 0.16666666666666666\n",
            "i 0.2492566429999965 \t 0.16666666666666666\n",
            "i 0.2492566439999965 \t 0.16666666666666666\n",
            "i 0.2492566449999965 \t 0.16666666666666666\n",
            "i 0.2492566459999965 \t 0.16666666666666666\n",
            "i 0.2492566469999965 \t 0.16666666666666666\n",
            "i 0.2492566479999965 \t 0.16666666666666666\n",
            "i 0.2492566489999965 \t 0.16666666666666666\n",
            "i 0.2492566499999965 \t 0.16666666666666666\n",
            "i 0.2492566509999965 \t 0.16666666666666666\n",
            "i 0.2492566519999965 \t 0.16666666666666666\n",
            "i 0.2492566529999965 \t 0.16666666666666666\n",
            "i 0.2492566539999965 \t 0.16666666666666666\n",
            "i 0.2492566549999965 \t 0.16666666666666666\n",
            "i 0.2492566559999965 \t 0.16666666666666666\n",
            "i 0.2492566569999965 \t 0.16666666666666666\n",
            "i 0.2492566579999965 \t 0.16666666666666666\n",
            "i 0.2492566589999965 \t 0.16666666666666666\n",
            "i 0.2492566599999965 \t 0.16666666666666666\n",
            "i 0.2492566609999965 \t 0.16666666666666666\n",
            "i 0.2492566619999965 \t 0.16666666666666666\n",
            "i 0.2492566629999965 \t 0.16666666666666666\n",
            "i 0.2492566639999965 \t 0.16666666666666666\n",
            "i 0.2492566649999965 \t 0.16666666666666666\n",
            "i 0.2492566659999965 \t 0.16666666666666666\n",
            "i 0.2492566669999965 \t 0.16666666666666666\n",
            "i 0.2492566679999965 \t 0.16666666666666666\n",
            "i 0.2492566689999965 \t 0.16666666666666666\n",
            "i 0.2492566699999965 \t 0.16666666666666666\n",
            "i 0.2492566709999965 \t 0.16666666666666666\n",
            "i 0.2492566719999965 \t 0.16666666666666666\n",
            "i 0.2492566729999965 \t 0.16666666666666666\n",
            "i 0.2492566739999965 \t 0.16666666666666666\n",
            "i 0.24925667499999649 \t 0.16666666666666666\n",
            "i 0.24925667599999649 \t 0.16666666666666666\n",
            "i 0.24925667699999648 \t 0.16666666666666666\n",
            "i 0.24925667799999648 \t 0.16666666666666666\n",
            "i 0.24925667899999648 \t 0.16666666666666666\n",
            "i 0.24925667999999648 \t 0.16666666666666666\n",
            "i 0.24925668099999648 \t 0.16666666666666666\n",
            "i 0.24925668199999648 \t 0.16666666666666666\n",
            "i 0.24925668299999648 \t 0.16666666666666666\n",
            "i 0.24925668399999648 \t 0.16666666666666666\n",
            "i 0.24925668499999648 \t 0.16666666666666666\n",
            "i 0.24925668599999648 \t 0.16666666666666666\n",
            "i 0.24925668699999648 \t 0.16666666666666666\n",
            "i 0.24925668799999648 \t 0.16666666666666666\n",
            "i 0.24925668899999648 \t 0.16666666666666666\n",
            "i 0.24925668999999648 \t 0.16666666666666666\n",
            "i 0.24925669099999648 \t 0.16666666666666666\n",
            "i 0.24925669199999648 \t 0.16666666666666666\n",
            "i 0.24925669299999648 \t 0.16666666666666666\n",
            "i 0.24925669399999648 \t 0.16666666666666666\n",
            "i 0.24925669499999648 \t 0.16666666666666666\n",
            "i 0.24925669599999647 \t 0.16666666666666666\n",
            "i 0.24925669699999647 \t 0.16666666666666666\n",
            "i 0.24925669799999647 \t 0.16666666666666666\n",
            "i 0.24925669899999647 \t 0.16666666666666666\n",
            "i 0.24925669999999647 \t 0.16666666666666666\n",
            "i 0.24925670099999647 \t 0.16666666666666666\n",
            "i 0.24925670199999647 \t 0.16666666666666666\n",
            "i 0.24925670299999647 \t 0.16666666666666666\n",
            "i 0.24925670399999647 \t 0.16666666666666666\n",
            "i 0.24925670499999647 \t 0.16666666666666666\n",
            "i 0.24925670599999647 \t 0.16666666666666666\n",
            "i 0.24925670699999647 \t 0.16666666666666666\n",
            "i 0.24925670799999647 \t 0.16666666666666666\n",
            "i 0.24925670899999647 \t 0.16666666666666666\n",
            "i 0.24925670999999647 \t 0.16666666666666666\n",
            "i 0.24925671099999647 \t 0.16666666666666666\n",
            "i 0.24925671199999647 \t 0.16666666666666666\n",
            "i 0.24925671299999647 \t 0.16666666666666666\n",
            "i 0.24925671399999647 \t 0.16666666666666666\n",
            "i 0.24925671499999646 \t 0.16666666666666666\n",
            "i 0.24925671599999646 \t 0.16666666666666666\n",
            "i 0.24925671699999646 \t 0.16666666666666666\n",
            "i 0.24925671799999646 \t 0.16666666666666666\n",
            "i 0.24925671899999646 \t 0.16666666666666666\n",
            "i 0.24925671999999646 \t 0.16666666666666666\n",
            "i 0.24925672099999646 \t 0.16666666666666666\n",
            "i 0.24925672199999646 \t 0.16666666666666666\n",
            "i 0.24925672299999646 \t 0.16666666666666666\n",
            "i 0.24925672399999646 \t 0.16666666666666666\n",
            "i 0.24925672499999646 \t 0.16666666666666666\n",
            "i 0.24925672599999646 \t 0.16666666666666666\n",
            "i 0.24925672699999646 \t 0.16666666666666666\n",
            "i 0.24925672799999646 \t 0.16666666666666666\n",
            "i 0.24925672899999646 \t 0.16666666666666666\n",
            "i 0.24925672999999646 \t 0.16666666666666666\n",
            "i 0.24925673099999646 \t 0.16666666666666666\n",
            "i 0.24925673199999646 \t 0.16666666666666666\n",
            "i 0.24925673299999646 \t 0.16666666666666666\n",
            "i 0.24925673399999645 \t 0.16666666666666666\n",
            "i 0.24925673499999645 \t 0.16666666666666666\n",
            "i 0.24925673599999645 \t 0.16666666666666666\n",
            "i 0.24925673699999645 \t 0.16666666666666666\n",
            "i 0.24925673799999645 \t 0.16666666666666666\n",
            "i 0.24925673899999645 \t 0.16666666666666666\n",
            "i 0.24925673999999645 \t 0.16666666666666666\n",
            "i 0.24925674099999645 \t 0.16666666666666666\n",
            "i 0.24925674199999645 \t 0.16666666666666666\n",
            "i 0.24925674299999645 \t 0.16666666666666666\n",
            "i 0.24925674399999645 \t 0.16666666666666666\n",
            "i 0.24925674499999645 \t 0.16666666666666666\n",
            "i 0.24925674599999645 \t 0.16666666666666666\n",
            "i 0.24925674699999645 \t 0.16666666666666666\n",
            "i 0.24925674799999645 \t 0.16666666666666666\n",
            "i 0.24925674899999645 \t 0.16666666666666666\n",
            "i 0.24925674999999645 \t 0.16666666666666666\n",
            "i 0.24925675099999645 \t 0.16666666666666666\n",
            "i 0.24925675199999645 \t 0.16666666666666666\n",
            "i 0.24925675299999644 \t 0.16666666666666666\n",
            "i 0.24925675399999644 \t 0.16666666666666666\n",
            "i 0.24925675499999644 \t 0.16666666666666666\n",
            "i 0.24925675599999644 \t 0.16666666666666666\n",
            "i 0.24925675699999644 \t 0.16666666666666666\n",
            "i 0.24925675799999644 \t 0.16666666666666666\n",
            "i 0.24925675899999644 \t 0.16666666666666666\n",
            "i 0.24925675999999644 \t 0.16666666666666666\n",
            "i 0.24925676099999644 \t 0.16666666666666666\n",
            "i 0.24925676199999644 \t 0.16666666666666666\n",
            "i 0.24925676299999644 \t 0.16666666666666666\n",
            "i 0.24925676399999644 \t 0.16666666666666666\n",
            "i 0.24925676499999644 \t 0.16666666666666666\n",
            "i 0.24925676599999644 \t 0.16666666666666666\n",
            "i 0.24925676699999644 \t 0.16666666666666666\n",
            "i 0.24925676799999644 \t 0.16666666666666666\n",
            "i 0.24925676899999644 \t 0.16666666666666666\n",
            "i 0.24925676999999644 \t 0.16666666666666666\n",
            "i 0.24925677099999644 \t 0.16666666666666666\n",
            "i 0.24925677199999643 \t 0.16666666666666666\n",
            "i 0.24925677299999643 \t 0.16666666666666666\n",
            "i 0.24925677399999643 \t 0.16666666666666666\n",
            "i 0.24925677499999643 \t 0.16666666666666666\n",
            "i 0.24925677599999643 \t 0.16666666666666666\n",
            "i 0.24925677699999643 \t 0.16666666666666666\n",
            "i 0.24925677799999643 \t 0.16666666666666666\n",
            "i 0.24925677899999643 \t 0.16666666666666666\n",
            "i 0.24925677999999643 \t 0.16666666666666666\n",
            "i 0.24925678099999643 \t 0.16666666666666666\n",
            "i 0.24925678199999643 \t 0.16666666666666666\n",
            "i 0.24925678299999643 \t 0.16666666666666666\n",
            "i 0.24925678399999643 \t 0.16666666666666666\n",
            "i 0.24925678499999643 \t 0.16666666666666666\n",
            "i 0.24925678599999643 \t 0.16666666666666666\n",
            "i 0.24925678699999643 \t 0.16666666666666666\n",
            "i 0.24925678799999643 \t 0.16666666666666666\n",
            "i 0.24925678899999643 \t 0.16666666666666666\n",
            "i 0.24925678999999643 \t 0.16666666666666666\n",
            "i 0.24925679099999642 \t 0.16666666666666666\n",
            "i 0.24925679199999642 \t 0.16666666666666666\n",
            "i 0.24925679299999642 \t 0.16666666666666666\n",
            "i 0.24925679399999642 \t 0.16666666666666666\n",
            "i 0.24925679499999642 \t 0.16666666666666666\n",
            "i 0.24925679599999642 \t 0.16666666666666666\n",
            "i 0.24925679699999642 \t 0.16666666666666666\n",
            "i 0.24925679799999642 \t 0.16666666666666666\n",
            "i 0.24925679899999642 \t 0.16666666666666666\n",
            "i 0.24925679999999642 \t 0.16666666666666666\n",
            "i 0.24925680099999642 \t 0.16666666666666666\n",
            "i 0.24925680199999642 \t 0.16666666666666666\n",
            "i 0.24925680299999642 \t 0.16666666666666666\n",
            "i 0.24925680399999642 \t 0.16666666666666666\n",
            "i 0.24925680499999642 \t 0.16666666666666666\n",
            "i 0.24925680599999642 \t 0.16666666666666666\n",
            "i 0.24925680699999642 \t 0.16666666666666666\n",
            "i 0.24925680799999642 \t 0.16666666666666666\n",
            "i 0.24925680899999642 \t 0.16666666666666666\n",
            "i 0.24925680999999641 \t 0.16666666666666666\n",
            "i 0.24925681099999641 \t 0.16666666666666666\n",
            "i 0.2492568119999964 \t 0.16666666666666666\n",
            "i 0.2492568129999964 \t 0.16666666666666666\n",
            "i 0.2492568139999964 \t 0.16666666666666666\n",
            "i 0.2492568149999964 \t 0.16666666666666666\n",
            "i 0.2492568159999964 \t 0.16666666666666666\n",
            "i 0.2492568169999964 \t 0.16666666666666666\n",
            "i 0.2492568179999964 \t 0.16666666666666666\n",
            "i 0.2492568189999964 \t 0.16666666666666666\n",
            "i 0.2492568199999964 \t 0.16666666666666666\n",
            "i 0.2492568209999964 \t 0.16666666666666666\n",
            "i 0.2492568219999964 \t 0.16666666666666666\n",
            "i 0.2492568229999964 \t 0.16666666666666666\n",
            "i 0.2492568239999964 \t 0.16666666666666666\n",
            "i 0.2492568249999964 \t 0.16666666666666666\n",
            "i 0.2492568259999964 \t 0.16666666666666666\n",
            "i 0.2492568269999964 \t 0.16666666666666666\n",
            "i 0.2492568279999964 \t 0.16666666666666666\n",
            "i 0.2492568289999964 \t 0.16666666666666666\n",
            "i 0.2492568299999964 \t 0.16666666666666666\n",
            "i 0.2492568309999964 \t 0.16666666666666666\n",
            "i 0.2492568319999964 \t 0.16666666666666666\n",
            "i 0.2492568329999964 \t 0.16666666666666666\n",
            "i 0.2492568339999964 \t 0.16666666666666666\n",
            "i 0.2492568349999964 \t 0.16666666666666666\n",
            "i 0.2492568359999964 \t 0.16666666666666666\n",
            "i 0.2492568369999964 \t 0.16666666666666666\n",
            "i 0.2492568379999964 \t 0.16666666666666666\n",
            "i 0.2492568389999964 \t 0.16666666666666666\n",
            "i 0.2492568399999964 \t 0.16666666666666666\n",
            "i 0.2492568409999964 \t 0.16666666666666666\n",
            "i 0.2492568419999964 \t 0.16666666666666666\n",
            "i 0.2492568429999964 \t 0.16666666666666666\n",
            "i 0.2492568439999964 \t 0.16666666666666666\n",
            "i 0.2492568449999964 \t 0.16666666666666666\n",
            "i 0.2492568459999964 \t 0.16666666666666666\n",
            "i 0.2492568469999964 \t 0.16666666666666666\n",
            "i 0.2492568479999964 \t 0.16666666666666666\n",
            "i 0.2492568489999964 \t 0.16666666666666666\n",
            "i 0.2492568499999964 \t 0.16666666666666666\n",
            "i 0.2492568509999964 \t 0.16666666666666666\n",
            "i 0.2492568519999964 \t 0.16666666666666666\n",
            "i 0.2492568529999964 \t 0.16666666666666666\n",
            "i 0.2492568539999964 \t 0.16666666666666666\n",
            "i 0.2492568549999964 \t 0.16666666666666666\n",
            "i 0.2492568559999964 \t 0.16666666666666666\n",
            "i 0.2492568569999964 \t 0.16666666666666666\n",
            "i 0.2492568579999964 \t 0.16666666666666666\n",
            "i 0.2492568589999964 \t 0.16666666666666666\n",
            "i 0.2492568599999964 \t 0.16666666666666666\n",
            "i 0.2492568609999964 \t 0.16666666666666666\n",
            "i 0.2492568619999964 \t 0.16666666666666666\n",
            "i 0.2492568629999964 \t 0.16666666666666666\n",
            "i 0.2492568639999964 \t 0.16666666666666666\n",
            "i 0.24925686499999639 \t 0.16666666666666666\n",
            "i 0.24925686599999639 \t 0.16666666666666666\n",
            "i 0.24925686699999638 \t 0.16666666666666666\n",
            "i 0.24925686799999638 \t 0.16666666666666666\n",
            "i 0.24925686899999638 \t 0.16666666666666666\n",
            "i 0.24925686999999638 \t 0.16666666666666666\n",
            "i 0.24925687099999638 \t 0.16666666666666666\n",
            "i 0.24925687199999638 \t 0.16666666666666666\n",
            "i 0.24925687299999638 \t 0.16666666666666666\n",
            "i 0.24925687399999638 \t 0.16666666666666666\n",
            "i 0.24925687499999638 \t 0.16666666666666666\n",
            "i 0.24925687599999638 \t 0.16666666666666666\n",
            "i 0.24925687699999638 \t 0.16666666666666666\n",
            "i 0.24925687799999638 \t 0.16666666666666666\n",
            "i 0.24925687899999638 \t 0.16666666666666666\n",
            "i 0.24925687999999638 \t 0.16666666666666666\n",
            "i 0.24925688099999638 \t 0.16666666666666666\n",
            "i 0.24925688199999638 \t 0.16666666666666666\n",
            "i 0.24925688299999638 \t 0.16666666666666666\n",
            "i 0.24925688399999638 \t 0.16666666666666666\n",
            "i 0.24925688499999638 \t 0.16666666666666666\n",
            "i 0.24925688599999637 \t 0.16666666666666666\n",
            "i 0.24925688699999637 \t 0.16666666666666666\n",
            "i 0.24925688799999637 \t 0.16666666666666666\n",
            "i 0.24925688899999637 \t 0.16666666666666666\n",
            "i 0.24925688999999637 \t 0.16666666666666666\n",
            "i 0.24925689099999637 \t 0.16666666666666666\n",
            "i 0.24925689199999637 \t 0.16666666666666666\n",
            "i 0.24925689299999637 \t 0.16666666666666666\n",
            "i 0.24925689399999637 \t 0.16666666666666666\n",
            "i 0.24925689499999637 \t 0.16666666666666666\n",
            "i 0.24925689599999637 \t 0.16666666666666666\n",
            "i 0.24925689699999637 \t 0.16666666666666666\n",
            "i 0.24925689799999637 \t 0.16666666666666666\n",
            "i 0.24925689899999637 \t 0.16666666666666666\n",
            "i 0.24925689999999637 \t 0.16666666666666666\n",
            "i 0.24925690099999637 \t 0.16666666666666666\n",
            "i 0.24925690199999637 \t 0.16666666666666666\n",
            "i 0.24925690299999637 \t 0.16666666666666666\n",
            "i 0.24925690399999637 \t 0.16666666666666666\n",
            "i 0.24925690499999636 \t 0.16666666666666666\n",
            "i 0.24925690599999636 \t 0.16666666666666666\n",
            "i 0.24925690699999636 \t 0.16666666666666666\n",
            "i 0.24925690799999636 \t 0.16666666666666666\n",
            "i 0.24925690899999636 \t 0.16666666666666666\n",
            "i 0.24925690999999636 \t 0.16666666666666666\n",
            "i 0.24925691099999636 \t 0.16666666666666666\n",
            "i 0.24925691199999636 \t 0.16666666666666666\n",
            "i 0.24925691299999636 \t 0.16666666666666666\n",
            "i 0.24925691399999636 \t 0.16666666666666666\n",
            "i 0.24925691499999636 \t 0.16666666666666666\n",
            "i 0.24925691599999636 \t 0.16666666666666666\n",
            "i 0.24925691699999636 \t 0.16666666666666666\n",
            "i 0.24925691799999636 \t 0.16666666666666666\n",
            "i 0.24925691899999636 \t 0.16666666666666666\n",
            "i 0.24925691999999636 \t 0.16666666666666666\n",
            "i 0.24925692099999636 \t 0.16666666666666666\n",
            "i 0.24925692199999636 \t 0.16666666666666666\n",
            "i 0.24925692299999636 \t 0.16666666666666666\n",
            "i 0.24925692399999635 \t 0.16666666666666666\n",
            "i 0.24925692499999635 \t 0.16666666666666666\n",
            "i 0.24925692599999635 \t 0.16666666666666666\n",
            "i 0.24925692699999635 \t 0.16666666666666666\n",
            "i 0.24925692799999635 \t 0.16666666666666666\n",
            "i 0.24925692899999635 \t 0.16666666666666666\n",
            "i 0.24925692999999635 \t 0.16666666666666666\n",
            "i 0.24925693099999635 \t 0.16666666666666666\n",
            "i 0.24925693199999635 \t 0.16666666666666666\n",
            "i 0.24925693299999635 \t 0.16666666666666666\n",
            "i 0.24925693399999635 \t 0.16666666666666666\n",
            "i 0.24925693499999635 \t 0.16666666666666666\n",
            "i 0.24925693599999635 \t 0.16666666666666666\n",
            "i 0.24925693699999635 \t 0.16666666666666666\n",
            "i 0.24925693799999635 \t 0.16666666666666666\n",
            "i 0.24925693899999635 \t 0.16666666666666666\n",
            "i 0.24925693999999635 \t 0.16666666666666666\n",
            "i 0.24925694099999635 \t 0.16666666666666666\n",
            "i 0.24925694199999635 \t 0.16666666666666666\n",
            "i 0.24925694299999634 \t 0.16666666666666666\n",
            "i 0.24925694399999634 \t 0.16666666666666666\n",
            "i 0.24925694499999634 \t 0.16666666666666666\n",
            "i 0.24925694599999634 \t 0.16666666666666666\n",
            "i 0.24925694699999634 \t 0.16666666666666666\n",
            "i 0.24925694799999634 \t 0.16666666666666666\n",
            "i 0.24925694899999634 \t 0.16666666666666666\n",
            "i 0.24925694999999634 \t 0.16666666666666666\n",
            "i 0.24925695099999634 \t 0.16666666666666666\n",
            "i 0.24925695199999634 \t 0.16666666666666666\n",
            "i 0.24925695299999634 \t 0.16666666666666666\n",
            "i 0.24925695399999634 \t 0.16666666666666666\n",
            "i 0.24925695499999634 \t 0.16666666666666666\n",
            "i 0.24925695599999634 \t 0.16666666666666666\n",
            "i 0.24925695699999634 \t 0.16666666666666666\n",
            "i 0.24925695799999634 \t 0.16666666666666666\n",
            "i 0.24925695899999634 \t 0.16666666666666666\n",
            "i 0.24925695999999634 \t 0.16666666666666666\n",
            "i 0.24925696099999634 \t 0.16666666666666666\n",
            "i 0.24925696199999633 \t 0.16666666666666666\n",
            "i 0.24925696299999633 \t 0.16666666666666666\n",
            "i 0.24925696399999633 \t 0.16666666666666666\n",
            "i 0.24925696499999633 \t 0.16666666666666666\n",
            "i 0.24925696599999633 \t 0.16666666666666666\n",
            "i 0.24925696699999633 \t 0.16666666666666666\n",
            "i 0.24925696799999633 \t 0.16666666666666666\n",
            "i 0.24925696899999633 \t 0.16666666666666666\n",
            "i 0.24925696999999633 \t 0.16666666666666666\n",
            "i 0.24925697099999633 \t 0.16666666666666666\n",
            "i 0.24925697199999633 \t 0.16666666666666666\n",
            "i 0.24925697299999633 \t 0.16666666666666666\n",
            "i 0.24925697399999633 \t 0.16666666666666666\n",
            "i 0.24925697499999633 \t 0.16666666666666666\n",
            "i 0.24925697599999633 \t 0.16666666666666666\n",
            "i 0.24925697699999633 \t 0.16666666666666666\n",
            "i 0.24925697799999633 \t 0.16666666666666666\n",
            "i 0.24925697899999633 \t 0.16666666666666666\n",
            "i 0.24925697999999633 \t 0.16666666666666666\n",
            "i 0.24925698099999632 \t 0.16666666666666666\n",
            "i 0.24925698199999632 \t 0.16666666666666666\n",
            "i 0.24925698299999632 \t 0.16666666666666666\n",
            "i 0.24925698399999632 \t 0.16666666666666666\n",
            "i 0.24925698499999632 \t 0.16666666666666666\n",
            "i 0.24925698599999632 \t 0.16666666666666666\n",
            "i 0.24925698699999632 \t 0.16666666666666666\n",
            "i 0.24925698799999632 \t 0.16666666666666666\n",
            "i 0.24925698899999632 \t 0.16666666666666666\n",
            "i 0.24925698999999632 \t 0.16666666666666666\n",
            "i 0.24925699099999632 \t 0.16666666666666666\n",
            "i 0.24925699199999632 \t 0.16666666666666666\n",
            "i 0.24925699299999632 \t 0.16666666666666666\n",
            "i 0.24925699399999632 \t 0.16666666666666666\n",
            "i 0.24925699499999632 \t 0.16666666666666666\n",
            "i 0.24925699599999632 \t 0.16666666666666666\n",
            "i 0.24925699699999632 \t 0.16666666666666666\n",
            "i 0.24925699799999632 \t 0.16666666666666666\n",
            "i 0.24925699899999632 \t 0.16666666666666666\n",
            "i 0.24925699999999631 \t 0.16666666666666666\n",
            "i 0.24925700099999631 \t 0.16666666666666666\n",
            "i 0.2492570019999963 \t 0.16666666666666666\n",
            "i 0.2492570029999963 \t 0.16666666666666666\n",
            "i 0.2492570039999963 \t 0.16666666666666666\n",
            "i 0.2492570049999963 \t 0.16666666666666666\n",
            "i 0.2492570059999963 \t 0.16666666666666666\n",
            "i 0.2492570069999963 \t 0.16666666666666666\n",
            "i 0.2492570079999963 \t 0.16666666666666666\n",
            "i 0.2492570089999963 \t 0.16666666666666666\n",
            "i 0.2492570099999963 \t 0.16666666666666666\n",
            "i 0.2492570109999963 \t 0.16666666666666666\n",
            "i 0.2492570119999963 \t 0.16666666666666666\n",
            "i 0.2492570129999963 \t 0.16666666666666666\n",
            "i 0.2492570139999963 \t 0.16666666666666666\n",
            "i 0.2492570149999963 \t 0.16666666666666666\n",
            "i 0.2492570159999963 \t 0.16666666666666666\n",
            "i 0.2492570169999963 \t 0.16666666666666666\n",
            "i 0.2492570179999963 \t 0.16666666666666666\n",
            "i 0.2492570189999963 \t 0.16666666666666666\n",
            "i 0.2492570199999963 \t 0.16666666666666666\n",
            "i 0.2492570209999963 \t 0.16666666666666666\n",
            "i 0.2492570219999963 \t 0.16666666666666666\n",
            "i 0.2492570229999963 \t 0.16666666666666666\n",
            "i 0.2492570239999963 \t 0.16666666666666666\n",
            "i 0.2492570249999963 \t 0.16666666666666666\n",
            "i 0.2492570259999963 \t 0.16666666666666666\n",
            "i 0.2492570269999963 \t 0.16666666666666666\n",
            "i 0.2492570279999963 \t 0.16666666666666666\n",
            "i 0.2492570289999963 \t 0.16666666666666666\n",
            "i 0.2492570299999963 \t 0.16666666666666666\n",
            "i 0.2492570309999963 \t 0.16666666666666666\n",
            "i 0.2492570319999963 \t 0.16666666666666666\n",
            "i 0.2492570329999963 \t 0.16666666666666666\n",
            "i 0.2492570339999963 \t 0.16666666666666666\n",
            "i 0.2492570349999963 \t 0.16666666666666666\n",
            "i 0.2492570359999963 \t 0.16666666666666666\n",
            "i 0.2492570369999963 \t 0.16666666666666666\n",
            "i 0.2492570379999963 \t 0.16666666666666666\n",
            "i 0.2492570389999963 \t 0.16666666666666666\n",
            "i 0.2492570399999963 \t 0.16666666666666666\n",
            "i 0.2492570409999963 \t 0.16666666666666666\n",
            "i 0.2492570419999963 \t 0.16666666666666666\n",
            "i 0.2492570429999963 \t 0.16666666666666666\n",
            "i 0.2492570439999963 \t 0.16666666666666666\n",
            "i 0.2492570449999963 \t 0.16666666666666666\n",
            "i 0.2492570459999963 \t 0.16666666666666666\n",
            "i 0.2492570469999963 \t 0.16666666666666666\n",
            "i 0.2492570479999963 \t 0.16666666666666666\n",
            "i 0.2492570489999963 \t 0.16666666666666666\n",
            "i 0.2492570499999963 \t 0.16666666666666666\n",
            "i 0.2492570509999963 \t 0.16666666666666666\n",
            "i 0.2492570519999963 \t 0.16666666666666666\n",
            "i 0.2492570529999963 \t 0.16666666666666666\n",
            "i 0.2492570539999963 \t 0.16666666666666666\n",
            "i 0.24925705499999629 \t 0.16666666666666666\n",
            "i 0.24925705599999629 \t 0.16666666666666666\n",
            "i 0.24925705699999628 \t 0.16666666666666666\n",
            "i 0.24925705799999628 \t 0.16666666666666666\n",
            "i 0.24925705899999628 \t 0.16666666666666666\n",
            "i 0.24925705999999628 \t 0.16666666666666666\n",
            "i 0.24925706099999628 \t 0.16666666666666666\n",
            "i 0.24925706199999628 \t 0.16666666666666666\n",
            "i 0.24925706299999628 \t 0.16666666666666666\n",
            "i 0.24925706399999628 \t 0.16666666666666666\n",
            "i 0.24925706499999628 \t 0.16666666666666666\n",
            "i 0.24925706599999628 \t 0.16666666666666666\n",
            "i 0.24925706699999628 \t 0.16666666666666666\n",
            "i 0.24925706799999628 \t 0.16666666666666666\n",
            "i 0.24925706899999628 \t 0.16666666666666666\n",
            "i 0.24925706999999628 \t 0.16666666666666666\n",
            "i 0.24925707099999628 \t 0.16666666666666666\n",
            "i 0.24925707199999628 \t 0.16666666666666666\n",
            "i 0.24925707299999628 \t 0.16666666666666666\n",
            "i 0.24925707399999628 \t 0.16666666666666666\n",
            "i 0.24925707499999628 \t 0.16666666666666666\n",
            "i 0.24925707599999627 \t 0.16666666666666666\n",
            "i 0.24925707699999627 \t 0.16666666666666666\n",
            "i 0.24925707799999627 \t 0.16666666666666666\n",
            "i 0.24925707899999627 \t 0.16666666666666666\n",
            "i 0.24925707999999627 \t 0.16666666666666666\n",
            "i 0.24925708099999627 \t 0.16666666666666666\n",
            "i 0.24925708199999627 \t 0.16666666666666666\n",
            "i 0.24925708299999627 \t 0.16666666666666666\n",
            "i 0.24925708399999627 \t 0.16666666666666666\n",
            "i 0.24925708499999627 \t 0.16666666666666666\n",
            "i 0.24925708599999627 \t 0.16666666666666666\n",
            "i 0.24925708699999627 \t 0.16666666666666666\n",
            "i 0.24925708799999627 \t 0.16666666666666666\n",
            "i 0.24925708899999627 \t 0.16666666666666666\n",
            "i 0.24925708999999627 \t 0.16666666666666666\n",
            "i 0.24925709099999627 \t 0.16666666666666666\n",
            "i 0.24925709199999627 \t 0.16666666666666666\n",
            "i 0.24925709299999627 \t 0.16666666666666666\n",
            "i 0.24925709399999627 \t 0.16666666666666666\n",
            "i 0.24925709499999626 \t 0.16666666666666666\n",
            "i 0.24925709599999626 \t 0.16666666666666666\n",
            "i 0.24925709699999626 \t 0.16666666666666666\n",
            "i 0.24925709799999626 \t 0.16666666666666666\n",
            "i 0.24925709899999626 \t 0.16666666666666666\n",
            "i 0.24925709999999626 \t 0.16666666666666666\n",
            "i 0.24925710099999626 \t 0.16666666666666666\n",
            "i 0.24925710199999626 \t 0.16666666666666666\n",
            "i 0.24925710299999626 \t 0.16666666666666666\n",
            "i 0.24925710399999626 \t 0.16666666666666666\n",
            "i 0.24925710499999626 \t 0.16666666666666666\n",
            "i 0.24925710599999626 \t 0.16666666666666666\n",
            "i 0.24925710699999626 \t 0.16666666666666666\n",
            "i 0.24925710799999626 \t 0.16666666666666666\n",
            "i 0.24925710899999626 \t 0.16666666666666666\n",
            "i 0.24925710999999626 \t 0.16666666666666666\n",
            "i 0.24925711099999626 \t 0.16666666666666666\n",
            "i 0.24925711199999626 \t 0.16666666666666666\n",
            "i 0.24925711299999626 \t 0.16666666666666666\n",
            "i 0.24925711399999625 \t 0.16666666666666666\n",
            "i 0.24925711499999625 \t 0.16666666666666666\n",
            "i 0.24925711599999625 \t 0.16666666666666666\n",
            "i 0.24925711699999625 \t 0.16666666666666666\n",
            "i 0.24925711799999625 \t 0.16666666666666666\n",
            "i 0.24925711899999625 \t 0.16666666666666666\n",
            "i 0.24925711999999625 \t 0.16666666666666666\n",
            "i 0.24925712099999625 \t 0.16666666666666666\n",
            "i 0.24925712199999625 \t 0.16666666666666666\n",
            "i 0.24925712299999625 \t 0.16666666666666666\n",
            "i 0.24925712399999625 \t 0.16666666666666666\n",
            "i 0.24925712499999625 \t 0.16666666666666666\n",
            "i 0.24925712599999625 \t 0.16666666666666666\n",
            "i 0.24925712699999625 \t 0.16666666666666666\n",
            "i 0.24925712799999625 \t 0.16666666666666666\n",
            "i 0.24925712899999625 \t 0.16666666666666666\n",
            "i 0.24925712999999625 \t 0.16666666666666666\n",
            "i 0.24925713099999625 \t 0.16666666666666666\n",
            "i 0.24925713199999625 \t 0.16666666666666666\n",
            "i 0.24925713299999624 \t 0.16666666666666666\n",
            "i 0.24925713399999624 \t 0.16666666666666666\n",
            "i 0.24925713499999624 \t 0.16666666666666666\n",
            "i 0.24925713599999624 \t 0.16666666666666666\n",
            "i 0.24925713699999624 \t 0.16666666666666666\n",
            "i 0.24925713799999624 \t 0.16666666666666666\n",
            "i 0.24925713899999624 \t 0.16666666666666666\n",
            "i 0.24925713999999624 \t 0.16666666666666666\n",
            "i 0.24925714099999624 \t 0.16666666666666666\n",
            "i 0.24925714199999624 \t 0.16666666666666666\n",
            "i 0.24925714299999624 \t 0.16666666666666666\n",
            "i 0.24925714399999624 \t 0.16666666666666666\n",
            "i 0.24925714499999624 \t 0.16666666666666666\n",
            "i 0.24925714599999624 \t 0.16666666666666666\n",
            "i 0.24925714699999624 \t 0.16666666666666666\n",
            "i 0.24925714799999624 \t 0.16666666666666666\n",
            "i 0.24925714899999624 \t 0.16666666666666666\n",
            "i 0.24925714999999624 \t 0.16666666666666666\n",
            "i 0.24925715099999624 \t 0.16666666666666666\n",
            "i 0.24925715199999623 \t 0.16666666666666666\n",
            "i 0.24925715299999623 \t 0.16666666666666666\n",
            "i 0.24925715399999623 \t 0.16666666666666666\n",
            "i 0.24925715499999623 \t 0.16666666666666666\n",
            "i 0.24925715599999623 \t 0.16666666666666666\n",
            "i 0.24925715699999623 \t 0.16666666666666666\n",
            "i 0.24925715799999623 \t 0.16666666666666666\n",
            "i 0.24925715899999623 \t 0.16666666666666666\n",
            "i 0.24925715999999623 \t 0.16666666666666666\n",
            "i 0.24925716099999623 \t 0.16666666666666666\n",
            "i 0.24925716199999623 \t 0.16666666666666666\n",
            "i 0.24925716299999623 \t 0.16666666666666666\n",
            "i 0.24925716399999623 \t 0.16666666666666666\n",
            "i 0.24925716499999623 \t 0.16666666666666666\n",
            "i 0.24925716599999623 \t 0.16666666666666666\n",
            "i 0.24925716699999623 \t 0.16666666666666666\n",
            "i 0.24925716799999623 \t 0.16666666666666666\n",
            "i 0.24925716899999623 \t 0.16666666666666666\n",
            "i 0.24925716999999623 \t 0.16666666666666666\n",
            "i 0.24925717099999622 \t 0.16666666666666666\n",
            "i 0.24925717199999622 \t 0.16666666666666666\n",
            "i 0.24925717299999622 \t 0.16666666666666666\n",
            "i 0.24925717399999622 \t 0.16666666666666666\n",
            "i 0.24925717499999622 \t 0.16666666666666666\n",
            "i 0.24925717599999622 \t 0.16666666666666666\n",
            "i 0.24925717699999622 \t 0.16666666666666666\n",
            "i 0.24925717799999622 \t 0.16666666666666666\n",
            "i 0.24925717899999622 \t 0.16666666666666666\n",
            "i 0.24925717999999622 \t 0.16666666666666666\n",
            "i 0.24925718099999622 \t 0.16666666666666666\n",
            "i 0.24925718199999622 \t 0.16666666666666666\n",
            "i 0.24925718299999622 \t 0.16666666666666666\n",
            "i 0.24925718399999622 \t 0.16666666666666666\n",
            "i 0.24925718499999622 \t 0.16666666666666666\n",
            "i 0.24925718599999622 \t 0.16666666666666666\n",
            "i 0.24925718699999622 \t 0.16666666666666666\n",
            "i 0.24925718799999622 \t 0.16666666666666666\n",
            "i 0.24925718899999622 \t 0.16666666666666666\n",
            "i 0.24925718999999621 \t 0.16666666666666666\n",
            "i 0.24925719099999621 \t 0.16666666666666666\n",
            "i 0.2492571919999962 \t 0.16666666666666666\n",
            "i 0.2492571929999962 \t 0.16666666666666666\n",
            "i 0.2492571939999962 \t 0.16666666666666666\n",
            "i 0.2492571949999962 \t 0.16666666666666666\n",
            "i 0.2492571959999962 \t 0.16666666666666666\n",
            "i 0.2492571969999962 \t 0.16666666666666666\n",
            "i 0.2492571979999962 \t 0.16666666666666666\n",
            "i 0.2492571989999962 \t 0.16666666666666666\n",
            "i 0.2492571999999962 \t 0.16666666666666666\n",
            "i 0.2492572009999962 \t 0.16666666666666666\n",
            "i 0.2492572019999962 \t 0.16666666666666666\n",
            "i 0.2492572029999962 \t 0.16666666666666666\n",
            "i 0.2492572039999962 \t 0.16666666666666666\n",
            "i 0.2492572049999962 \t 0.16666666666666666\n",
            "i 0.2492572059999962 \t 0.16666666666666666\n",
            "i 0.2492572069999962 \t 0.16666666666666666\n",
            "i 0.2492572079999962 \t 0.16666666666666666\n",
            "i 0.2492572089999962 \t 0.16666666666666666\n",
            "i 0.2492572099999962 \t 0.16666666666666666\n",
            "i 0.2492572109999962 \t 0.16666666666666666\n",
            "i 0.2492572119999962 \t 0.16666666666666666\n",
            "i 0.2492572129999962 \t 0.16666666666666666\n",
            "i 0.2492572139999962 \t 0.16666666666666666\n",
            "i 0.2492572149999962 \t 0.16666666666666666\n",
            "i 0.2492572159999962 \t 0.16666666666666666\n",
            "i 0.2492572169999962 \t 0.16666666666666666\n",
            "i 0.2492572179999962 \t 0.16666666666666666\n",
            "i 0.2492572189999962 \t 0.16666666666666666\n",
            "i 0.2492572199999962 \t 0.16666666666666666\n",
            "i 0.2492572209999962 \t 0.16666666666666666\n",
            "i 0.2492572219999962 \t 0.16666666666666666\n",
            "i 0.2492572229999962 \t 0.16666666666666666\n",
            "i 0.2492572239999962 \t 0.16666666666666666\n",
            "i 0.2492572249999962 \t 0.16666666666666666\n",
            "i 0.2492572259999962 \t 0.16666666666666666\n",
            "i 0.2492572269999962 \t 0.16666666666666666\n",
            "i 0.2492572279999962 \t 0.16666666666666666\n",
            "i 0.2492572289999962 \t 0.16666666666666666\n",
            "i 0.2492572299999962 \t 0.16666666666666666\n",
            "i 0.2492572309999962 \t 0.16666666666666666\n",
            "i 0.2492572319999962 \t 0.16666666666666666\n",
            "i 0.2492572329999962 \t 0.16666666666666666\n",
            "i 0.2492572339999962 \t 0.16666666666666666\n",
            "i 0.2492572349999962 \t 0.16666666666666666\n",
            "i 0.2492572359999962 \t 0.16666666666666666\n",
            "i 0.2492572369999962 \t 0.16666666666666666\n",
            "i 0.2492572379999962 \t 0.16666666666666666\n",
            "i 0.2492572389999962 \t 0.16666666666666666\n",
            "i 0.2492572399999962 \t 0.16666666666666666\n",
            "i 0.2492572409999962 \t 0.16666666666666666\n",
            "i 0.2492572419999962 \t 0.16666666666666666\n",
            "i 0.2492572429999962 \t 0.16666666666666666\n",
            "i 0.2492572439999962 \t 0.16666666666666666\n",
            "i 0.24925724499999619 \t 0.16666666666666666\n",
            "i 0.24925724599999619 \t 0.16666666666666666\n",
            "i 0.24925724699999618 \t 0.16666666666666666\n",
            "i 0.24925724799999618 \t 0.16666666666666666\n",
            "i 0.24925724899999618 \t 0.16666666666666666\n",
            "i 0.24925724999999618 \t 0.16666666666666666\n",
            "i 0.24925725099999618 \t 0.16666666666666666\n",
            "i 0.24925725199999618 \t 0.16666666666666666\n",
            "i 0.24925725299999618 \t 0.16666666666666666\n",
            "i 0.24925725399999618 \t 0.16666666666666666\n",
            "i 0.24925725499999618 \t 0.16666666666666666\n",
            "i 0.24925725599999618 \t 0.16666666666666666\n",
            "i 0.24925725699999618 \t 0.16666666666666666\n",
            "i 0.24925725799999618 \t 0.16666666666666666\n",
            "i 0.24925725899999618 \t 0.16666666666666666\n",
            "i 0.24925725999999618 \t 0.16666666666666666\n",
            "i 0.24925726099999618 \t 0.16666666666666666\n",
            "i 0.24925726199999618 \t 0.16666666666666666\n",
            "i 0.24925726299999618 \t 0.16666666666666666\n",
            "i 0.24925726399999618 \t 0.16666666666666666\n",
            "i 0.24925726499999618 \t 0.16666666666666666\n",
            "i 0.24925726599999617 \t 0.16666666666666666\n",
            "i 0.24925726699999617 \t 0.16666666666666666\n",
            "i 0.24925726799999617 \t 0.16666666666666666\n",
            "i 0.24925726899999617 \t 0.16666666666666666\n",
            "i 0.24925726999999617 \t 0.16666666666666666\n",
            "i 0.24925727099999617 \t 0.16666666666666666\n",
            "i 0.24925727199999617 \t 0.16666666666666666\n",
            "i 0.24925727299999617 \t 0.16666666666666666\n",
            "i 0.24925727399999617 \t 0.16666666666666666\n",
            "i 0.24925727499999617 \t 0.16666666666666666\n",
            "i 0.24925727599999617 \t 0.16666666666666666\n",
            "i 0.24925727699999617 \t 0.16666666666666666\n",
            "i 0.24925727799999617 \t 0.16666666666666666\n",
            "i 0.24925727899999617 \t 0.16666666666666666\n",
            "i 0.24925727999999617 \t 0.16666666666666666\n",
            "i 0.24925728099999617 \t 0.16666666666666666\n",
            "i 0.24925728199999617 \t 0.16666666666666666\n",
            "i 0.24925728299999617 \t 0.16666666666666666\n",
            "i 0.24925728399999617 \t 0.16666666666666666\n",
            "i 0.24925728499999616 \t 0.16666666666666666\n",
            "i 0.24925728599999616 \t 0.16666666666666666\n",
            "i 0.24925728699999616 \t 0.16666666666666666\n",
            "i 0.24925728799999616 \t 0.16666666666666666\n",
            "i 0.24925728899999616 \t 0.16666666666666666\n",
            "i 0.24925728999999616 \t 0.16666666666666666\n",
            "i 0.24925729099999616 \t 0.16666666666666666\n",
            "i 0.24925729199999616 \t 0.16666666666666666\n",
            "i 0.24925729299999616 \t 0.16666666666666666\n",
            "i 0.24925729399999616 \t 0.16666666666666666\n",
            "i 0.24925729499999616 \t 0.16666666666666666\n",
            "i 0.24925729599999616 \t 0.16666666666666666\n",
            "i 0.24925729699999616 \t 0.16666666666666666\n",
            "i 0.24925729799999616 \t 0.16666666666666666\n",
            "i 0.24925729899999616 \t 0.16666666666666666\n",
            "i 0.24925729999999616 \t 0.16666666666666666\n",
            "i 0.24925730099999616 \t 0.16666666666666666\n",
            "i 0.24925730199999616 \t 0.16666666666666666\n",
            "i 0.24925730299999616 \t 0.16666666666666666\n",
            "i 0.24925730399999615 \t 0.16666666666666666\n",
            "i 0.24925730499999615 \t 0.16666666666666666\n",
            "i 0.24925730599999615 \t 0.16666666666666666\n",
            "i 0.24925730699999615 \t 0.16666666666666666\n",
            "i 0.24925730799999615 \t 0.16666666666666666\n",
            "i 0.24925730899999615 \t 0.16666666666666666\n",
            "i 0.24925730999999615 \t 0.16666666666666666\n",
            "i 0.24925731099999615 \t 0.16666666666666666\n",
            "i 0.24925731199999615 \t 0.16666666666666666\n",
            "i 0.24925731299999615 \t 0.16666666666666666\n",
            "i 0.24925731399999615 \t 0.16666666666666666\n",
            "i 0.24925731499999615 \t 0.16666666666666666\n",
            "i 0.24925731599999615 \t 0.16666666666666666\n",
            "i 0.24925731699999615 \t 0.16666666666666666\n",
            "i 0.24925731799999615 \t 0.16666666666666666\n",
            "i 0.24925731899999615 \t 0.16666666666666666\n",
            "i 0.24925731999999615 \t 0.16666666666666666\n",
            "i 0.24925732099999615 \t 0.16666666666666666\n",
            "i 0.24925732199999615 \t 0.16666666666666666\n",
            "i 0.24925732299999614 \t 0.16666666666666666\n",
            "i 0.24925732399999614 \t 0.16666666666666666\n",
            "i 0.24925732499999614 \t 0.16666666666666666\n",
            "i 0.24925732599999614 \t 0.16666666666666666\n",
            "i 0.24925732699999614 \t 0.16666666666666666\n",
            "i 0.24925732799999614 \t 0.16666666666666666\n",
            "i 0.24925732899999614 \t 0.16666666666666666\n",
            "i 0.24925732999999614 \t 0.16666666666666666\n",
            "i 0.24925733099999614 \t 0.16666666666666666\n",
            "i 0.24925733199999614 \t 0.16666666666666666\n",
            "i 0.24925733299999614 \t 0.16666666666666666\n",
            "i 0.24925733399999614 \t 0.16666666666666666\n",
            "i 0.24925733499999614 \t 0.16666666666666666\n",
            "i 0.24925733599999614 \t 0.16666666666666666\n",
            "i 0.24925733699999614 \t 0.16666666666666666\n",
            "i 0.24925733799999614 \t 0.16666666666666666\n",
            "i 0.24925733899999614 \t 0.16666666666666666\n",
            "i 0.24925733999999614 \t 0.16666666666666666\n",
            "i 0.24925734099999614 \t 0.16666666666666666\n",
            "i 0.24925734199999613 \t 0.16666666666666666\n",
            "i 0.24925734299999613 \t 0.16666666666666666\n",
            "i 0.24925734399999613 \t 0.16666666666666666\n",
            "i 0.24925734499999613 \t 0.16666666666666666\n",
            "i 0.24925734599999613 \t 0.16666666666666666\n",
            "i 0.24925734699999613 \t 0.16666666666666666\n",
            "i 0.24925734799999613 \t 0.16666666666666666\n",
            "i 0.24925734899999613 \t 0.16666666666666666\n",
            "i 0.24925734999999613 \t 0.16666666666666666\n",
            "i 0.24925735099999613 \t 0.16666666666666666\n",
            "i 0.24925735199999613 \t 0.16666666666666666\n",
            "i 0.24925735299999613 \t 0.16666666666666666\n",
            "i 0.24925735399999613 \t 0.16666666666666666\n",
            "i 0.24925735499999613 \t 0.16666666666666666\n",
            "i 0.24925735599999613 \t 0.16666666666666666\n",
            "i 0.24925735699999613 \t 0.16666666666666666\n",
            "i 0.24925735799999613 \t 0.16666666666666666\n",
            "i 0.24925735899999613 \t 0.16666666666666666\n",
            "i 0.24925735999999613 \t 0.16666666666666666\n",
            "i 0.24925736099999612 \t 0.16666666666666666\n",
            "i 0.24925736199999612 \t 0.16666666666666666\n",
            "i 0.24925736299999612 \t 0.16666666666666666\n",
            "i 0.24925736399999612 \t 0.16666666666666666\n",
            "i 0.24925736499999612 \t 0.16666666666666666\n",
            "i 0.24925736599999612 \t 0.16666666666666666\n",
            "i 0.24925736699999612 \t 0.16666666666666666\n",
            "i 0.24925736799999612 \t 0.16666666666666666\n",
            "i 0.24925736899999612 \t 0.16666666666666666\n",
            "i 0.24925736999999612 \t 0.16666666666666666\n",
            "i 0.24925737099999612 \t 0.16666666666666666\n",
            "i 0.24925737199999612 \t 0.16666666666666666\n",
            "i 0.24925737299999612 \t 0.16666666666666666\n",
            "i 0.24925737399999612 \t 0.16666666666666666\n",
            "i 0.24925737499999612 \t 0.16666666666666666\n",
            "i 0.24925737599999612 \t 0.16666666666666666\n",
            "i 0.24925737699999612 \t 0.16666666666666666\n",
            "i 0.24925737799999612 \t 0.16666666666666666\n",
            "i 0.24925737899999612 \t 0.16666666666666666\n",
            "i 0.24925737999999611 \t 0.16666666666666666\n",
            "i 0.24925738099999611 \t 0.16666666666666666\n",
            "i 0.2492573819999961 \t 0.16666666666666666\n",
            "i 0.2492573829999961 \t 0.16666666666666666\n",
            "i 0.2492573839999961 \t 0.16666666666666666\n",
            "i 0.2492573849999961 \t 0.16666666666666666\n",
            "i 0.2492573859999961 \t 0.16666666666666666\n",
            "i 0.2492573869999961 \t 0.16666666666666666\n",
            "i 0.2492573879999961 \t 0.16666666666666666\n",
            "i 0.2492573889999961 \t 0.16666666666666666\n",
            "i 0.2492573899999961 \t 0.16666666666666666\n",
            "i 0.2492573909999961 \t 0.16666666666666666\n",
            "i 0.2492573919999961 \t 0.16666666666666666\n",
            "i 0.2492573929999961 \t 0.16666666666666666\n",
            "i 0.2492573939999961 \t 0.16666666666666666\n",
            "i 0.2492573949999961 \t 0.16666666666666666\n",
            "i 0.2492573959999961 \t 0.16666666666666666\n",
            "i 0.2492573969999961 \t 0.16666666666666666\n",
            "i 0.2492573979999961 \t 0.16666666666666666\n",
            "i 0.2492573989999961 \t 0.16666666666666666\n",
            "i 0.2492573999999961 \t 0.16666666666666666\n",
            "i 0.2492574009999961 \t 0.16666666666666666\n",
            "i 0.2492574019999961 \t 0.16666666666666666\n",
            "i 0.2492574029999961 \t 0.16666666666666666\n",
            "i 0.2492574039999961 \t 0.16666666666666666\n",
            "i 0.2492574049999961 \t 0.16666666666666666\n",
            "i 0.2492574059999961 \t 0.16666666666666666\n",
            "i 0.2492574069999961 \t 0.16666666666666666\n",
            "i 0.2492574079999961 \t 0.16666666666666666\n",
            "i 0.2492574089999961 \t 0.16666666666666666\n",
            "i 0.2492574099999961 \t 0.16666666666666666\n",
            "i 0.2492574109999961 \t 0.16666666666666666\n",
            "i 0.2492574119999961 \t 0.16666666666666666\n",
            "i 0.2492574129999961 \t 0.16666666666666666\n",
            "i 0.2492574139999961 \t 0.16666666666666666\n",
            "i 0.2492574149999961 \t 0.16666666666666666\n",
            "i 0.2492574159999961 \t 0.16666666666666666\n",
            "i 0.2492574169999961 \t 0.16666666666666666\n",
            "i 0.2492574179999961 \t 0.16666666666666666\n",
            "i 0.2492574189999961 \t 0.16666666666666666\n",
            "i 0.2492574199999961 \t 0.16666666666666666\n",
            "i 0.2492574209999961 \t 0.16666666666666666\n",
            "i 0.2492574219999961 \t 0.16666666666666666\n",
            "i 0.2492574229999961 \t 0.16666666666666666\n",
            "i 0.2492574239999961 \t 0.16666666666666666\n",
            "i 0.2492574249999961 \t 0.16666666666666666\n",
            "i 0.2492574259999961 \t 0.16666666666666666\n",
            "i 0.2492574269999961 \t 0.16666666666666666\n",
            "i 0.2492574279999961 \t 0.16666666666666666\n",
            "i 0.2492574289999961 \t 0.16666666666666666\n",
            "i 0.2492574299999961 \t 0.16666666666666666\n",
            "i 0.2492574309999961 \t 0.16666666666666666\n",
            "i 0.2492574319999961 \t 0.16666666666666666\n",
            "i 0.2492574329999961 \t 0.16666666666666666\n",
            "i 0.2492574339999961 \t 0.16666666666666666\n",
            "i 0.24925743499999609 \t 0.16666666666666666\n",
            "i 0.24925743599999609 \t 0.16666666666666666\n",
            "i 0.24925743699999608 \t 0.16666666666666666\n",
            "i 0.24925743799999608 \t 0.16666666666666666\n",
            "i 0.24925743899999608 \t 0.16666666666666666\n",
            "i 0.24925743999999608 \t 0.16666666666666666\n",
            "i 0.24925744099999608 \t 0.16666666666666666\n",
            "i 0.24925744199999608 \t 0.16666666666666666\n",
            "i 0.24925744299999608 \t 0.16666666666666666\n",
            "i 0.24925744399999608 \t 0.16666666666666666\n",
            "i 0.24925744499999608 \t 0.16666666666666666\n",
            "i 0.24925744599999608 \t 0.16666666666666666\n",
            "i 0.24925744699999608 \t 0.16666666666666666\n",
            "i 0.24925744799999608 \t 0.16666666666666666\n",
            "i 0.24925744899999608 \t 0.16666666666666666\n",
            "i 0.24925744999999608 \t 0.16666666666666666\n",
            "i 0.24925745099999608 \t 0.16666666666666666\n",
            "i 0.24925745199999608 \t 0.16666666666666666\n",
            "i 0.24925745299999608 \t 0.16666666666666666\n",
            "i 0.24925745399999608 \t 0.16666666666666666\n",
            "i 0.24925745499999608 \t 0.16666666666666666\n",
            "i 0.24925745599999607 \t 0.16666666666666666\n",
            "i 0.24925745699999607 \t 0.16666666666666666\n",
            "i 0.24925745799999607 \t 0.16666666666666666\n",
            "i 0.24925745899999607 \t 0.16666666666666666\n",
            "i 0.24925745999999607 \t 0.16666666666666666\n",
            "i 0.24925746099999607 \t 0.16666666666666666\n",
            "i 0.24925746199999607 \t 0.16666666666666666\n",
            "i 0.24925746299999607 \t 0.16666666666666666\n",
            "i 0.24925746399999607 \t 0.16666666666666666\n",
            "i 0.24925746499999607 \t 0.16666666666666666\n",
            "i 0.24925746599999607 \t 0.16666666666666666\n",
            "i 0.24925746699999607 \t 0.16666666666666666\n",
            "i 0.24925746799999607 \t 0.16666666666666666\n",
            "i 0.24925746899999607 \t 0.16666666666666666\n",
            "i 0.24925746999999607 \t 0.16666666666666666\n",
            "i 0.24925747099999607 \t 0.16666666666666666\n",
            "i 0.24925747199999607 \t 0.16666666666666666\n",
            "i 0.24925747299999607 \t 0.16666666666666666\n",
            "i 0.24925747399999607 \t 0.16666666666666666\n",
            "i 0.24925747499999606 \t 0.16666666666666666\n",
            "i 0.24925747599999606 \t 0.16666666666666666\n",
            "i 0.24925747699999606 \t 0.16666666666666666\n",
            "i 0.24925747799999606 \t 0.16666666666666666\n",
            "i 0.24925747899999606 \t 0.16666666666666666\n",
            "i 0.24925747999999606 \t 0.16666666666666666\n",
            "i 0.24925748099999606 \t 0.16666666666666666\n",
            "i 0.24925748199999606 \t 0.16666666666666666\n",
            "i 0.24925748299999606 \t 0.16666666666666666\n",
            "i 0.24925748399999606 \t 0.16666666666666666\n",
            "i 0.24925748499999606 \t 0.16666666666666666\n",
            "i 0.24925748599999606 \t 0.16666666666666666\n",
            "i 0.24925748699999606 \t 0.16666666666666666\n",
            "i 0.24925748799999606 \t 0.16666666666666666\n",
            "i 0.24925748899999606 \t 0.16666666666666666\n",
            "i 0.24925748999999606 \t 0.16666666666666666\n",
            "i 0.24925749099999606 \t 0.16666666666666666\n",
            "i 0.24925749199999606 \t 0.16666666666666666\n",
            "i 0.24925749299999606 \t 0.16666666666666666\n",
            "i 0.24925749399999605 \t 0.16666666666666666\n",
            "i 0.24925749499999605 \t 0.16666666666666666\n",
            "i 0.24925749599999605 \t 0.16666666666666666\n",
            "i 0.24925749699999605 \t 0.16666666666666666\n",
            "i 0.24925749799999605 \t 0.16666666666666666\n",
            "i 0.24925749899999605 \t 0.16666666666666666\n",
            "i 0.24925749999999605 \t 0.16666666666666666\n",
            "i 0.24925750099999605 \t 0.16666666666666666\n",
            "i 0.24925750199999605 \t 0.16666666666666666\n",
            "i 0.24925750299999605 \t 0.16666666666666666\n",
            "i 0.24925750399999605 \t 0.16666666666666666\n",
            "i 0.24925750499999605 \t 0.16666666666666666\n",
            "i 0.24925750599999605 \t 0.16666666666666666\n",
            "i 0.24925750699999605 \t 0.16666666666666666\n",
            "i 0.24925750799999605 \t 0.16666666666666666\n",
            "i 0.24925750899999605 \t 0.16666666666666666\n",
            "i 0.24925750999999605 \t 0.16666666666666666\n",
            "i 0.24925751099999605 \t 0.16666666666666666\n",
            "i 0.24925751199999605 \t 0.16666666666666666\n",
            "i 0.24925751299999604 \t 0.16666666666666666\n",
            "i 0.24925751399999604 \t 0.16666666666666666\n",
            "i 0.24925751499999604 \t 0.16666666666666666\n",
            "i 0.24925751599999604 \t 0.16666666666666666\n",
            "i 0.24925751699999604 \t 0.16666666666666666\n",
            "i 0.24925751799999604 \t 0.16666666666666666\n",
            "i 0.24925751899999604 \t 0.16666666666666666\n",
            "i 0.24925751999999604 \t 0.16666666666666666\n",
            "i 0.24925752099999604 \t 0.16666666666666666\n",
            "i 0.24925752199999604 \t 0.16666666666666666\n",
            "i 0.24925752299999604 \t 0.16666666666666666\n",
            "i 0.24925752399999604 \t 0.16666666666666666\n",
            "i 0.24925752499999604 \t 0.16666666666666666\n",
            "i 0.24925752599999604 \t 0.16666666666666666\n",
            "i 0.24925752699999604 \t 0.16666666666666666\n",
            "i 0.24925752799999604 \t 0.16666666666666666\n",
            "i 0.24925752899999604 \t 0.16666666666666666\n",
            "i 0.24925752999999604 \t 0.16666666666666666\n",
            "i 0.24925753099999604 \t 0.16666666666666666\n",
            "i 0.24925753199999603 \t 0.16666666666666666\n",
            "i 0.24925753299999603 \t 0.16666666666666666\n",
            "i 0.24925753399999603 \t 0.16666666666666666\n",
            "i 0.24925753499999603 \t 0.16666666666666666\n",
            "i 0.24925753599999603 \t 0.16666666666666666\n",
            "i 0.24925753699999603 \t 0.16666666666666666\n",
            "i 0.24925753799999603 \t 0.16666666666666666\n",
            "i 0.24925753899999603 \t 0.16666666666666666\n",
            "i 0.24925753999999603 \t 0.16666666666666666\n",
            "i 0.24925754099999603 \t 0.16666666666666666\n",
            "i 0.24925754199999603 \t 0.16666666666666666\n",
            "i 0.24925754299999603 \t 0.16666666666666666\n",
            "i 0.24925754399999603 \t 0.16666666666666666\n",
            "i 0.24925754499999603 \t 0.16666666666666666\n",
            "i 0.24925754599999603 \t 0.16666666666666666\n",
            "i 0.24925754699999603 \t 0.16666666666666666\n",
            "i 0.24925754799999603 \t 0.16666666666666666\n",
            "i 0.24925754899999603 \t 0.16666666666666666\n",
            "i 0.24925754999999603 \t 0.16666666666666666\n",
            "i 0.24925755099999602 \t 0.16666666666666666\n",
            "i 0.24925755199999602 \t 0.16666666666666666\n",
            "i 0.24925755299999602 \t 0.16666666666666666\n",
            "i 0.24925755399999602 \t 0.16666666666666666\n",
            "i 0.24925755499999602 \t 0.16666666666666666\n",
            "i 0.24925755599999602 \t 0.16666666666666666\n",
            "i 0.24925755699999602 \t 0.16666666666666666\n",
            "i 0.24925755799999602 \t 0.16666666666666666\n",
            "i 0.24925755899999602 \t 0.16666666666666666\n",
            "i 0.24925755999999602 \t 0.16666666666666666\n",
            "i 0.24925756099999602 \t 0.16666666666666666\n",
            "i 0.24925756199999602 \t 0.16666666666666666\n",
            "i 0.24925756299999602 \t 0.16666666666666666\n",
            "i 0.24925756399999602 \t 0.16666666666666666\n",
            "i 0.24925756499999602 \t 0.16666666666666666\n",
            "i 0.24925756599999602 \t 0.16666666666666666\n",
            "i 0.24925756699999602 \t 0.16666666666666666\n",
            "i 0.24925756799999602 \t 0.16666666666666666\n",
            "i 0.24925756899999602 \t 0.16666666666666666\n",
            "i 0.24925756999999601 \t 0.16666666666666666\n",
            "i 0.24925757099999601 \t 0.16666666666666666\n",
            "i 0.249257571999996 \t 0.16666666666666666\n",
            "i 0.249257572999996 \t 0.16666666666666666\n",
            "i 0.249257573999996 \t 0.16666666666666666\n",
            "i 0.249257574999996 \t 0.16666666666666666\n",
            "i 0.249257575999996 \t 0.16666666666666666\n",
            "i 0.249257576999996 \t 0.16666666666666666\n",
            "i 0.249257577999996 \t 0.16666666666666666\n",
            "i 0.249257578999996 \t 0.16666666666666666\n",
            "i 0.249257579999996 \t 0.16666666666666666\n",
            "i 0.249257580999996 \t 0.16666666666666666\n",
            "i 0.249257581999996 \t 0.16666666666666666\n",
            "i 0.249257582999996 \t 0.16666666666666666\n",
            "i 0.249257583999996 \t 0.16666666666666666\n",
            "i 0.249257584999996 \t 0.16666666666666666\n",
            "i 0.249257585999996 \t 0.16666666666666666\n",
            "i 0.249257586999996 \t 0.16666666666666666\n",
            "i 0.249257587999996 \t 0.16666666666666666\n",
            "i 0.249257588999996 \t 0.16666666666666666\n",
            "i 0.249257589999996 \t 0.16666666666666666\n",
            "i 0.249257590999996 \t 0.16666666666666666\n",
            "i 0.249257591999996 \t 0.16666666666666666\n",
            "i 0.249257592999996 \t 0.16666666666666666\n",
            "i 0.249257593999996 \t 0.16666666666666666\n",
            "i 0.249257594999996 \t 0.16666666666666666\n",
            "i 0.249257595999996 \t 0.16666666666666666\n",
            "i 0.249257596999996 \t 0.16666666666666666\n",
            "i 0.249257597999996 \t 0.16666666666666666\n",
            "i 0.249257598999996 \t 0.16666666666666666\n",
            "i 0.249257599999996 \t 0.16666666666666666\n",
            "i 0.249257600999996 \t 0.16666666666666666\n",
            "i 0.249257601999996 \t 0.16666666666666666\n",
            "i 0.249257602999996 \t 0.16666666666666666\n",
            "i 0.249257603999996 \t 0.16666666666666666\n",
            "i 0.249257604999996 \t 0.16666666666666666\n",
            "i 0.249257605999996 \t 0.16666666666666666\n",
            "i 0.249257606999996 \t 0.16666666666666666\n",
            "i 0.249257607999996 \t 0.16666666666666666\n",
            "i 0.249257608999996 \t 0.16666666666666666\n",
            "i 0.249257609999996 \t 0.16666666666666666\n",
            "i 0.249257610999996 \t 0.16666666666666666\n",
            "i 0.249257611999996 \t 0.16666666666666666\n",
            "i 0.249257612999996 \t 0.16666666666666666\n",
            "i 0.249257613999996 \t 0.16666666666666666\n",
            "i 0.249257614999996 \t 0.16666666666666666\n",
            "i 0.249257615999996 \t 0.16666666666666666\n",
            "i 0.249257616999996 \t 0.16666666666666666\n",
            "i 0.249257617999996 \t 0.16666666666666666\n",
            "i 0.249257618999996 \t 0.16666666666666666\n",
            "i 0.249257619999996 \t 0.16666666666666666\n",
            "i 0.249257620999996 \t 0.16666666666666666\n",
            "i 0.249257621999996 \t 0.16666666666666666\n",
            "i 0.249257622999996 \t 0.16666666666666666\n",
            "i 0.249257623999996 \t 0.16666666666666666\n",
            "i 0.24925762499999599 \t 0.16666666666666666\n",
            "i 0.24925762599999599 \t 0.16666666666666666\n",
            "i 0.24925762699999598 \t 0.16666666666666666\n",
            "i 0.24925762799999598 \t 0.16666666666666666\n",
            "i 0.24925762899999598 \t 0.16666666666666666\n",
            "i 0.24925762999999598 \t 0.16666666666666666\n",
            "i 0.24925763099999598 \t 0.16666666666666666\n",
            "i 0.24925763199999598 \t 0.16666666666666666\n",
            "i 0.24925763299999598 \t 0.16666666666666666\n",
            "i 0.24925763399999598 \t 0.16666666666666666\n",
            "i 0.24925763499999598 \t 0.16666666666666666\n",
            "i 0.24925763599999598 \t 0.16666666666666666\n",
            "i 0.24925763699999598 \t 0.16666666666666666\n",
            "i 0.24925763799999598 \t 0.16666666666666666\n",
            "i 0.24925763899999598 \t 0.16666666666666666\n",
            "i 0.24925763999999598 \t 0.16666666666666666\n",
            "i 0.24925764099999598 \t 0.16666666666666666\n",
            "i 0.24925764199999598 \t 0.16666666666666666\n",
            "i 0.24925764299999598 \t 0.16666666666666666\n",
            "i 0.24925764399999598 \t 0.16666666666666666\n",
            "i 0.24925764499999598 \t 0.16666666666666666\n",
            "i 0.24925764599999597 \t 0.16666666666666666\n",
            "i 0.24925764699999597 \t 0.16666666666666666\n",
            "i 0.24925764799999597 \t 0.16666666666666666\n",
            "i 0.24925764899999597 \t 0.16666666666666666\n",
            "i 0.24925764999999597 \t 0.16666666666666666\n",
            "i 0.24925765099999597 \t 0.16666666666666666\n",
            "i 0.24925765199999597 \t 0.16666666666666666\n",
            "i 0.24925765299999597 \t 0.16666666666666666\n",
            "i 0.24925765399999597 \t 0.16666666666666666\n",
            "i 0.24925765499999597 \t 0.16666666666666666\n",
            "i 0.24925765599999597 \t 0.16666666666666666\n",
            "i 0.24925765699999597 \t 0.16666666666666666\n",
            "i 0.24925765799999597 \t 0.16666666666666666\n",
            "i 0.24925765899999597 \t 0.16666666666666666\n",
            "i 0.24925765999999597 \t 0.16666666666666666\n",
            "i 0.24925766099999597 \t 0.16666666666666666\n",
            "i 0.24925766199999597 \t 0.16666666666666666\n",
            "i 0.24925766299999597 \t 0.16666666666666666\n",
            "i 0.24925766399999597 \t 0.16666666666666666\n",
            "i 0.24925766499999596 \t 0.16666666666666666\n",
            "i 0.24925766599999596 \t 0.16666666666666666\n",
            "i 0.24925766699999596 \t 0.16666666666666666\n",
            "i 0.24925766799999596 \t 0.16666666666666666\n",
            "i 0.24925766899999596 \t 0.16666666666666666\n",
            "i 0.24925766999999596 \t 0.16666666666666666\n",
            "i 0.24925767099999596 \t 0.16666666666666666\n",
            "i 0.24925767199999596 \t 0.16666666666666666\n",
            "i 0.24925767299999596 \t 0.16666666666666666\n",
            "i 0.24925767399999596 \t 0.16666666666666666\n",
            "i 0.24925767499999596 \t 0.16666666666666666\n",
            "i 0.24925767599999596 \t 0.16666666666666666\n",
            "i 0.24925767699999596 \t 0.16666666666666666\n",
            "i 0.24925767799999596 \t 0.16666666666666666\n",
            "i 0.24925767899999596 \t 0.16666666666666666\n",
            "i 0.24925767999999596 \t 0.16666666666666666\n",
            "i 0.24925768099999596 \t 0.16666666666666666\n",
            "i 0.24925768199999596 \t 0.16666666666666666\n",
            "i 0.24925768299999596 \t 0.16666666666666666\n",
            "i 0.24925768399999595 \t 0.16666666666666666\n",
            "i 0.24925768499999595 \t 0.16666666666666666\n",
            "i 0.24925768599999595 \t 0.16666666666666666\n",
            "i 0.24925768699999595 \t 0.16666666666666666\n",
            "i 0.24925768799999595 \t 0.16666666666666666\n",
            "i 0.24925768899999595 \t 0.16666666666666666\n",
            "i 0.24925768999999595 \t 0.16666666666666666\n",
            "i 0.24925769099999595 \t 0.16666666666666666\n",
            "i 0.24925769199999595 \t 0.16666666666666666\n",
            "i 0.24925769299999595 \t 0.16666666666666666\n",
            "i 0.24925769399999595 \t 0.16666666666666666\n",
            "i 0.24925769499999595 \t 0.16666666666666666\n",
            "i 0.24925769599999595 \t 0.16666666666666666\n",
            "i 0.24925769699999595 \t 0.16666666666666666\n",
            "i 0.24925769799999595 \t 0.16666666666666666\n",
            "i 0.24925769899999595 \t 0.16666666666666666\n",
            "i 0.24925769999999595 \t 0.16666666666666666\n",
            "i 0.24925770099999595 \t 0.16666666666666666\n",
            "i 0.24925770199999595 \t 0.16666666666666666\n",
            "i 0.24925770299999594 \t 0.16666666666666666\n",
            "i 0.24925770399999594 \t 0.16666666666666666\n",
            "i 0.24925770499999594 \t 0.16666666666666666\n",
            "i 0.24925770599999594 \t 0.16666666666666666\n",
            "i 0.24925770699999594 \t 0.16666666666666666\n",
            "i 0.24925770799999594 \t 0.16666666666666666\n",
            "i 0.24925770899999594 \t 0.16666666666666666\n",
            "i 0.24925770999999594 \t 0.16666666666666666\n",
            "i 0.24925771099999594 \t 0.16666666666666666\n",
            "i 0.24925771199999594 \t 0.16666666666666666\n",
            "i 0.24925771299999594 \t 0.16666666666666666\n",
            "i 0.24925771399999594 \t 0.16666666666666666\n",
            "i 0.24925771499999594 \t 0.16666666666666666\n",
            "i 0.24925771599999594 \t 0.16666666666666666\n",
            "i 0.24925771699999594 \t 0.16666666666666666\n",
            "i 0.24925771799999594 \t 0.16666666666666666\n",
            "i 0.24925771899999594 \t 0.16666666666666666\n",
            "i 0.24925771999999594 \t 0.16666666666666666\n",
            "i 0.24925772099999594 \t 0.16666666666666666\n",
            "i 0.24925772199999593 \t 0.16666666666666666\n",
            "i 0.24925772299999593 \t 0.16666666666666666\n",
            "i 0.24925772399999593 \t 0.16666666666666666\n",
            "i 0.24925772499999593 \t 0.16666666666666666\n",
            "i 0.24925772599999593 \t 0.16666666666666666\n",
            "i 0.24925772699999593 \t 0.16666666666666666\n",
            "i 0.24925772799999593 \t 0.16666666666666666\n",
            "i 0.24925772899999593 \t 0.16666666666666666\n",
            "i 0.24925772999999593 \t 0.16666666666666666\n",
            "i 0.24925773099999593 \t 0.16666666666666666\n",
            "i 0.24925773199999593 \t 0.16666666666666666\n",
            "i 0.24925773299999593 \t 0.16666666666666666\n",
            "i 0.24925773399999593 \t 0.16666666666666666\n",
            "i 0.24925773499999593 \t 0.16666666666666666\n",
            "i 0.24925773599999593 \t 0.16666666666666666\n",
            "i 0.24925773699999593 \t 0.16666666666666666\n",
            "i 0.24925773799999593 \t 0.16666666666666666\n",
            "i 0.24925773899999593 \t 0.16666666666666666\n",
            "i 0.24925773999999593 \t 0.16666666666666666\n",
            "i 0.24925774099999592 \t 0.16666666666666666\n",
            "i 0.24925774199999592 \t 0.16666666666666666\n",
            "i 0.24925774299999592 \t 0.16666666666666666\n",
            "i 0.24925774399999592 \t 0.16666666666666666\n",
            "i 0.24925774499999592 \t 0.16666666666666666\n",
            "i 0.24925774599999592 \t 0.16666666666666666\n",
            "i 0.24925774699999592 \t 0.16666666666666666\n",
            "i 0.24925774799999592 \t 0.16666666666666666\n",
            "i 0.24925774899999592 \t 0.16666666666666666\n",
            "i 0.24925774999999592 \t 0.16666666666666666\n",
            "i 0.24925775099999592 \t 0.16666666666666666\n",
            "i 0.24925775199999592 \t 0.16666666666666666\n",
            "i 0.24925775299999592 \t 0.16666666666666666\n",
            "i 0.24925775399999592 \t 0.16666666666666666\n",
            "i 0.24925775499999592 \t 0.16666666666666666\n",
            "i 0.24925775599999592 \t 0.16666666666666666\n",
            "i 0.24925775699999592 \t 0.16666666666666666\n",
            "i 0.24925775799999592 \t 0.16666666666666666\n",
            "i 0.24925775899999592 \t 0.16666666666666666\n",
            "i 0.24925775999999591 \t 0.16666666666666666\n",
            "i 0.24925776099999591 \t 0.16666666666666666\n",
            "i 0.2492577619999959 \t 0.16666666666666666\n",
            "i 0.2492577629999959 \t 0.16666666666666666\n",
            "i 0.2492577639999959 \t 0.16666666666666666\n",
            "i 0.2492577649999959 \t 0.16666666666666666\n",
            "i 0.2492577659999959 \t 0.16666666666666666\n",
            "i 0.2492577669999959 \t 0.16666666666666666\n",
            "i 0.2492577679999959 \t 0.16666666666666666\n",
            "i 0.2492577689999959 \t 0.16666666666666666\n",
            "i 0.2492577699999959 \t 0.16666666666666666\n",
            "i 0.2492577709999959 \t 0.16666666666666666\n",
            "i 0.2492577719999959 \t 0.16666666666666666\n",
            "i 0.2492577729999959 \t 0.16666666666666666\n",
            "i 0.2492577739999959 \t 0.16666666666666666\n",
            "i 0.2492577749999959 \t 0.16666666666666666\n",
            "i 0.2492577759999959 \t 0.16666666666666666\n",
            "i 0.2492577769999959 \t 0.16666666666666666\n",
            "i 0.2492577779999959 \t 0.16666666666666666\n",
            "i 0.2492577789999959 \t 0.16666666666666666\n",
            "i 0.2492577799999959 \t 0.16666666666666666\n",
            "i 0.2492577809999959 \t 0.16666666666666666\n",
            "i 0.2492577819999959 \t 0.16666666666666666\n",
            "i 0.2492577829999959 \t 0.16666666666666666\n",
            "i 0.2492577839999959 \t 0.16666666666666666\n",
            "i 0.2492577849999959 \t 0.16666666666666666\n",
            "i 0.2492577859999959 \t 0.16666666666666666\n",
            "i 0.2492577869999959 \t 0.16666666666666666\n",
            "i 0.2492577879999959 \t 0.16666666666666666\n",
            "i 0.2492577889999959 \t 0.16666666666666666\n",
            "i 0.2492577899999959 \t 0.16666666666666666\n",
            "i 0.2492577909999959 \t 0.16666666666666666\n",
            "i 0.2492577919999959 \t 0.16666666666666666\n",
            "i 0.2492577929999959 \t 0.16666666666666666\n",
            "i 0.2492577939999959 \t 0.16666666666666666\n",
            "i 0.2492577949999959 \t 0.16666666666666666\n",
            "i 0.2492577959999959 \t 0.16666666666666666\n",
            "i 0.2492577969999959 \t 0.16666666666666666\n",
            "i 0.2492577979999959 \t 0.16666666666666666\n",
            "i 0.2492577989999959 \t 0.16666666666666666\n",
            "i 0.2492577999999959 \t 0.16666666666666666\n",
            "i 0.2492578009999959 \t 0.16666666666666666\n",
            "i 0.2492578019999959 \t 0.16666666666666666\n",
            "i 0.2492578029999959 \t 0.16666666666666666\n",
            "i 0.2492578039999959 \t 0.16666666666666666\n",
            "i 0.2492578049999959 \t 0.16666666666666666\n",
            "i 0.2492578059999959 \t 0.16666666666666666\n",
            "i 0.2492578069999959 \t 0.16666666666666666\n",
            "i 0.2492578079999959 \t 0.16666666666666666\n",
            "i 0.2492578089999959 \t 0.16666666666666666\n",
            "i 0.2492578099999959 \t 0.16666666666666666\n",
            "i 0.2492578109999959 \t 0.16666666666666666\n",
            "i 0.2492578119999959 \t 0.16666666666666666\n",
            "i 0.2492578129999959 \t 0.16666666666666666\n",
            "i 0.2492578139999959 \t 0.16666666666666666\n",
            "i 0.24925781499999589 \t 0.16666666666666666\n",
            "i 0.24925781599999589 \t 0.16666666666666666\n",
            "i 0.24925781699999588 \t 0.16666666666666666\n",
            "i 0.24925781799999588 \t 0.16666666666666666\n",
            "i 0.24925781899999588 \t 0.16666666666666666\n",
            "i 0.24925781999999588 \t 0.16666666666666666\n",
            "i 0.24925782099999588 \t 0.16666666666666666\n",
            "i 0.24925782199999588 \t 0.16666666666666666\n",
            "i 0.24925782299999588 \t 0.16666666666666666\n",
            "i 0.24925782399999588 \t 0.16666666666666666\n",
            "i 0.24925782499999588 \t 0.16666666666666666\n",
            "i 0.24925782599999588 \t 0.16666666666666666\n",
            "i 0.24925782699999588 \t 0.16666666666666666\n",
            "i 0.24925782799999588 \t 0.16666666666666666\n",
            "i 0.24925782899999588 \t 0.16666666666666666\n",
            "i 0.24925782999999588 \t 0.16666666666666666\n",
            "i 0.24925783099999588 \t 0.16666666666666666\n",
            "i 0.24925783199999588 \t 0.16666666666666666\n",
            "i 0.24925783299999588 \t 0.16666666666666666\n",
            "i 0.24925783399999588 \t 0.16666666666666666\n",
            "i 0.24925783499999588 \t 0.16666666666666666\n",
            "i 0.24925783599999587 \t 0.16666666666666666\n",
            "i 0.24925783699999587 \t 0.16666666666666666\n",
            "i 0.24925783799999587 \t 0.16666666666666666\n",
            "i 0.24925783899999587 \t 0.16666666666666666\n",
            "i 0.24925783999999587 \t 0.16666666666666666\n",
            "i 0.24925784099999587 \t 0.16666666666666666\n",
            "i 0.24925784199999587 \t 0.16666666666666666\n",
            "i 0.24925784299999587 \t 0.16666666666666666\n",
            "i 0.24925784399999587 \t 0.16666666666666666\n",
            "i 0.24925784499999587 \t 0.16666666666666666\n",
            "i 0.24925784599999587 \t 0.16666666666666666\n",
            "i 0.24925784699999587 \t 0.16666666666666666\n",
            "i 0.24925784799999587 \t 0.16666666666666666\n",
            "i 0.24925784899999587 \t 0.16666666666666666\n",
            "i 0.24925784999999587 \t 0.16666666666666666\n",
            "i 0.24925785099999587 \t 0.16666666666666666\n",
            "i 0.24925785199999587 \t 0.16666666666666666\n",
            "i 0.24925785299999587 \t 0.16666666666666666\n",
            "i 0.24925785399999587 \t 0.16666666666666666\n",
            "i 0.24925785499999586 \t 0.16666666666666666\n",
            "i 0.24925785599999586 \t 0.16666666666666666\n",
            "i 0.24925785699999586 \t 0.16666666666666666\n",
            "i 0.24925785799999586 \t 0.16666666666666666\n",
            "i 0.24925785899999586 \t 0.16666666666666666\n",
            "i 0.24925785999999586 \t 0.16666666666666666\n",
            "i 0.24925786099999586 \t 0.16666666666666666\n",
            "i 0.24925786199999586 \t 0.16666666666666666\n",
            "i 0.24925786299999586 \t 0.16666666666666666\n",
            "i 0.24925786399999586 \t 0.16666666666666666\n",
            "i 0.24925786499999586 \t 0.16666666666666666\n",
            "i 0.24925786599999586 \t 0.16666666666666666\n",
            "i 0.24925786699999586 \t 0.16666666666666666\n",
            "i 0.24925786799999586 \t 0.16666666666666666\n",
            "i 0.24925786899999586 \t 0.16666666666666666\n",
            "i 0.24925786999999586 \t 0.16666666666666666\n",
            "i 0.24925787099999586 \t 0.16666666666666666\n",
            "i 0.24925787199999586 \t 0.16666666666666666\n",
            "i 0.24925787299999586 \t 0.16666666666666666\n",
            "i 0.24925787399999585 \t 0.16666666666666666\n",
            "i 0.24925787499999585 \t 0.16666666666666666\n",
            "i 0.24925787599999585 \t 0.16666666666666666\n",
            "i 0.24925787699999585 \t 0.16666666666666666\n",
            "i 0.24925787799999585 \t 0.16666666666666666\n",
            "i 0.24925787899999585 \t 0.16666666666666666\n",
            "i 0.24925787999999585 \t 0.16666666666666666\n",
            "i 0.24925788099999585 \t 0.16666666666666666\n",
            "i 0.24925788199999585 \t 0.16666666666666666\n",
            "i 0.24925788299999585 \t 0.16666666666666666\n",
            "i 0.24925788399999585 \t 0.16666666666666666\n",
            "i 0.24925788499999585 \t 0.16666666666666666\n",
            "i 0.24925788599999585 \t 0.16666666666666666\n",
            "i 0.24925788699999585 \t 0.16666666666666666\n",
            "i 0.24925788799999585 \t 0.16666666666666666\n",
            "i 0.24925788899999585 \t 0.16666666666666666\n",
            "i 0.24925788999999585 \t 0.16666666666666666\n",
            "i 0.24925789099999585 \t 0.16666666666666666\n",
            "i 0.24925789199999585 \t 0.16666666666666666\n",
            "i 0.24925789299999584 \t 0.16666666666666666\n",
            "i 0.24925789399999584 \t 0.16666666666666666\n",
            "i 0.24925789499999584 \t 0.16666666666666666\n",
            "i 0.24925789599999584 \t 0.16666666666666666\n",
            "i 0.24925789699999584 \t 0.16666666666666666\n",
            "i 0.24925789799999584 \t 0.16666666666666666\n",
            "i 0.24925789899999584 \t 0.16666666666666666\n",
            "i 0.24925789999999584 \t 0.16666666666666666\n",
            "i 0.24925790099999584 \t 0.16666666666666666\n",
            "i 0.24925790199999584 \t 0.16666666666666666\n",
            "i 0.24925790299999584 \t 0.16666666666666666\n",
            "i 0.24925790399999584 \t 0.16666666666666666\n",
            "i 0.24925790499999584 \t 0.16666666666666666\n",
            "i 0.24925790599999584 \t 0.16666666666666666\n",
            "i 0.24925790699999584 \t 0.16666666666666666\n",
            "i 0.24925790799999584 \t 0.16666666666666666\n",
            "i 0.24925790899999584 \t 0.16666666666666666\n",
            "i 0.24925790999999584 \t 0.16666666666666666\n",
            "i 0.24925791099999584 \t 0.16666666666666666\n",
            "i 0.24925791199999583 \t 0.16666666666666666\n",
            "i 0.24925791299999583 \t 0.16666666666666666\n",
            "i 0.24925791399999583 \t 0.16666666666666666\n",
            "i 0.24925791499999583 \t 0.16666666666666666\n",
            "i 0.24925791599999583 \t 0.16666666666666666\n",
            "i 0.24925791699999583 \t 0.16666666666666666\n",
            "i 0.24925791799999583 \t 0.16666666666666666\n",
            "i 0.24925791899999583 \t 0.16666666666666666\n",
            "i 0.24925791999999583 \t 0.16666666666666666\n",
            "i 0.24925792099999583 \t 0.16666666666666666\n",
            "i 0.24925792199999583 \t 0.16666666666666666\n",
            "i 0.24925792299999583 \t 0.16666666666666666\n",
            "i 0.24925792399999583 \t 0.16666666666666666\n",
            "i 0.24925792499999583 \t 0.16666666666666666\n",
            "i 0.24925792599999583 \t 0.16666666666666666\n",
            "i 0.24925792699999583 \t 0.16666666666666666\n",
            "i 0.24925792799999583 \t 0.16666666666666666\n",
            "i 0.24925792899999583 \t 0.16666666666666666\n",
            "i 0.24925792999999583 \t 0.16666666666666666\n",
            "i 0.24925793099999582 \t 0.16666666666666666\n",
            "i 0.24925793199999582 \t 0.16666666666666666\n",
            "i 0.24925793299999582 \t 0.16666666666666666\n",
            "i 0.24925793399999582 \t 0.16666666666666666\n",
            "i 0.24925793499999582 \t 0.16666666666666666\n",
            "i 0.24925793599999582 \t 0.16666666666666666\n",
            "i 0.24925793699999582 \t 0.16666666666666666\n",
            "i 0.24925793799999582 \t 0.16666666666666666\n",
            "i 0.24925793899999582 \t 0.16666666666666666\n",
            "i 0.24925793999999582 \t 0.16666666666666666\n",
            "i 0.24925794099999582 \t 0.16666666666666666\n",
            "i 0.24925794199999582 \t 0.16666666666666666\n",
            "i 0.24925794299999582 \t 0.16666666666666666\n",
            "i 0.24925794399999582 \t 0.16666666666666666\n",
            "i 0.24925794499999582 \t 0.16666666666666666\n",
            "i 0.24925794599999582 \t 0.16666666666666666\n",
            "i 0.24925794699999582 \t 0.16666666666666666\n",
            "i 0.24925794799999582 \t 0.16666666666666666\n",
            "i 0.24925794899999582 \t 0.16666666666666666\n",
            "i 0.24925794999999581 \t 0.16666666666666666\n",
            "i 0.24925795099999581 \t 0.16666666666666666\n",
            "i 0.2492579519999958 \t 0.16666666666666666\n",
            "i 0.2492579529999958 \t 0.16666666666666666\n",
            "i 0.2492579539999958 \t 0.16666666666666666\n",
            "i 0.2492579549999958 \t 0.16666666666666666\n",
            "i 0.2492579559999958 \t 0.16666666666666666\n",
            "i 0.2492579569999958 \t 0.16666666666666666\n",
            "i 0.2492579579999958 \t 0.16666666666666666\n",
            "i 0.2492579589999958 \t 0.16666666666666666\n",
            "i 0.2492579599999958 \t 0.16666666666666666\n",
            "i 0.2492579609999958 \t 0.16666666666666666\n",
            "i 0.2492579619999958 \t 0.16666666666666666\n",
            "i 0.2492579629999958 \t 0.16666666666666666\n",
            "i 0.2492579639999958 \t 0.16666666666666666\n",
            "i 0.2492579649999958 \t 0.16666666666666666\n",
            "i 0.2492579659999958 \t 0.16666666666666666\n",
            "i 0.2492579669999958 \t 0.16666666666666666\n",
            "i 0.2492579679999958 \t 0.16666666666666666\n",
            "i 0.2492579689999958 \t 0.16666666666666666\n",
            "i 0.2492579699999958 \t 0.16666666666666666\n",
            "i 0.2492579709999958 \t 0.16666666666666666\n",
            "i 0.2492579719999958 \t 0.16666666666666666\n",
            "i 0.2492579729999958 \t 0.16666666666666666\n",
            "i 0.2492579739999958 \t 0.16666666666666666\n",
            "i 0.2492579749999958 \t 0.16666666666666666\n",
            "i 0.2492579759999958 \t 0.16666666666666666\n",
            "i 0.2492579769999958 \t 0.16666666666666666\n",
            "i 0.2492579779999958 \t 0.16666666666666666\n",
            "i 0.2492579789999958 \t 0.16666666666666666\n",
            "i 0.2492579799999958 \t 0.16666666666666666\n",
            "i 0.2492579809999958 \t 0.16666666666666666\n",
            "i 0.2492579819999958 \t 0.16666666666666666\n",
            "i 0.2492579829999958 \t 0.16666666666666666\n",
            "i 0.2492579839999958 \t 0.16666666666666666\n",
            "i 0.2492579849999958 \t 0.16666666666666666\n",
            "i 0.2492579859999958 \t 0.16666666666666666\n",
            "i 0.2492579869999958 \t 0.16666666666666666\n",
            "i 0.2492579879999958 \t 0.16666666666666666\n",
            "i 0.2492579889999958 \t 0.16666666666666666\n",
            "i 0.2492579899999958 \t 0.16666666666666666\n",
            "i 0.2492579909999958 \t 0.16666666666666666\n",
            "i 0.2492579919999958 \t 0.16666666666666666\n",
            "i 0.2492579929999958 \t 0.16666666666666666\n",
            "i 0.2492579939999958 \t 0.16666666666666666\n",
            "i 0.2492579949999958 \t 0.16666666666666666\n",
            "i 0.2492579959999958 \t 0.16666666666666666\n",
            "i 0.2492579969999958 \t 0.16666666666666666\n",
            "i 0.2492579979999958 \t 0.16666666666666666\n",
            "i 0.2492579989999958 \t 0.16666666666666666\n",
            "i 0.2492579999999958 \t 0.16666666666666666\n",
            "i 0.2492580009999958 \t 0.16666666666666666\n",
            "i 0.2492580019999958 \t 0.16666666666666666\n",
            "i 0.2492580029999958 \t 0.16666666666666666\n",
            "i 0.2492580039999958 \t 0.16666666666666666\n",
            "i 0.24925800499999579 \t 0.16666666666666666\n",
            "i 0.24925800599999579 \t 0.16666666666666666\n",
            "i 0.24925800699999578 \t 0.16666666666666666\n",
            "i 0.24925800799999578 \t 0.16666666666666666\n",
            "i 0.24925800899999578 \t 0.16666666666666666\n",
            "i 0.24925800999999578 \t 0.16666666666666666\n",
            "i 0.24925801099999578 \t 0.16666666666666666\n",
            "i 0.24925801199999578 \t 0.16666666666666666\n",
            "i 0.24925801299999578 \t 0.16666666666666666\n",
            "i 0.24925801399999578 \t 0.16666666666666666\n",
            "i 0.24925801499999578 \t 0.16666666666666666\n",
            "i 0.24925801599999578 \t 0.16666666666666666\n",
            "i 0.24925801699999578 \t 0.16666666666666666\n",
            "i 0.24925801799999578 \t 0.16666666666666666\n",
            "i 0.24925801899999578 \t 0.16666666666666666\n",
            "i 0.24925801999999578 \t 0.16666666666666666\n",
            "i 0.24925802099999578 \t 0.16666666666666666\n",
            "i 0.24925802199999578 \t 0.16666666666666666\n",
            "i 0.24925802299999578 \t 0.16666666666666666\n",
            "i 0.24925802399999578 \t 0.16666666666666666\n",
            "i 0.24925802499999578 \t 0.16666666666666666\n",
            "i 0.24925802599999577 \t 0.16666666666666666\n",
            "i 0.24925802699999577 \t 0.16666666666666666\n",
            "i 0.24925802799999577 \t 0.16666666666666666\n",
            "i 0.24925802899999577 \t 0.16666666666666666\n",
            "i 0.24925802999999577 \t 0.16666666666666666\n",
            "i 0.24925803099999577 \t 0.16666666666666666\n",
            "i 0.24925803199999577 \t 0.16666666666666666\n",
            "i 0.24925803299999577 \t 0.16666666666666666\n",
            "i 0.24925803399999577 \t 0.16666666666666666\n",
            "i 0.24925803499999577 \t 0.16666666666666666\n",
            "i 0.24925803599999577 \t 0.16666666666666666\n",
            "i 0.24925803699999577 \t 0.16666666666666666\n",
            "i 0.24925803799999577 \t 0.16666666666666666\n",
            "i 0.24925803899999577 \t 0.16666666666666666\n",
            "i 0.24925803999999577 \t 0.16666666666666666\n",
            "i 0.24925804099999577 \t 0.16666666666666666\n",
            "i 0.24925804199999577 \t 0.16666666666666666\n",
            "i 0.24925804299999577 \t 0.16666666666666666\n",
            "i 0.24925804399999577 \t 0.16666666666666666\n",
            "i 0.24925804499999576 \t 0.16666666666666666\n",
            "i 0.24925804599999576 \t 0.16666666666666666\n",
            "i 0.24925804699999576 \t 0.16666666666666666\n",
            "i 0.24925804799999576 \t 0.16666666666666666\n",
            "i 0.24925804899999576 \t 0.16666666666666666\n",
            "i 0.24925804999999576 \t 0.16666666666666666\n",
            "i 0.24925805099999576 \t 0.16666666666666666\n",
            "i 0.24925805199999576 \t 0.16666666666666666\n",
            "i 0.24925805299999576 \t 0.16666666666666666\n",
            "i 0.24925805399999576 \t 0.16666666666666666\n",
            "i 0.24925805499999576 \t 0.16666666666666666\n",
            "i 0.24925805599999576 \t 0.16666666666666666\n",
            "i 0.24925805699999576 \t 0.16666666666666666\n",
            "i 0.24925805799999576 \t 0.16666666666666666\n",
            "i 0.24925805899999576 \t 0.16666666666666666\n",
            "i 0.24925805999999576 \t 0.16666666666666666\n",
            "i 0.24925806099999576 \t 0.16666666666666666\n",
            "i 0.24925806199999576 \t 0.16666666666666666\n",
            "i 0.24925806299999576 \t 0.16666666666666666\n",
            "i 0.24925806399999575 \t 0.16666666666666666\n",
            "i 0.24925806499999575 \t 0.16666666666666666\n",
            "i 0.24925806599999575 \t 0.16666666666666666\n",
            "i 0.24925806699999575 \t 0.16666666666666666\n",
            "i 0.24925806799999575 \t 0.16666666666666666\n",
            "i 0.24925806899999575 \t 0.16666666666666666\n",
            "i 0.24925806999999575 \t 0.16666666666666666\n",
            "i 0.24925807099999575 \t 0.16666666666666666\n",
            "i 0.24925807199999575 \t 0.16666666666666666\n",
            "i 0.24925807299999575 \t 0.16666666666666666\n",
            "i 0.24925807399999575 \t 0.16666666666666666\n",
            "i 0.24925807499999575 \t 0.16666666666666666\n",
            "i 0.24925807599999575 \t 0.16666666666666666\n",
            "i 0.24925807699999575 \t 0.16666666666666666\n",
            "i 0.24925807799999575 \t 0.16666666666666666\n",
            "i 0.24925807899999575 \t 0.16666666666666666\n",
            "i 0.24925807999999575 \t 0.16666666666666666\n",
            "i 0.24925808099999575 \t 0.16666666666666666\n",
            "i 0.24925808199999575 \t 0.16666666666666666\n",
            "i 0.24925808299999574 \t 0.16666666666666666\n",
            "i 0.24925808399999574 \t 0.16666666666666666\n",
            "i 0.24925808499999574 \t 0.16666666666666666\n",
            "i 0.24925808599999574 \t 0.16666666666666666\n",
            "i 0.24925808699999574 \t 0.16666666666666666\n",
            "i 0.24925808799999574 \t 0.16666666666666666\n",
            "i 0.24925808899999574 \t 0.16666666666666666\n",
            "i 0.24925808999999574 \t 0.16666666666666666\n",
            "i 0.24925809099999574 \t 0.16666666666666666\n",
            "i 0.24925809199999574 \t 0.16666666666666666\n",
            "i 0.24925809299999574 \t 0.16666666666666666\n",
            "i 0.24925809399999574 \t 0.16666666666666666\n",
            "i 0.24925809499999574 \t 0.16666666666666666\n",
            "i 0.24925809599999574 \t 0.16666666666666666\n",
            "i 0.24925809699999574 \t 0.16666666666666666\n",
            "i 0.24925809799999574 \t 0.16666666666666666\n",
            "i 0.24925809899999574 \t 0.16666666666666666\n",
            "i 0.24925809999999574 \t 0.16666666666666666\n",
            "i 0.24925810099999574 \t 0.16666666666666666\n",
            "i 0.24925810199999573 \t 0.16666666666666666\n",
            "i 0.24925810299999573 \t 0.16666666666666666\n",
            "i 0.24925810399999573 \t 0.16666666666666666\n",
            "i 0.24925810499999573 \t 0.16666666666666666\n",
            "i 0.24925810599999573 \t 0.16666666666666666\n",
            "i 0.24925810699999573 \t 0.16666666666666666\n",
            "i 0.24925810799999573 \t 0.16666666666666666\n",
            "i 0.24925810899999573 \t 0.16666666666666666\n",
            "i 0.24925810999999573 \t 0.16666666666666666\n",
            "i 0.24925811099999573 \t 0.16666666666666666\n",
            "i 0.24925811199999573 \t 0.16666666666666666\n",
            "i 0.24925811299999573 \t 0.16666666666666666\n",
            "i 0.24925811399999573 \t 0.16666666666666666\n",
            "i 0.24925811499999573 \t 0.16666666666666666\n",
            "i 0.24925811599999573 \t 0.16666666666666666\n",
            "i 0.24925811699999573 \t 0.16666666666666666\n",
            "i 0.24925811799999573 \t 0.16666666666666666\n",
            "i 0.24925811899999573 \t 0.16666666666666666\n",
            "i 0.24925811999999573 \t 0.16666666666666666\n",
            "i 0.24925812099999572 \t 0.16666666666666666\n",
            "i 0.24925812199999572 \t 0.16666666666666666\n",
            "i 0.24925812299999572 \t 0.16666666666666666\n",
            "i 0.24925812399999572 \t 0.16666666666666666\n",
            "i 0.24925812499999572 \t 0.16666666666666666\n",
            "i 0.24925812599999572 \t 0.16666666666666666\n",
            "i 0.24925812699999572 \t 0.16666666666666666\n",
            "i 0.24925812799999572 \t 0.16666666666666666\n",
            "i 0.24925812899999572 \t 0.16666666666666666\n",
            "i 0.24925812999999572 \t 0.16666666666666666\n",
            "i 0.24925813099999572 \t 0.16666666666666666\n",
            "i 0.24925813199999572 \t 0.16666666666666666\n",
            "i 0.24925813299999572 \t 0.16666666666666666\n",
            "i 0.24925813399999572 \t 0.16666666666666666\n",
            "i 0.24925813499999572 \t 0.16666666666666666\n",
            "i 0.24925813599999572 \t 0.16666666666666666\n",
            "i 0.24925813699999572 \t 0.16666666666666666\n",
            "i 0.24925813799999572 \t 0.16666666666666666\n",
            "i 0.24925813899999572 \t 0.16666666666666666\n",
            "i 0.24925813999999571 \t 0.16666666666666666\n",
            "i 0.24925814099999571 \t 0.16666666666666666\n",
            "i 0.2492581419999957 \t 0.16666666666666666\n",
            "i 0.2492581429999957 \t 0.16666666666666666\n",
            "i 0.2492581439999957 \t 0.16666666666666666\n",
            "i 0.2492581449999957 \t 0.16666666666666666\n",
            "i 0.2492581459999957 \t 0.16666666666666666\n",
            "i 0.2492581469999957 \t 0.16666666666666666\n",
            "i 0.2492581479999957 \t 0.16666666666666666\n",
            "i 0.2492581489999957 \t 0.16666666666666666\n",
            "i 0.2492581499999957 \t 0.16666666666666666\n",
            "i 0.2492581509999957 \t 0.16666666666666666\n",
            "i 0.2492581519999957 \t 0.16666666666666666\n",
            "i 0.2492581529999957 \t 0.16666666666666666\n",
            "i 0.2492581539999957 \t 0.16666666666666666\n",
            "i 0.2492581549999957 \t 0.16666666666666666\n",
            "i 0.2492581559999957 \t 0.16666666666666666\n",
            "i 0.2492581569999957 \t 0.16666666666666666\n",
            "i 0.2492581579999957 \t 0.16666666666666666\n",
            "i 0.2492581589999957 \t 0.16666666666666666\n",
            "i 0.2492581599999957 \t 0.16666666666666666\n",
            "i 0.2492581609999957 \t 0.16666666666666666\n",
            "i 0.2492581619999957 \t 0.16666666666666666\n",
            "i 0.2492581629999957 \t 0.16666666666666666\n",
            "i 0.2492581639999957 \t 0.16666666666666666\n",
            "i 0.2492581649999957 \t 0.16666666666666666\n",
            "i 0.2492581659999957 \t 0.16666666666666666\n",
            "i 0.2492581669999957 \t 0.16666666666666666\n",
            "i 0.2492581679999957 \t 0.16666666666666666\n",
            "i 0.2492581689999957 \t 0.16666666666666666\n",
            "i 0.2492581699999957 \t 0.16666666666666666\n",
            "i 0.2492581709999957 \t 0.16666666666666666\n",
            "i 0.2492581719999957 \t 0.16666666666666666\n",
            "i 0.2492581729999957 \t 0.16666666666666666\n",
            "i 0.2492581739999957 \t 0.16666666666666666\n",
            "i 0.2492581749999957 \t 0.16666666666666666\n",
            "i 0.2492581759999957 \t 0.16666666666666666\n",
            "i 0.2492581769999957 \t 0.16666666666666666\n",
            "i 0.2492581779999957 \t 0.16666666666666666\n",
            "i 0.2492581789999957 \t 0.16666666666666666\n",
            "i 0.2492581799999957 \t 0.16666666666666666\n",
            "i 0.2492581809999957 \t 0.16666666666666666\n",
            "i 0.2492581819999957 \t 0.16666666666666666\n",
            "i 0.2492581829999957 \t 0.16666666666666666\n",
            "i 0.2492581839999957 \t 0.16666666666666666\n",
            "i 0.2492581849999957 \t 0.16666666666666666\n",
            "i 0.2492581859999957 \t 0.16666666666666666\n",
            "i 0.2492581869999957 \t 0.16666666666666666\n",
            "i 0.2492581879999957 \t 0.16666666666666666\n",
            "i 0.2492581889999957 \t 0.16666666666666666\n",
            "i 0.2492581899999957 \t 0.16666666666666666\n",
            "i 0.2492581909999957 \t 0.16666666666666666\n",
            "i 0.2492581919999957 \t 0.16666666666666666\n",
            "i 0.2492581929999957 \t 0.16666666666666666\n",
            "i 0.2492581939999957 \t 0.16666666666666666\n",
            "i 0.24925819499999569 \t 0.16666666666666666\n",
            "i 0.24925819599999569 \t 0.16666666666666666\n",
            "i 0.24925819699999568 \t 0.16666666666666666\n",
            "i 0.24925819799999568 \t 0.16666666666666666\n",
            "i 0.24925819899999568 \t 0.16666666666666666\n",
            "i 0.24925819999999568 \t 0.16666666666666666\n",
            "i 0.24925820099999568 \t 0.16666666666666666\n",
            "i 0.24925820199999568 \t 0.16666666666666666\n",
            "i 0.24925820299999568 \t 0.16666666666666666\n",
            "i 0.24925820399999568 \t 0.16666666666666666\n",
            "i 0.24925820499999568 \t 0.16666666666666666\n",
            "i 0.24925820599999568 \t 0.16666666666666666\n",
            "i 0.24925820699999568 \t 0.16666666666666666\n",
            "i 0.24925820799999568 \t 0.16666666666666666\n",
            "i 0.24925820899999568 \t 0.16666666666666666\n",
            "i 0.24925820999999568 \t 0.16666666666666666\n",
            "i 0.24925821099999568 \t 0.16666666666666666\n",
            "i 0.24925821199999568 \t 0.16666666666666666\n",
            "i 0.24925821299999568 \t 0.16666666666666666\n",
            "i 0.24925821399999568 \t 0.16666666666666666\n",
            "i 0.24925821499999568 \t 0.16666666666666666\n",
            "i 0.24925821599999567 \t 0.16666666666666666\n",
            "i 0.24925821699999567 \t 0.16666666666666666\n",
            "i 0.24925821799999567 \t 0.16666666666666666\n",
            "i 0.24925821899999567 \t 0.16666666666666666\n",
            "i 0.24925821999999567 \t 0.16666666666666666\n",
            "i 0.24925822099999567 \t 0.16666666666666666\n",
            "i 0.24925822199999567 \t 0.16666666666666666\n",
            "i 0.24925822299999567 \t 0.16666666666666666\n",
            "i 0.24925822399999567 \t 0.16666666666666666\n",
            "i 0.24925822499999567 \t 0.16666666666666666\n",
            "i 0.24925822599999567 \t 0.16666666666666666\n",
            "i 0.24925822699999567 \t 0.16666666666666666\n",
            "i 0.24925822799999567 \t 0.16666666666666666\n",
            "i 0.24925822899999567 \t 0.16666666666666666\n",
            "i 0.24925822999999567 \t 0.16666666666666666\n",
            "i 0.24925823099999567 \t 0.16666666666666666\n",
            "i 0.24925823199999567 \t 0.16666666666666666\n",
            "i 0.24925823299999567 \t 0.16666666666666666\n",
            "i 0.24925823399999567 \t 0.16666666666666666\n",
            "i 0.24925823499999566 \t 0.16666666666666666\n",
            "i 0.24925823599999566 \t 0.16666666666666666\n",
            "i 0.24925823699999566 \t 0.16666666666666666\n",
            "i 0.24925823799999566 \t 0.16666666666666666\n",
            "i 0.24925823899999566 \t 0.16666666666666666\n",
            "i 0.24925823999999566 \t 0.16666666666666666\n",
            "i 0.24925824099999566 \t 0.16666666666666666\n",
            "i 0.24925824199999566 \t 0.16666666666666666\n",
            "i 0.24925824299999566 \t 0.16666666666666666\n",
            "i 0.24925824399999566 \t 0.16666666666666666\n",
            "i 0.24925824499999566 \t 0.16666666666666666\n",
            "i 0.24925824599999566 \t 0.16666666666666666\n",
            "i 0.24925824699999566 \t 0.16666666666666666\n",
            "i 0.24925824799999566 \t 0.16666666666666666\n",
            "i 0.24925824899999566 \t 0.16666666666666666\n",
            "i 0.24925824999999566 \t 0.16666666666666666\n",
            "i 0.24925825099999566 \t 0.16666666666666666\n",
            "i 0.24925825199999566 \t 0.16666666666666666\n",
            "i 0.24925825299999566 \t 0.16666666666666666\n",
            "i 0.24925825399999565 \t 0.16666666666666666\n",
            "i 0.24925825499999565 \t 0.16666666666666666\n",
            "i 0.24925825599999565 \t 0.16666666666666666\n",
            "i 0.24925825699999565 \t 0.16666666666666666\n",
            "i 0.24925825799999565 \t 0.16666666666666666\n",
            "i 0.24925825899999565 \t 0.16666666666666666\n",
            "i 0.24925825999999565 \t 0.16666666666666666\n",
            "i 0.24925826099999565 \t 0.16666666666666666\n",
            "i 0.24925826199999565 \t 0.16666666666666666\n",
            "i 0.24925826299999565 \t 0.16666666666666666\n",
            "i 0.24925826399999565 \t 0.16666666666666666\n",
            "i 0.24925826499999565 \t 0.16666666666666666\n",
            "i 0.24925826599999565 \t 0.16666666666666666\n",
            "i 0.24925826699999565 \t 0.16666666666666666\n",
            "i 0.24925826799999565 \t 0.16666666666666666\n",
            "i 0.24925826899999565 \t 0.16666666666666666\n",
            "i 0.24925826999999565 \t 0.16666666666666666\n",
            "i 0.24925827099999565 \t 0.16666666666666666\n",
            "i 0.24925827199999565 \t 0.16666666666666666\n",
            "i 0.24925827299999564 \t 0.16666666666666666\n",
            "i 0.24925827399999564 \t 0.16666666666666666\n",
            "i 0.24925827499999564 \t 0.16666666666666666\n",
            "i 0.24925827599999564 \t 0.16666666666666666\n",
            "i 0.24925827699999564 \t 0.16666666666666666\n",
            "i 0.24925827799999564 \t 0.16666666666666666\n",
            "i 0.24925827899999564 \t 0.16666666666666666\n",
            "i 0.24925827999999564 \t 0.16666666666666666\n",
            "i 0.24925828099999564 \t 0.16666666666666666\n",
            "i 0.24925828199999564 \t 0.16666666666666666\n",
            "i 0.24925828299999564 \t 0.16666666666666666\n",
            "i 0.24925828399999564 \t 0.16666666666666666\n",
            "i 0.24925828499999564 \t 0.16666666666666666\n",
            "i 0.24925828599999564 \t 0.16666666666666666\n",
            "i 0.24925828699999564 \t 0.16666666666666666\n",
            "i 0.24925828799999564 \t 0.16666666666666666\n",
            "i 0.24925828899999564 \t 0.16666666666666666\n",
            "i 0.24925828999999564 \t 0.16666666666666666\n",
            "i 0.24925829099999564 \t 0.16666666666666666\n",
            "i 0.24925829199999563 \t 0.16666666666666666\n",
            "i 0.24925829299999563 \t 0.16666666666666666\n",
            "i 0.24925829399999563 \t 0.16666666666666666\n",
            "i 0.24925829499999563 \t 0.16666666666666666\n",
            "i 0.24925829599999563 \t 0.16666666666666666\n",
            "i 0.24925829699999563 \t 0.16666666666666666\n",
            "i 0.24925829799999563 \t 0.16666666666666666\n",
            "i 0.24925829899999563 \t 0.16666666666666666\n",
            "i 0.24925829999999563 \t 0.16666666666666666\n",
            "i 0.24925830099999563 \t 0.16666666666666666\n",
            "i 0.24925830199999563 \t 0.16666666666666666\n",
            "i 0.24925830299999563 \t 0.16666666666666666\n",
            "i 0.24925830399999563 \t 0.16666666666666666\n",
            "i 0.24925830499999563 \t 0.16666666666666666\n",
            "i 0.24925830599999563 \t 0.16666666666666666\n",
            "i 0.24925830699999563 \t 0.16666666666666666\n",
            "i 0.24925830799999563 \t 0.16666666666666666\n",
            "i 0.24925830899999563 \t 0.16666666666666666\n",
            "i 0.24925830999999563 \t 0.16666666666666666\n",
            "i 0.24925831099999562 \t 0.16666666666666666\n",
            "i 0.24925831199999562 \t 0.16666666666666666\n",
            "i 0.24925831299999562 \t 0.16666666666666666\n",
            "i 0.24925831399999562 \t 0.16666666666666666\n",
            "i 0.24925831499999562 \t 0.16666666666666666\n",
            "i 0.24925831599999562 \t 0.16666666666666666\n",
            "i 0.24925831699999562 \t 0.16666666666666666\n",
            "i 0.24925831799999562 \t 0.16666666666666666\n",
            "i 0.24925831899999562 \t 0.16666666666666666\n",
            "i 0.24925831999999562 \t 0.16666666666666666\n",
            "i 0.24925832099999562 \t 0.16666666666666666\n",
            "i 0.24925832199999562 \t 0.16666666666666666\n",
            "i 0.24925832299999562 \t 0.16666666666666666\n",
            "i 0.24925832399999562 \t 0.16666666666666666\n",
            "i 0.24925832499999562 \t 0.16666666666666666\n",
            "i 0.24925832599999562 \t 0.16666666666666666\n",
            "i 0.24925832699999562 \t 0.16666666666666666\n",
            "i 0.24925832799999562 \t 0.16666666666666666\n",
            "i 0.24925832899999562 \t 0.16666666666666666\n",
            "i 0.24925832999999561 \t 0.16666666666666666\n",
            "i 0.24925833099999561 \t 0.16666666666666666\n",
            "i 0.2492583319999956 \t 0.16666666666666666\n",
            "i 0.2492583329999956 \t 0.16666666666666666\n",
            "i 0.2492583339999956 \t 0.16666666666666666\n",
            "i 0.2492583349999956 \t 0.16666666666666666\n",
            "i 0.2492583359999956 \t 0.16666666666666666\n",
            "i 0.2492583369999956 \t 0.16666666666666666\n",
            "i 0.2492583379999956 \t 0.16666666666666666\n",
            "i 0.2492583389999956 \t 0.16666666666666666\n",
            "i 0.2492583399999956 \t 0.16666666666666666\n",
            "i 0.2492583409999956 \t 0.16666666666666666\n",
            "i 0.2492583419999956 \t 0.16666666666666666\n",
            "i 0.2492583429999956 \t 0.16666666666666666\n",
            "i 0.2492583439999956 \t 0.16666666666666666\n",
            "i 0.2492583449999956 \t 0.16666666666666666\n",
            "i 0.2492583459999956 \t 0.16666666666666666\n",
            "i 0.2492583469999956 \t 0.16666666666666666\n",
            "i 0.2492583479999956 \t 0.16666666666666666\n",
            "i 0.2492583489999956 \t 0.16666666666666666\n",
            "i 0.2492583499999956 \t 0.16666666666666666\n",
            "i 0.2492583509999956 \t 0.16666666666666666\n",
            "i 0.2492583519999956 \t 0.16666666666666666\n",
            "i 0.2492583529999956 \t 0.16666666666666666\n",
            "i 0.2492583539999956 \t 0.16666666666666666\n",
            "i 0.2492583549999956 \t 0.16666666666666666\n",
            "i 0.2492583559999956 \t 0.16666666666666666\n",
            "i 0.2492583569999956 \t 0.16666666666666666\n",
            "i 0.2492583579999956 \t 0.16666666666666666\n",
            "i 0.2492583589999956 \t 0.16666666666666666\n",
            "i 0.2492583599999956 \t 0.16666666666666666\n",
            "i 0.2492583609999956 \t 0.16666666666666666\n",
            "i 0.2492583619999956 \t 0.16666666666666666\n",
            "i 0.2492583629999956 \t 0.16666666666666666\n",
            "i 0.2492583639999956 \t 0.16666666666666666\n",
            "i 0.2492583649999956 \t 0.16666666666666666\n",
            "i 0.2492583659999956 \t 0.16666666666666666\n",
            "i 0.2492583669999956 \t 0.16666666666666666\n",
            "i 0.2492583679999956 \t 0.16666666666666666\n",
            "i 0.2492583689999956 \t 0.16666666666666666\n",
            "i 0.2492583699999956 \t 0.16666666666666666\n",
            "i 0.2492583709999956 \t 0.16666666666666666\n",
            "i 0.2492583719999956 \t 0.16666666666666666\n",
            "i 0.2492583729999956 \t 0.16666666666666666\n",
            "i 0.2492583739999956 \t 0.16666666666666666\n",
            "i 0.2492583749999956 \t 0.16666666666666666\n",
            "i 0.2492583759999956 \t 0.16666666666666666\n",
            "i 0.2492583769999956 \t 0.16666666666666666\n",
            "i 0.2492583779999956 \t 0.16666666666666666\n",
            "i 0.2492583789999956 \t 0.16666666666666666\n",
            "i 0.2492583799999956 \t 0.16666666666666666\n",
            "i 0.2492583809999956 \t 0.16666666666666666\n",
            "i 0.2492583819999956 \t 0.16666666666666666\n",
            "i 0.2492583829999956 \t 0.16666666666666666\n",
            "i 0.2492583839999956 \t 0.16666666666666666\n",
            "i 0.24925838499999559 \t 0.16666666666666666\n",
            "i 0.24925838599999559 \t 0.16666666666666666\n",
            "i 0.24925838699999558 \t 0.16666666666666666\n",
            "i 0.24925838799999558 \t 0.16666666666666666\n",
            "i 0.24925838899999558 \t 0.16666666666666666\n",
            "i 0.24925838999999558 \t 0.16666666666666666\n",
            "i 0.24925839099999558 \t 0.16666666666666666\n",
            "i 0.24925839199999558 \t 0.16666666666666666\n",
            "i 0.24925839299999558 \t 0.16666666666666666\n",
            "i 0.24925839399999558 \t 0.16666666666666666\n",
            "i 0.24925839499999558 \t 0.16666666666666666\n",
            "i 0.24925839599999558 \t 0.16666666666666666\n",
            "i 0.24925839699999558 \t 0.16666666666666666\n",
            "i 0.24925839799999558 \t 0.16666666666666666\n",
            "i 0.24925839899999558 \t 0.16666666666666666\n",
            "i 0.24925839999999558 \t 0.16666666666666666\n",
            "i 0.24925840099999558 \t 0.16666666666666666\n",
            "i 0.24925840199999558 \t 0.16666666666666666\n",
            "i 0.24925840299999558 \t 0.16666666666666666\n",
            "i 0.24925840399999558 \t 0.16666666666666666\n",
            "i 0.24925840499999558 \t 0.16666666666666666\n",
            "i 0.24925840599999557 \t 0.16666666666666666\n",
            "i 0.24925840699999557 \t 0.16666666666666666\n",
            "i 0.24925840799999557 \t 0.16666666666666666\n",
            "i 0.24925840899999557 \t 0.16666666666666666\n",
            "i 0.24925840999999557 \t 0.16666666666666666\n",
            "i 0.24925841099999557 \t 0.16666666666666666\n",
            "i 0.24925841199999557 \t 0.16666666666666666\n",
            "i 0.24925841299999557 \t 0.16666666666666666\n",
            "i 0.24925841399999557 \t 0.16666666666666666\n",
            "i 0.24925841499999557 \t 0.16666666666666666\n",
            "i 0.24925841599999557 \t 0.16666666666666666\n",
            "i 0.24925841699999557 \t 0.16666666666666666\n",
            "i 0.24925841799999557 \t 0.16666666666666666\n",
            "i 0.24925841899999557 \t 0.16666666666666666\n",
            "i 0.24925841999999557 \t 0.16666666666666666\n",
            "i 0.24925842099999557 \t 0.16666666666666666\n",
            "i 0.24925842199999557 \t 0.16666666666666666\n",
            "i 0.24925842299999557 \t 0.16666666666666666\n",
            "i 0.24925842399999557 \t 0.16666666666666666\n",
            "i 0.24925842499999556 \t 0.16666666666666666\n",
            "i 0.24925842599999556 \t 0.16666666666666666\n",
            "i 0.24925842699999556 \t 0.16666666666666666\n",
            "i 0.24925842799999556 \t 0.16666666666666666\n",
            "i 0.24925842899999556 \t 0.16666666666666666\n",
            "i 0.24925842999999556 \t 0.16666666666666666\n",
            "i 0.24925843099999556 \t 0.16666666666666666\n",
            "i 0.24925843199999556 \t 0.16666666666666666\n",
            "i 0.24925843299999556 \t 0.16666666666666666\n",
            "i 0.24925843399999556 \t 0.16666666666666666\n",
            "i 0.24925843499999556 \t 0.16666666666666666\n",
            "i 0.24925843599999556 \t 0.16666666666666666\n",
            "i 0.24925843699999556 \t 0.16666666666666666\n",
            "i 0.24925843799999556 \t 0.16666666666666666\n",
            "i 0.24925843899999556 \t 0.16666666666666666\n",
            "i 0.24925843999999556 \t 0.16666666666666666\n",
            "i 0.24925844099999556 \t 0.16666666666666666\n",
            "i 0.24925844199999556 \t 0.16666666666666666\n",
            "i 0.24925844299999556 \t 0.16666666666666666\n",
            "i 0.24925844399999555 \t 0.16666666666666666\n",
            "i 0.24925844499999555 \t 0.16666666666666666\n",
            "i 0.24925844599999555 \t 0.16666666666666666\n",
            "i 0.24925844699999555 \t 0.16666666666666666\n",
            "i 0.24925844799999555 \t 0.16666666666666666\n",
            "i 0.24925844899999555 \t 0.16666666666666666\n",
            "i 0.24925844999999555 \t 0.16666666666666666\n",
            "i 0.24925845099999555 \t 0.16666666666666666\n",
            "i 0.24925845199999555 \t 0.16666666666666666\n",
            "i 0.24925845299999555 \t 0.16666666666666666\n",
            "i 0.24925845399999555 \t 0.16666666666666666\n",
            "i 0.24925845499999555 \t 0.16666666666666666\n",
            "i 0.24925845599999555 \t 0.16666666666666666\n",
            "i 0.24925845699999555 \t 0.16666666666666666\n",
            "i 0.24925845799999555 \t 0.16666666666666666\n",
            "i 0.24925845899999555 \t 0.16666666666666666\n",
            "i 0.24925845999999555 \t 0.16666666666666666\n",
            "i 0.24925846099999555 \t 0.16666666666666666\n",
            "i 0.24925846199999555 \t 0.16666666666666666\n",
            "i 0.24925846299999554 \t 0.16666666666666666\n",
            "i 0.24925846399999554 \t 0.16666666666666666\n",
            "i 0.24925846499999554 \t 0.16666666666666666\n",
            "i 0.24925846599999554 \t 0.16666666666666666\n",
            "i 0.24925846699999554 \t 0.16666666666666666\n",
            "i 0.24925846799999554 \t 0.16666666666666666\n",
            "i 0.24925846899999554 \t 0.16666666666666666\n",
            "i 0.24925846999999554 \t 0.16666666666666666\n",
            "i 0.24925847099999554 \t 0.16666666666666666\n",
            "i 0.24925847199999554 \t 0.16666666666666666\n",
            "i 0.24925847299999554 \t 0.16666666666666666\n",
            "i 0.24925847399999554 \t 0.16666666666666666\n",
            "i 0.24925847499999554 \t 0.16666666666666666\n",
            "i 0.24925847599999554 \t 0.16666666666666666\n",
            "i 0.24925847699999554 \t 0.16666666666666666\n",
            "i 0.24925847799999554 \t 0.16666666666666666\n",
            "i 0.24925847899999554 \t 0.16666666666666666\n",
            "i 0.24925847999999554 \t 0.16666666666666666\n",
            "i 0.24925848099999554 \t 0.16666666666666666\n",
            "i 0.24925848199999553 \t 0.16666666666666666\n",
            "i 0.24925848299999553 \t 0.16666666666666666\n",
            "i 0.24925848399999553 \t 0.16666666666666666\n",
            "i 0.24925848499999553 \t 0.16666666666666666\n",
            "i 0.24925848599999553 \t 0.16666666666666666\n",
            "i 0.24925848699999553 \t 0.16666666666666666\n",
            "i 0.24925848799999553 \t 0.16666666666666666\n",
            "i 0.24925848899999553 \t 0.16666666666666666\n",
            "i 0.24925848999999553 \t 0.16666666666666666\n",
            "i 0.24925849099999553 \t 0.16666666666666666\n",
            "i 0.24925849199999553 \t 0.16666666666666666\n",
            "i 0.24925849299999553 \t 0.16666666666666666\n",
            "i 0.24925849399999553 \t 0.16666666666666666\n",
            "i 0.24925849499999553 \t 0.16666666666666666\n",
            "i 0.24925849599999553 \t 0.16666666666666666\n",
            "i 0.24925849699999553 \t 0.16666666666666666\n",
            "i 0.24925849799999553 \t 0.16666666666666666\n",
            "i 0.24925849899999553 \t 0.16666666666666666\n",
            "i 0.24925849999999553 \t 0.16666666666666666\n",
            "i 0.24925850099999552 \t 0.16666666666666666\n",
            "i 0.24925850199999552 \t 0.16666666666666666\n",
            "i 0.24925850299999552 \t 0.16666666666666666\n",
            "i 0.24925850399999552 \t 0.16666666666666666\n",
            "i 0.24925850499999552 \t 0.16666666666666666\n",
            "i 0.24925850599999552 \t 0.16666666666666666\n",
            "i 0.24925850699999552 \t 0.16666666666666666\n",
            "i 0.24925850799999552 \t 0.16666666666666666\n",
            "i 0.24925850899999552 \t 0.16666666666666666\n",
            "i 0.24925850999999552 \t 0.16666666666666666\n",
            "i 0.24925851099999552 \t 0.16666666666666666\n",
            "i 0.24925851199999552 \t 0.16666666666666666\n",
            "i 0.24925851299999552 \t 0.16666666666666666\n",
            "i 0.24925851399999552 \t 0.16666666666666666\n",
            "i 0.24925851499999552 \t 0.16666666666666666\n",
            "i 0.24925851599999552 \t 0.16666666666666666\n",
            "i 0.24925851699999552 \t 0.16666666666666666\n",
            "i 0.24925851799999552 \t 0.16666666666666666\n",
            "i 0.24925851899999552 \t 0.16666666666666666\n",
            "i 0.24925851999999551 \t 0.16666666666666666\n",
            "i 0.24925852099999551 \t 0.16666666666666666\n",
            "i 0.2492585219999955 \t 0.16666666666666666\n",
            "i 0.2492585229999955 \t 0.16666666666666666\n",
            "i 0.2492585239999955 \t 0.16666666666666666\n",
            "i 0.2492585249999955 \t 0.16666666666666666\n",
            "i 0.2492585259999955 \t 0.16666666666666666\n",
            "i 0.2492585269999955 \t 0.16666666666666666\n",
            "i 0.2492585279999955 \t 0.16666666666666666\n",
            "i 0.2492585289999955 \t 0.16666666666666666\n",
            "i 0.2492585299999955 \t 0.16666666666666666\n",
            "i 0.2492585309999955 \t 0.16666666666666666\n",
            "i 0.2492585319999955 \t 0.16666666666666666\n",
            "i 0.2492585329999955 \t 0.16666666666666666\n",
            "i 0.2492585339999955 \t 0.16666666666666666\n",
            "i 0.2492585349999955 \t 0.16666666666666666\n",
            "i 0.2492585359999955 \t 0.16666666666666666\n",
            "i 0.2492585369999955 \t 0.16666666666666666\n",
            "i 0.2492585379999955 \t 0.16666666666666666\n",
            "i 0.2492585389999955 \t 0.16666666666666666\n",
            "i 0.2492585399999955 \t 0.16666666666666666\n",
            "i 0.2492585409999955 \t 0.16666666666666666\n",
            "i 0.2492585419999955 \t 0.16666666666666666\n",
            "i 0.2492585429999955 \t 0.16666666666666666\n",
            "i 0.2492585439999955 \t 0.16666666666666666\n",
            "i 0.2492585449999955 \t 0.16666666666666666\n",
            "i 0.2492585459999955 \t 0.16666666666666666\n",
            "i 0.2492585469999955 \t 0.16666666666666666\n",
            "i 0.2492585479999955 \t 0.16666666666666666\n",
            "i 0.2492585489999955 \t 0.16666666666666666\n",
            "i 0.2492585499999955 \t 0.16666666666666666\n",
            "i 0.2492585509999955 \t 0.16666666666666666\n",
            "i 0.2492585519999955 \t 0.16666666666666666\n",
            "i 0.2492585529999955 \t 0.16666666666666666\n",
            "i 0.2492585539999955 \t 0.16666666666666666\n",
            "i 0.2492585549999955 \t 0.16666666666666666\n",
            "i 0.2492585559999955 \t 0.16666666666666666\n",
            "i 0.2492585569999955 \t 0.16666666666666666\n",
            "i 0.2492585579999955 \t 0.16666666666666666\n",
            "i 0.2492585589999955 \t 0.16666666666666666\n",
            "i 0.2492585599999955 \t 0.16666666666666666\n",
            "i 0.2492585609999955 \t 0.16666666666666666\n",
            "i 0.2492585619999955 \t 0.16666666666666666\n",
            "i 0.2492585629999955 \t 0.16666666666666666\n",
            "i 0.2492585639999955 \t 0.16666666666666666\n",
            "i 0.2492585649999955 \t 0.16666666666666666\n",
            "i 0.2492585659999955 \t 0.16666666666666666\n",
            "i 0.2492585669999955 \t 0.16666666666666666\n",
            "i 0.2492585679999955 \t 0.16666666666666666\n",
            "i 0.2492585689999955 \t 0.16666666666666666\n",
            "i 0.2492585699999955 \t 0.16666666666666666\n",
            "i 0.2492585709999955 \t 0.16666666666666666\n",
            "i 0.2492585719999955 \t 0.16666666666666666\n",
            "i 0.2492585729999955 \t 0.16666666666666666\n",
            "i 0.2492585739999955 \t 0.16666666666666666\n",
            "i 0.24925857499999549 \t 0.16666666666666666\n",
            "i 0.24925857599999549 \t 0.16666666666666666\n",
            "i 0.24925857699999548 \t 0.16666666666666666\n",
            "i 0.24925857799999548 \t 0.16666666666666666\n",
            "i 0.24925857899999548 \t 0.16666666666666666\n",
            "i 0.24925857999999548 \t 0.16666666666666666\n",
            "i 0.24925858099999548 \t 0.16666666666666666\n",
            "i 0.24925858199999548 \t 0.16666666666666666\n",
            "i 0.24925858299999548 \t 0.16666666666666666\n",
            "i 0.24925858399999548 \t 0.16666666666666666\n",
            "i 0.24925858499999548 \t 0.16666666666666666\n",
            "i 0.24925858599999548 \t 0.16666666666666666\n",
            "i 0.24925858699999548 \t 0.16666666666666666\n",
            "i 0.24925858799999548 \t 0.16666666666666666\n",
            "i 0.24925858899999548 \t 0.16666666666666666\n",
            "i 0.24925858999999548 \t 0.16666666666666666\n",
            "i 0.24925859099999548 \t 0.16666666666666666\n",
            "i 0.24925859199999548 \t 0.16666666666666666\n",
            "i 0.24925859299999548 \t 0.16666666666666666\n",
            "i 0.24925859399999548 \t 0.16666666666666666\n",
            "i 0.24925859499999548 \t 0.16666666666666666\n",
            "i 0.24925859599999547 \t 0.16666666666666666\n",
            "i 0.24925859699999547 \t 0.16666666666666666\n",
            "i 0.24925859799999547 \t 0.16666666666666666\n",
            "i 0.24925859899999547 \t 0.16666666666666666\n",
            "i 0.24925859999999547 \t 0.16666666666666666\n",
            "i 0.24925860099999547 \t 0.16666666666666666\n",
            "i 0.24925860199999547 \t 0.16666666666666666\n",
            "i 0.24925860299999547 \t 0.16666666666666666\n",
            "i 0.24925860399999547 \t 0.16666666666666666\n",
            "i 0.24925860499999547 \t 0.16666666666666666\n",
            "i 0.24925860599999547 \t 0.16666666666666666\n",
            "i 0.24925860699999547 \t 0.16666666666666666\n",
            "i 0.24925860799999547 \t 0.16666666666666666\n",
            "i 0.24925860899999547 \t 0.16666666666666666\n",
            "i 0.24925860999999547 \t 0.16666666666666666\n",
            "i 0.24925861099999547 \t 0.16666666666666666\n",
            "i 0.24925861199999547 \t 0.16666666666666666\n",
            "i 0.24925861299999547 \t 0.16666666666666666\n",
            "i 0.24925861399999547 \t 0.16666666666666666\n",
            "i 0.24925861499999546 \t 0.16666666666666666\n",
            "i 0.24925861599999546 \t 0.16666666666666666\n",
            "i 0.24925861699999546 \t 0.16666666666666666\n",
            "i 0.24925861799999546 \t 0.16666666666666666\n",
            "i 0.24925861899999546 \t 0.16666666666666666\n",
            "i 0.24925861999999546 \t 0.16666666666666666\n",
            "i 0.24925862099999546 \t 0.16666666666666666\n",
            "i 0.24925862199999546 \t 0.16666666666666666\n",
            "i 0.24925862299999546 \t 0.16666666666666666\n",
            "i 0.24925862399999546 \t 0.16666666666666666\n",
            "i 0.24925862499999546 \t 0.16666666666666666\n",
            "i 0.24925862599999546 \t 0.16666666666666666\n",
            "i 0.24925862699999546 \t 0.16666666666666666\n",
            "i 0.24925862799999546 \t 0.16666666666666666\n",
            "i 0.24925862899999546 \t 0.16666666666666666\n",
            "i 0.24925862999999546 \t 0.16666666666666666\n",
            "i 0.24925863099999546 \t 0.16666666666666666\n",
            "i 0.24925863199999546 \t 0.16666666666666666\n",
            "i 0.24925863299999546 \t 0.16666666666666666\n",
            "i 0.24925863399999545 \t 0.16666666666666666\n",
            "i 0.24925863499999545 \t 0.16666666666666666\n",
            "i 0.24925863599999545 \t 0.16666666666666666\n",
            "i 0.24925863699999545 \t 0.16666666666666666\n",
            "i 0.24925863799999545 \t 0.16666666666666666\n",
            "i 0.24925863899999545 \t 0.16666666666666666\n",
            "i 0.24925863999999545 \t 0.16666666666666666\n",
            "i 0.24925864099999545 \t 0.16666666666666666\n",
            "i 0.24925864199999545 \t 0.16666666666666666\n",
            "i 0.24925864299999545 \t 0.16666666666666666\n",
            "i 0.24925864399999545 \t 0.16666666666666666\n",
            "i 0.24925864499999545 \t 0.16666666666666666\n",
            "i 0.24925864599999545 \t 0.16666666666666666\n",
            "i 0.24925864699999545 \t 0.16666666666666666\n",
            "i 0.24925864799999545 \t 0.16666666666666666\n",
            "i 0.24925864899999545 \t 0.16666666666666666\n",
            "i 0.24925864999999545 \t 0.16666666666666666\n",
            "i 0.24925865099999545 \t 0.16666666666666666\n",
            "i 0.24925865199999545 \t 0.16666666666666666\n",
            "i 0.24925865299999544 \t 0.16666666666666666\n",
            "i 0.24925865399999544 \t 0.16666666666666666\n",
            "i 0.24925865499999544 \t 0.16666666666666666\n",
            "i 0.24925865599999544 \t 0.16666666666666666\n",
            "i 0.24925865699999544 \t 0.16666666666666666\n",
            "i 0.24925865799999544 \t 0.16666666666666666\n",
            "i 0.24925865899999544 \t 0.16666666666666666\n",
            "i 0.24925865999999544 \t 0.16666666666666666\n",
            "i 0.24925866099999544 \t 0.16666666666666666\n",
            "i 0.24925866199999544 \t 0.16666666666666666\n",
            "i 0.24925866299999544 \t 0.16666666666666666\n",
            "i 0.24925866399999544 \t 0.16666666666666666\n",
            "i 0.24925866499999544 \t 0.16666666666666666\n",
            "i 0.24925866599999544 \t 0.16666666666666666\n",
            "i 0.24925866699999544 \t 0.16666666666666666\n",
            "i 0.24925866799999544 \t 0.16666666666666666\n",
            "i 0.24925866899999544 \t 0.16666666666666666\n",
            "i 0.24925866999999544 \t 0.16666666666666666\n",
            "i 0.24925867099999544 \t 0.16666666666666666\n",
            "i 0.24925867199999543 \t 0.16666666666666666\n",
            "i 0.24925867299999543 \t 0.16666666666666666\n",
            "i 0.24925867399999543 \t 0.16666666666666666\n",
            "i 0.24925867499999543 \t 0.16666666666666666\n",
            "i 0.24925867599999543 \t 0.16666666666666666\n",
            "i 0.24925867699999543 \t 0.16666666666666666\n",
            "i 0.24925867799999543 \t 0.16666666666666666\n",
            "i 0.24925867899999543 \t 0.16666666666666666\n",
            "i 0.24925867999999543 \t 0.16666666666666666\n",
            "i 0.24925868099999543 \t 0.16666666666666666\n",
            "i 0.24925868199999543 \t 0.16666666666666666\n",
            "i 0.24925868299999543 \t 0.16666666666666666\n",
            "i 0.24925868399999543 \t 0.16666666666666666\n",
            "i 0.24925868499999543 \t 0.16666666666666666\n",
            "i 0.24925868599999543 \t 0.16666666666666666\n",
            "i 0.24925868699999543 \t 0.16666666666666666\n",
            "i 0.24925868799999543 \t 0.16666666666666666\n",
            "i 0.24925868899999543 \t 0.16666666666666666\n",
            "i 0.24925868999999543 \t 0.16666666666666666\n",
            "i 0.24925869099999542 \t 0.16666666666666666\n",
            "i 0.24925869199999542 \t 0.16666666666666666\n",
            "i 0.24925869299999542 \t 0.16666666666666666\n",
            "i 0.24925869399999542 \t 0.16666666666666666\n",
            "i 0.24925869499999542 \t 0.16666666666666666\n",
            "i 0.24925869599999542 \t 0.16666666666666666\n",
            "i 0.24925869699999542 \t 0.16666666666666666\n",
            "i 0.24925869799999542 \t 0.16666666666666666\n",
            "i 0.24925869899999542 \t 0.16666666666666666\n",
            "i 0.24925869999999542 \t 0.16666666666666666\n",
            "i 0.24925870099999542 \t 0.16666666666666666\n",
            "i 0.24925870199999542 \t 0.16666666666666666\n",
            "i 0.24925870299999542 \t 0.16666666666666666\n",
            "i 0.24925870399999542 \t 0.16666666666666666\n",
            "i 0.24925870499999542 \t 0.16666666666666666\n",
            "i 0.24925870599999542 \t 0.16666666666666666\n",
            "i 0.24925870699999542 \t 0.16666666666666666\n",
            "i 0.24925870799999542 \t 0.16666666666666666\n",
            "i 0.24925870899999542 \t 0.16666666666666666\n",
            "i 0.24925870999999541 \t 0.16666666666666666\n",
            "i 0.24925871099999541 \t 0.16666666666666666\n",
            "i 0.2492587119999954 \t 0.16666666666666666\n",
            "i 0.2492587129999954 \t 0.16666666666666666\n",
            "i 0.2492587139999954 \t 0.16666666666666666\n",
            "i 0.2492587149999954 \t 0.16666666666666666\n",
            "i 0.2492587159999954 \t 0.16666666666666666\n",
            "i 0.2492587169999954 \t 0.16666666666666666\n",
            "i 0.2492587179999954 \t 0.16666666666666666\n",
            "i 0.2492587189999954 \t 0.16666666666666666\n",
            "i 0.2492587199999954 \t 0.16666666666666666\n",
            "i 0.2492587209999954 \t 0.16666666666666666\n",
            "i 0.2492587219999954 \t 0.16666666666666666\n",
            "i 0.2492587229999954 \t 0.16666666666666666\n",
            "i 0.2492587239999954 \t 0.16666666666666666\n",
            "i 0.2492587249999954 \t 0.16666666666666666\n",
            "i 0.2492587259999954 \t 0.16666666666666666\n",
            "i 0.2492587269999954 \t 0.16666666666666666\n",
            "i 0.2492587279999954 \t 0.16666666666666666\n",
            "i 0.2492587289999954 \t 0.16666666666666666\n",
            "i 0.2492587299999954 \t 0.16666666666666666\n",
            "i 0.2492587309999954 \t 0.16666666666666666\n",
            "i 0.2492587319999954 \t 0.16666666666666666\n",
            "i 0.2492587329999954 \t 0.16666666666666666\n",
            "i 0.2492587339999954 \t 0.16666666666666666\n",
            "i 0.2492587349999954 \t 0.16666666666666666\n",
            "i 0.2492587359999954 \t 0.16666666666666666\n",
            "i 0.2492587369999954 \t 0.16666666666666666\n",
            "i 0.2492587379999954 \t 0.16666666666666666\n",
            "i 0.2492587389999954 \t 0.16666666666666666\n",
            "i 0.2492587399999954 \t 0.16666666666666666\n",
            "i 0.2492587409999954 \t 0.16666666666666666\n",
            "i 0.2492587419999954 \t 0.16666666666666666\n",
            "i 0.2492587429999954 \t 0.16666666666666666\n",
            "i 0.2492587439999954 \t 0.16666666666666666\n",
            "i 0.2492587449999954 \t 0.16666666666666666\n",
            "i 0.2492587459999954 \t 0.16666666666666666\n",
            "i 0.2492587469999954 \t 0.16666666666666666\n",
            "i 0.2492587479999954 \t 0.16666666666666666\n",
            "i 0.2492587489999954 \t 0.16666666666666666\n",
            "i 0.2492587499999954 \t 0.16666666666666666\n",
            "i 0.2492587509999954 \t 0.16666666666666666\n",
            "i 0.2492587519999954 \t 0.16666666666666666\n",
            "i 0.2492587529999954 \t 0.16666666666666666\n",
            "i 0.2492587539999954 \t 0.16666666666666666\n",
            "i 0.2492587549999954 \t 0.16666666666666666\n",
            "i 0.2492587559999954 \t 0.16666666666666666\n",
            "i 0.2492587569999954 \t 0.16666666666666666\n",
            "i 0.2492587579999954 \t 0.16666666666666666\n",
            "i 0.2492587589999954 \t 0.16666666666666666\n",
            "i 0.2492587599999954 \t 0.16666666666666666\n",
            "i 0.2492587609999954 \t 0.16666666666666666\n",
            "i 0.2492587619999954 \t 0.16666666666666666\n",
            "i 0.2492587629999954 \t 0.16666666666666666\n",
            "i 0.2492587639999954 \t 0.16666666666666666\n",
            "i 0.24925876499999539 \t 0.16666666666666666\n",
            "i 0.24925876599999539 \t 0.16666666666666666\n",
            "i 0.24925876699999538 \t 0.16666666666666666\n",
            "i 0.24925876799999538 \t 0.16666666666666666\n",
            "i 0.24925876899999538 \t 0.16666666666666666\n",
            "i 0.24925876999999538 \t 0.16666666666666666\n",
            "i 0.24925877099999538 \t 0.16666666666666666\n",
            "i 0.24925877199999538 \t 0.16666666666666666\n",
            "i 0.24925877299999538 \t 0.16666666666666666\n",
            "i 0.24925877399999538 \t 0.16666666666666666\n",
            "i 0.24925877499999538 \t 0.16666666666666666\n",
            "i 0.24925877599999538 \t 0.16666666666666666\n",
            "i 0.24925877699999538 \t 0.16666666666666666\n",
            "i 0.24925877799999538 \t 0.16666666666666666\n",
            "i 0.24925877899999538 \t 0.16666666666666666\n",
            "i 0.24925877999999538 \t 0.16666666666666666\n",
            "i 0.24925878099999538 \t 0.16666666666666666\n",
            "i 0.24925878199999538 \t 0.16666666666666666\n",
            "i 0.24925878299999538 \t 0.16666666666666666\n",
            "i 0.24925878399999538 \t 0.16666666666666666\n",
            "i 0.24925878499999538 \t 0.16666666666666666\n",
            "i 0.24925878599999537 \t 0.16666666666666666\n",
            "i 0.24925878699999537 \t 0.16666666666666666\n",
            "i 0.24925878799999537 \t 0.16666666666666666\n",
            "i 0.24925878899999537 \t 0.16666666666666666\n",
            "i 0.24925878999999537 \t 0.16666666666666666\n",
            "i 0.24925879099999537 \t 0.16666666666666666\n",
            "i 0.24925879199999537 \t 0.16666666666666666\n",
            "i 0.24925879299999537 \t 0.16666666666666666\n",
            "i 0.24925879399999537 \t 0.16666666666666666\n",
            "i 0.24925879499999537 \t 0.16666666666666666\n",
            "i 0.24925879599999537 \t 0.16666666666666666\n",
            "i 0.24925879699999537 \t 0.16666666666666666\n",
            "i 0.24925879799999537 \t 0.16666666666666666\n",
            "i 0.24925879899999537 \t 0.16666666666666666\n",
            "i 0.24925879999999537 \t 0.16666666666666666\n",
            "i 0.24925880099999537 \t 0.16666666666666666\n",
            "i 0.24925880199999537 \t 0.16666666666666666\n",
            "i 0.24925880299999537 \t 0.16666666666666666\n",
            "i 0.24925880399999537 \t 0.16666666666666666\n",
            "i 0.24925880499999536 \t 0.16666666666666666\n",
            "i 0.24925880599999536 \t 0.16666666666666666\n",
            "i 0.24925880699999536 \t 0.16666666666666666\n",
            "i 0.24925880799999536 \t 0.16666666666666666\n",
            "i 0.24925880899999536 \t 0.16666666666666666\n",
            "i 0.24925880999999536 \t 0.16666666666666666\n",
            "i 0.24925881099999536 \t 0.16666666666666666\n",
            "i 0.24925881199999536 \t 0.16666666666666666\n",
            "i 0.24925881299999536 \t 0.16666666666666666\n",
            "i 0.24925881399999536 \t 0.16666666666666666\n",
            "i 0.24925881499999536 \t 0.16666666666666666\n",
            "i 0.24925881599999536 \t 0.16666666666666666\n",
            "i 0.24925881699999536 \t 0.16666666666666666\n",
            "i 0.24925881799999536 \t 0.16666666666666666\n",
            "i 0.24925881899999536 \t 0.16666666666666666\n",
            "i 0.24925881999999536 \t 0.16666666666666666\n",
            "i 0.24925882099999536 \t 0.16666666666666666\n",
            "i 0.24925882199999536 \t 0.16666666666666666\n",
            "i 0.24925882299999536 \t 0.16666666666666666\n",
            "i 0.24925882399999535 \t 0.16666666666666666\n",
            "i 0.24925882499999535 \t 0.16666666666666666\n",
            "i 0.24925882599999535 \t 0.16666666666666666\n",
            "i 0.24925882699999535 \t 0.16666666666666666\n",
            "i 0.24925882799999535 \t 0.16666666666666666\n",
            "i 0.24925882899999535 \t 0.16666666666666666\n",
            "i 0.24925882999999535 \t 0.16666666666666666\n",
            "i 0.24925883099999535 \t 0.16666666666666666\n",
            "i 0.24925883199999535 \t 0.16666666666666666\n",
            "i 0.24925883299999535 \t 0.16666666666666666\n",
            "i 0.24925883399999535 \t 0.16666666666666666\n",
            "i 0.24925883499999535 \t 0.16666666666666666\n",
            "i 0.24925883599999535 \t 0.16666666666666666\n",
            "i 0.24925883699999535 \t 0.16666666666666666\n",
            "i 0.24925883799999535 \t 0.16666666666666666\n",
            "i 0.24925883899999535 \t 0.16666666666666666\n",
            "i 0.24925883999999535 \t 0.16666666666666666\n",
            "i 0.24925884099999535 \t 0.16666666666666666\n",
            "i 0.24925884199999535 \t 0.16666666666666666\n",
            "i 0.24925884299999534 \t 0.16666666666666666\n",
            "i 0.24925884399999534 \t 0.16666666666666666\n",
            "i 0.24925884499999534 \t 0.16666666666666666\n",
            "i 0.24925884599999534 \t 0.16666666666666666\n",
            "i 0.24925884699999534 \t 0.16666666666666666\n",
            "i 0.24925884799999534 \t 0.16666666666666666\n",
            "i 0.24925884899999534 \t 0.16666666666666666\n",
            "i 0.24925884999999534 \t 0.16666666666666666\n",
            "i 0.24925885099999534 \t 0.16666666666666666\n",
            "i 0.24925885199999534 \t 0.16666666666666666\n",
            "i 0.24925885299999534 \t 0.16666666666666666\n",
            "i 0.24925885399999534 \t 0.16666666666666666\n",
            "i 0.24925885499999534 \t 0.16666666666666666\n",
            "i 0.24925885599999534 \t 0.16666666666666666\n",
            "i 0.24925885699999534 \t 0.16666666666666666\n",
            "i 0.24925885799999534 \t 0.16666666666666666\n",
            "i 0.24925885899999534 \t 0.16666666666666666\n",
            "i 0.24925885999999534 \t 0.16666666666666666\n",
            "i 0.24925886099999534 \t 0.16666666666666666\n",
            "i 0.24925886199999533 \t 0.16666666666666666\n",
            "i 0.24925886299999533 \t 0.16666666666666666\n",
            "i 0.24925886399999533 \t 0.16666666666666666\n",
            "i 0.24925886499999533 \t 0.16666666666666666\n",
            "i 0.24925886599999533 \t 0.16666666666666666\n",
            "i 0.24925886699999533 \t 0.16666666666666666\n",
            "i 0.24925886799999533 \t 0.16666666666666666\n",
            "i 0.24925886899999533 \t 0.16666666666666666\n",
            "i 0.24925886999999533 \t 0.16666666666666666\n",
            "i 0.24925887099999533 \t 0.16666666666666666\n",
            "i 0.24925887199999533 \t 0.16666666666666666\n",
            "i 0.24925887299999533 \t 0.16666666666666666\n",
            "i 0.24925887399999533 \t 0.16666666666666666\n",
            "i 0.24925887499999533 \t 0.16666666666666666\n",
            "i 0.24925887599999533 \t 0.16666666666666666\n",
            "i 0.24925887699999533 \t 0.16666666666666666\n",
            "i 0.24925887799999533 \t 0.16666666666666666\n",
            "i 0.24925887899999533 \t 0.16666666666666666\n",
            "i 0.24925887999999533 \t 0.16666666666666666\n",
            "i 0.24925888099999532 \t 0.16666666666666666\n",
            "i 0.24925888199999532 \t 0.16666666666666666\n",
            "i 0.24925888299999532 \t 0.16666666666666666\n",
            "i 0.24925888399999532 \t 0.16666666666666666\n",
            "i 0.24925888499999532 \t 0.16666666666666666\n",
            "i 0.24925888599999532 \t 0.16666666666666666\n",
            "i 0.24925888699999532 \t 0.16666666666666666\n",
            "i 0.24925888799999532 \t 0.16666666666666666\n",
            "i 0.24925888899999532 \t 0.16666666666666666\n",
            "i 0.24925888999999532 \t 0.16666666666666666\n",
            "i 0.24925889099999532 \t 0.16666666666666666\n",
            "i 0.24925889199999532 \t 0.16666666666666666\n",
            "i 0.24925889299999532 \t 0.16666666666666666\n",
            "i 0.24925889399999532 \t 0.16666666666666666\n",
            "i 0.24925889499999532 \t 0.16666666666666666\n",
            "i 0.24925889599999532 \t 0.16666666666666666\n",
            "i 0.24925889699999532 \t 0.16666666666666666\n",
            "i 0.24925889799999532 \t 0.16666666666666666\n",
            "i 0.24925889899999532 \t 0.16666666666666666\n",
            "i 0.24925889999999531 \t 0.16666666666666666\n",
            "i 0.24925890099999531 \t 0.16666666666666666\n",
            "i 0.2492589019999953 \t 0.16666666666666666\n",
            "i 0.2492589029999953 \t 0.16666666666666666\n",
            "i 0.2492589039999953 \t 0.16666666666666666\n",
            "i 0.2492589049999953 \t 0.16666666666666666\n",
            "i 0.2492589059999953 \t 0.16666666666666666\n",
            "i 0.2492589069999953 \t 0.16666666666666666\n",
            "i 0.2492589079999953 \t 0.16666666666666666\n",
            "i 0.2492589089999953 \t 0.16666666666666666\n",
            "i 0.2492589099999953 \t 0.16666666666666666\n",
            "i 0.2492589109999953 \t 0.16666666666666666\n",
            "i 0.2492589119999953 \t 0.16666666666666666\n",
            "i 0.2492589129999953 \t 0.16666666666666666\n",
            "i 0.2492589139999953 \t 0.16666666666666666\n",
            "i 0.2492589149999953 \t 0.16666666666666666\n",
            "i 0.2492589159999953 \t 0.16666666666666666\n",
            "i 0.2492589169999953 \t 0.16666666666666666\n",
            "i 0.2492589179999953 \t 0.16666666666666666\n",
            "i 0.2492589189999953 \t 0.16666666666666666\n",
            "i 0.2492589199999953 \t 0.16666666666666666\n",
            "i 0.2492589209999953 \t 0.16666666666666666\n",
            "i 0.2492589219999953 \t 0.16666666666666666\n",
            "i 0.2492589229999953 \t 0.16666666666666666\n",
            "i 0.2492589239999953 \t 0.16666666666666666\n",
            "i 0.2492589249999953 \t 0.16666666666666666\n",
            "i 0.2492589259999953 \t 0.16666666666666666\n",
            "i 0.2492589269999953 \t 0.16666666666666666\n",
            "i 0.2492589279999953 \t 0.16666666666666666\n",
            "i 0.2492589289999953 \t 0.16666666666666666\n",
            "i 0.2492589299999953 \t 0.16666666666666666\n",
            "i 0.2492589309999953 \t 0.16666666666666666\n",
            "i 0.2492589319999953 \t 0.16666666666666666\n",
            "i 0.2492589329999953 \t 0.16666666666666666\n",
            "i 0.2492589339999953 \t 0.16666666666666666\n",
            "i 0.2492589349999953 \t 0.16666666666666666\n",
            "i 0.2492589359999953 \t 0.16666666666666666\n",
            "i 0.2492589369999953 \t 0.16666666666666666\n",
            "i 0.2492589379999953 \t 0.16666666666666666\n",
            "i 0.2492589389999953 \t 0.16666666666666666\n",
            "i 0.2492589399999953 \t 0.16666666666666666\n",
            "i 0.2492589409999953 \t 0.16666666666666666\n",
            "i 0.2492589419999953 \t 0.16666666666666666\n",
            "i 0.2492589429999953 \t 0.16666666666666666\n",
            "i 0.2492589439999953 \t 0.16666666666666666\n",
            "i 0.2492589449999953 \t 0.16666666666666666\n",
            "i 0.2492589459999953 \t 0.16666666666666666\n",
            "i 0.2492589469999953 \t 0.16666666666666666\n",
            "i 0.2492589479999953 \t 0.16666666666666666\n",
            "i 0.2492589489999953 \t 0.16666666666666666\n",
            "i 0.2492589499999953 \t 0.16666666666666666\n",
            "i 0.2492589509999953 \t 0.16666666666666666\n",
            "i 0.2492589519999953 \t 0.16666666666666666\n",
            "i 0.2492589529999953 \t 0.16666666666666666\n",
            "i 0.2492589539999953 \t 0.16666666666666666\n",
            "i 0.24925895499999529 \t 0.16666666666666666\n",
            "i 0.24925895599999529 \t 0.16666666666666666\n",
            "i 0.24925895699999528 \t 0.16666666666666666\n",
            "i 0.24925895799999528 \t 0.16666666666666666\n",
            "i 0.24925895899999528 \t 0.16666666666666666\n",
            "i 0.24925895999999528 \t 0.16666666666666666\n",
            "i 0.24925896099999528 \t 0.16666666666666666\n",
            "i 0.24925896199999528 \t 0.16666666666666666\n",
            "i 0.24925896299999528 \t 0.16666666666666666\n",
            "i 0.24925896399999528 \t 0.16666666666666666\n",
            "i 0.24925896499999528 \t 0.16666666666666666\n",
            "i 0.24925896599999528 \t 0.16666666666666666\n",
            "i 0.24925896699999528 \t 0.16666666666666666\n",
            "i 0.24925896799999528 \t 0.16666666666666666\n",
            "i 0.24925896899999528 \t 0.16666666666666666\n",
            "i 0.24925896999999528 \t 0.16666666666666666\n",
            "i 0.24925897099999528 \t 0.16666666666666666\n",
            "i 0.24925897199999528 \t 0.16666666666666666\n",
            "i 0.24925897299999528 \t 0.16666666666666666\n",
            "i 0.24925897399999528 \t 0.16666666666666666\n",
            "i 0.24925897499999528 \t 0.16666666666666666\n",
            "i 0.24925897599999527 \t 0.16666666666666666\n",
            "i 0.24925897699999527 \t 0.16666666666666666\n",
            "i 0.24925897799999527 \t 0.16666666666666666\n",
            "i 0.24925897899999527 \t 0.16666666666666666\n",
            "i 0.24925897999999527 \t 0.16666666666666666\n",
            "i 0.24925898099999527 \t 0.16666666666666666\n",
            "i 0.24925898199999527 \t 0.16666666666666666\n",
            "i 0.24925898299999527 \t 0.16666666666666666\n",
            "i 0.24925898399999527 \t 0.16666666666666666\n",
            "i 0.24925898499999527 \t 0.16666666666666666\n",
            "i 0.24925898599999527 \t 0.16666666666666666\n",
            "i 0.24925898699999527 \t 0.16666666666666666\n",
            "i 0.24925898799999527 \t 0.16666666666666666\n",
            "i 0.24925898899999527 \t 0.16666666666666666\n",
            "i 0.24925898999999527 \t 0.16666666666666666\n",
            "i 0.24925899099999527 \t 0.16666666666666666\n",
            "i 0.24925899199999527 \t 0.16666666666666666\n",
            "i 0.24925899299999527 \t 0.16666666666666666\n",
            "i 0.24925899399999527 \t 0.16666666666666666\n",
            "i 0.24925899499999526 \t 0.16666666666666666\n",
            "i 0.24925899599999526 \t 0.16666666666666666\n",
            "i 0.24925899699999526 \t 0.16666666666666666\n",
            "i 0.24925899799999526 \t 0.16666666666666666\n",
            "i 0.24925899899999526 \t 0.16666666666666666\n",
            "i 0.24925899999999526 \t 0.16666666666666666\n",
            "i 0.24925900099999526 \t 0.16666666666666666\n",
            "i 0.24925900199999526 \t 0.16666666666666666\n",
            "i 0.24925900299999526 \t 0.16666666666666666\n",
            "i 0.24925900399999526 \t 0.16666666666666666\n",
            "i 0.24925900499999526 \t 0.16666666666666666\n",
            "i 0.24925900599999526 \t 0.16666666666666666\n",
            "i 0.24925900699999526 \t 0.16666666666666666\n",
            "i 0.24925900799999526 \t 0.16666666666666666\n",
            "i 0.24925900899999526 \t 0.16666666666666666\n",
            "i 0.24925900999999526 \t 0.16666666666666666\n",
            "i 0.24925901099999526 \t 0.16666666666666666\n",
            "i 0.24925901199999526 \t 0.16666666666666666\n",
            "i 0.24925901299999526 \t 0.16666666666666666\n",
            "i 0.24925901399999525 \t 0.16666666666666666\n",
            "i 0.24925901499999525 \t 0.16666666666666666\n",
            "i 0.24925901599999525 \t 0.16666666666666666\n",
            "i 0.24925901699999525 \t 0.16666666666666666\n",
            "i 0.24925901799999525 \t 0.16666666666666666\n",
            "i 0.24925901899999525 \t 0.16666666666666666\n",
            "i 0.24925901999999525 \t 0.16666666666666666\n",
            "i 0.24925902099999525 \t 0.16666666666666666\n",
            "i 0.24925902199999525 \t 0.16666666666666666\n",
            "i 0.24925902299999525 \t 0.16666666666666666\n",
            "i 0.24925902399999525 \t 0.16666666666666666\n",
            "i 0.24925902499999525 \t 0.16666666666666666\n",
            "i 0.24925902599999525 \t 0.16666666666666666\n",
            "i 0.24925902699999525 \t 0.16666666666666666\n",
            "i 0.24925902799999525 \t 0.16666666666666666\n",
            "i 0.24925902899999525 \t 0.16666666666666666\n",
            "i 0.24925902999999525 \t 0.16666666666666666\n",
            "i 0.24925903099999525 \t 0.16666666666666666\n",
            "i 0.24925903199999525 \t 0.16666666666666666\n",
            "i 0.24925903299999524 \t 0.16666666666666666\n",
            "i 0.24925903399999524 \t 0.16666666666666666\n",
            "i 0.24925903499999524 \t 0.16666666666666666\n",
            "i 0.24925903599999524 \t 0.16666666666666666\n",
            "i 0.24925903699999524 \t 0.16666666666666666\n",
            "i 0.24925903799999524 \t 0.16666666666666666\n",
            "i 0.24925903899999524 \t 0.16666666666666666\n",
            "i 0.24925903999999524 \t 0.16666666666666666\n",
            "i 0.24925904099999524 \t 0.16666666666666666\n",
            "i 0.24925904199999524 \t 0.16666666666666666\n",
            "i 0.24925904299999524 \t 0.16666666666666666\n",
            "i 0.24925904399999524 \t 0.16666666666666666\n",
            "i 0.24925904499999524 \t 0.16666666666666666\n",
            "i 0.24925904599999524 \t 0.16666666666666666\n",
            "i 0.24925904699999524 \t 0.16666666666666666\n",
            "i 0.24925904799999524 \t 0.16666666666666666\n",
            "i 0.24925904899999524 \t 0.16666666666666666\n",
            "i 0.24925904999999524 \t 0.16666666666666666\n",
            "i 0.24925905099999524 \t 0.16666666666666666\n",
            "i 0.24925905199999523 \t 0.16666666666666666\n",
            "i 0.24925905299999523 \t 0.16666666666666666\n",
            "i 0.24925905399999523 \t 0.16666666666666666\n",
            "i 0.24925905499999523 \t 0.16666666666666666\n",
            "i 0.24925905599999523 \t 0.16666666666666666\n",
            "i 0.24925905699999523 \t 0.16666666666666666\n",
            "i 0.24925905799999523 \t 0.16666666666666666\n",
            "i 0.24925905899999523 \t 0.16666666666666666\n",
            "i 0.24925905999999523 \t 0.16666666666666666\n",
            "i 0.24925906099999523 \t 0.16666666666666666\n",
            "i 0.24925906199999523 \t 0.16666666666666666\n",
            "i 0.24925906299999523 \t 0.16666666666666666\n",
            "i 0.24925906399999523 \t 0.16666666666666666\n",
            "i 0.24925906499999523 \t 0.16666666666666666\n",
            "i 0.24925906599999523 \t 0.16666666666666666\n",
            "i 0.24925906699999523 \t 0.16666666666666666\n",
            "i 0.24925906799999523 \t 0.16666666666666666\n",
            "i 0.24925906899999523 \t 0.16666666666666666\n",
            "i 0.24925906999999523 \t 0.16666666666666666\n",
            "i 0.24925907099999522 \t 0.16666666666666666\n",
            "i 0.24925907199999522 \t 0.16666666666666666\n",
            "i 0.24925907299999522 \t 0.16666666666666666\n",
            "i 0.24925907399999522 \t 0.16666666666666666\n",
            "i 0.24925907499999522 \t 0.16666666666666666\n",
            "i 0.24925907599999522 \t 0.16666666666666666\n",
            "i 0.24925907699999522 \t 0.16666666666666666\n",
            "i 0.24925907799999522 \t 0.16666666666666666\n",
            "i 0.24925907899999522 \t 0.16666666666666666\n",
            "i 0.24925907999999522 \t 0.16666666666666666\n",
            "i 0.24925908099999522 \t 0.16666666666666666\n",
            "i 0.24925908199999522 \t 0.16666666666666666\n",
            "i 0.24925908299999522 \t 0.16666666666666666\n",
            "i 0.24925908399999522 \t 0.16666666666666666\n",
            "i 0.24925908499999522 \t 0.16666666666666666\n",
            "i 0.24925908599999522 \t 0.16666666666666666\n",
            "i 0.24925908699999522 \t 0.16666666666666666\n",
            "i 0.24925908799999522 \t 0.16666666666666666\n",
            "i 0.24925908899999522 \t 0.16666666666666666\n",
            "i 0.24925908999999521 \t 0.16666666666666666\n",
            "i 0.24925909099999521 \t 0.16666666666666666\n",
            "i 0.2492590919999952 \t 0.16666666666666666\n",
            "i 0.2492590929999952 \t 0.16666666666666666\n",
            "i 0.2492590939999952 \t 0.16666666666666666\n",
            "i 0.2492590949999952 \t 0.16666666666666666\n",
            "i 0.2492590959999952 \t 0.16666666666666666\n",
            "i 0.2492590969999952 \t 0.16666666666666666\n",
            "i 0.2492590979999952 \t 0.16666666666666666\n",
            "i 0.2492590989999952 \t 0.16666666666666666\n",
            "i 0.2492590999999952 \t 0.16666666666666666\n",
            "i 0.2492591009999952 \t 0.16666666666666666\n",
            "i 0.2492591019999952 \t 0.16666666666666666\n",
            "i 0.2492591029999952 \t 0.16666666666666666\n",
            "i 0.2492591039999952 \t 0.16666666666666666\n",
            "i 0.2492591049999952 \t 0.16666666666666666\n",
            "i 0.2492591059999952 \t 0.16666666666666666\n",
            "i 0.2492591069999952 \t 0.16666666666666666\n",
            "i 0.2492591079999952 \t 0.16666666666666666\n",
            "i 0.2492591089999952 \t 0.16666666666666666\n",
            "i 0.2492591099999952 \t 0.16666666666666666\n",
            "i 0.2492591109999952 \t 0.16666666666666666\n",
            "i 0.2492591119999952 \t 0.16666666666666666\n",
            "i 0.2492591129999952 \t 0.16666666666666666\n",
            "i 0.2492591139999952 \t 0.16666666666666666\n",
            "i 0.2492591149999952 \t 0.16666666666666666\n",
            "i 0.2492591159999952 \t 0.16666666666666666\n",
            "i 0.2492591169999952 \t 0.16666666666666666\n",
            "i 0.2492591179999952 \t 0.16666666666666666\n",
            "i 0.2492591189999952 \t 0.16666666666666666\n",
            "i 0.2492591199999952 \t 0.16666666666666666\n",
            "i 0.2492591209999952 \t 0.16666666666666666\n",
            "i 0.2492591219999952 \t 0.16666666666666666\n",
            "i 0.2492591229999952 \t 0.16666666666666666\n",
            "i 0.2492591239999952 \t 0.16666666666666666\n",
            "i 0.2492591249999952 \t 0.16666666666666666\n",
            "i 0.2492591259999952 \t 0.16666666666666666\n",
            "i 0.2492591269999952 \t 0.16666666666666666\n",
            "i 0.2492591279999952 \t 0.16666666666666666\n",
            "i 0.2492591289999952 \t 0.16666666666666666\n",
            "i 0.2492591299999952 \t 0.16666666666666666\n",
            "i 0.2492591309999952 \t 0.16666666666666666\n",
            "i 0.2492591319999952 \t 0.16666666666666666\n",
            "i 0.2492591329999952 \t 0.16666666666666666\n",
            "i 0.2492591339999952 \t 0.16666666666666666\n",
            "i 0.2492591349999952 \t 0.16666666666666666\n",
            "i 0.2492591359999952 \t 0.16666666666666666\n",
            "i 0.2492591369999952 \t 0.16666666666666666\n",
            "i 0.2492591379999952 \t 0.16666666666666666\n",
            "i 0.2492591389999952 \t 0.16666666666666666\n",
            "i 0.2492591399999952 \t 0.16666666666666666\n",
            "i 0.2492591409999952 \t 0.16666666666666666\n",
            "i 0.2492591419999952 \t 0.16666666666666666\n",
            "i 0.2492591429999952 \t 0.16666666666666666\n",
            "i 0.2492591439999952 \t 0.16666666666666666\n",
            "i 0.24925914499999519 \t 0.16666666666666666\n",
            "i 0.24925914599999519 \t 0.16666666666666666\n",
            "i 0.24925914699999518 \t 0.16666666666666666\n",
            "i 0.24925914799999518 \t 0.16666666666666666\n",
            "i 0.24925914899999518 \t 0.16666666666666666\n",
            "i 0.24925914999999518 \t 0.16666666666666666\n",
            "i 0.24925915099999518 \t 0.16666666666666666\n",
            "i 0.24925915199999518 \t 0.16666666666666666\n",
            "i 0.24925915299999518 \t 0.16666666666666666\n",
            "i 0.24925915399999518 \t 0.16666666666666666\n",
            "i 0.24925915499999518 \t 0.16666666666666666\n",
            "i 0.24925915599999518 \t 0.16666666666666666\n",
            "i 0.24925915699999518 \t 0.16666666666666666\n",
            "i 0.24925915799999518 \t 0.16666666666666666\n",
            "i 0.24925915899999518 \t 0.16666666666666666\n",
            "i 0.24925915999999518 \t 0.16666666666666666\n",
            "i 0.24925916099999518 \t 0.16666666666666666\n",
            "i 0.24925916199999518 \t 0.16666666666666666\n",
            "i 0.24925916299999518 \t 0.16666666666666666\n",
            "i 0.24925916399999518 \t 0.16666666666666666\n",
            "i 0.24925916499999518 \t 0.16666666666666666\n",
            "i 0.24925916599999517 \t 0.16666666666666666\n",
            "i 0.24925916699999517 \t 0.16666666666666666\n",
            "i 0.24925916799999517 \t 0.16666666666666666\n",
            "i 0.24925916899999517 \t 0.16666666666666666\n",
            "i 0.24925916999999517 \t 0.16666666666666666\n",
            "i 0.24925917099999517 \t 0.16666666666666666\n",
            "i 0.24925917199999517 \t 0.16666666666666666\n",
            "i 0.24925917299999517 \t 0.16666666666666666\n",
            "i 0.24925917399999517 \t 0.16666666666666666\n",
            "i 0.24925917499999517 \t 0.16666666666666666\n",
            "i 0.24925917599999517 \t 0.16666666666666666\n",
            "i 0.24925917699999517 \t 0.16666666666666666\n",
            "i 0.24925917799999517 \t 0.16666666666666666\n",
            "i 0.24925917899999517 \t 0.16666666666666666\n",
            "i 0.24925917999999517 \t 0.16666666666666666\n",
            "i 0.24925918099999517 \t 0.16666666666666666\n",
            "i 0.24925918199999517 \t 0.16666666666666666\n",
            "i 0.24925918299999517 \t 0.16666666666666666\n",
            "i 0.24925918399999517 \t 0.16666666666666666\n",
            "i 0.24925918499999516 \t 0.16666666666666666\n",
            "i 0.24925918599999516 \t 0.16666666666666666\n",
            "i 0.24925918699999516 \t 0.16666666666666666\n",
            "i 0.24925918799999516 \t 0.16666666666666666\n",
            "i 0.24925918899999516 \t 0.16666666666666666\n",
            "i 0.24925918999999516 \t 0.16666666666666666\n",
            "i 0.24925919099999516 \t 0.16666666666666666\n",
            "i 0.24925919199999516 \t 0.16666666666666666\n",
            "i 0.24925919299999516 \t 0.16666666666666666\n",
            "i 0.24925919399999516 \t 0.16666666666666666\n",
            "i 0.24925919499999516 \t 0.16666666666666666\n",
            "i 0.24925919599999516 \t 0.16666666666666666\n",
            "i 0.24925919699999516 \t 0.16666666666666666\n",
            "i 0.24925919799999516 \t 0.16666666666666666\n",
            "i 0.24925919899999516 \t 0.16666666666666666\n",
            "i 0.24925919999999516 \t 0.16666666666666666\n",
            "i 0.24925920099999516 \t 0.16666666666666666\n",
            "i 0.24925920199999516 \t 0.16666666666666666\n",
            "i 0.24925920299999516 \t 0.16666666666666666\n",
            "i 0.24925920399999515 \t 0.16666666666666666\n",
            "i 0.24925920499999515 \t 0.16666666666666666\n",
            "i 0.24925920599999515 \t 0.16666666666666666\n",
            "i 0.24925920699999515 \t 0.16666666666666666\n",
            "i 0.24925920799999515 \t 0.16666666666666666\n",
            "i 0.24925920899999515 \t 0.16666666666666666\n",
            "i 0.24925920999999515 \t 0.16666666666666666\n",
            "i 0.24925921099999515 \t 0.16666666666666666\n",
            "i 0.24925921199999515 \t 0.16666666666666666\n",
            "i 0.24925921299999515 \t 0.16666666666666666\n",
            "i 0.24925921399999515 \t 0.16666666666666666\n",
            "i 0.24925921499999515 \t 0.16666666666666666\n",
            "i 0.24925921599999515 \t 0.16666666666666666\n",
            "i 0.24925921699999515 \t 0.16666666666666666\n",
            "i 0.24925921799999515 \t 0.16666666666666666\n",
            "i 0.24925921899999515 \t 0.16666666666666666\n",
            "i 0.24925921999999515 \t 0.16666666666666666\n",
            "i 0.24925922099999515 \t 0.16666666666666666\n",
            "i 0.24925922199999515 \t 0.16666666666666666\n",
            "i 0.24925922299999514 \t 0.16666666666666666\n",
            "i 0.24925922399999514 \t 0.16666666666666666\n",
            "i 0.24925922499999514 \t 0.16666666666666666\n",
            "i 0.24925922599999514 \t 0.16666666666666666\n",
            "i 0.24925922699999514 \t 0.16666666666666666\n",
            "i 0.24925922799999514 \t 0.16666666666666666\n",
            "i 0.24925922899999514 \t 0.16666666666666666\n",
            "i 0.24925922999999514 \t 0.16666666666666666\n",
            "i 0.24925923099999514 \t 0.16666666666666666\n",
            "i 0.24925923199999514 \t 0.16666666666666666\n",
            "i 0.24925923299999514 \t 0.16666666666666666\n",
            "i 0.24925923399999514 \t 0.16666666666666666\n",
            "i 0.24925923499999514 \t 0.16666666666666666\n",
            "i 0.24925923599999514 \t 0.16666666666666666\n",
            "i 0.24925923699999514 \t 0.16666666666666666\n",
            "i 0.24925923799999514 \t 0.16666666666666666\n",
            "i 0.24925923899999514 \t 0.16666666666666666\n",
            "i 0.24925923999999514 \t 0.16666666666666666\n",
            "i 0.24925924099999514 \t 0.16666666666666666\n",
            "i 0.24925924199999513 \t 0.16666666666666666\n",
            "i 0.24925924299999513 \t 0.16666666666666666\n",
            "i 0.24925924399999513 \t 0.16666666666666666\n",
            "i 0.24925924499999513 \t 0.16666666666666666\n",
            "i 0.24925924599999513 \t 0.16666666666666666\n",
            "i 0.24925924699999513 \t 0.16666666666666666\n",
            "i 0.24925924799999513 \t 0.16666666666666666\n",
            "i 0.24925924899999513 \t 0.16666666666666666\n",
            "i 0.24925924999999513 \t 0.16666666666666666\n",
            "i 0.24925925099999513 \t 0.16666666666666666\n",
            "i 0.24925925199999513 \t 0.16666666666666666\n",
            "i 0.24925925299999513 \t 0.16666666666666666\n",
            "i 0.24925925399999513 \t 0.16666666666666666\n",
            "i 0.24925925499999513 \t 0.16666666666666666\n",
            "i 0.24925925599999513 \t 0.16666666666666666\n",
            "i 0.24925925699999513 \t 0.16666666666666666\n",
            "i 0.24925925799999513 \t 0.16666666666666666\n",
            "i 0.24925925899999513 \t 0.16666666666666666\n",
            "i 0.24925925999999513 \t 0.16666666666666666\n",
            "i 0.24925926099999512 \t 0.16666666666666666\n",
            "i 0.24925926199999512 \t 0.16666666666666666\n",
            "i 0.24925926299999512 \t 0.16666666666666666\n",
            "i 0.24925926399999512 \t 0.16666666666666666\n",
            "i 0.24925926499999512 \t 0.16666666666666666\n",
            "i 0.24925926599999512 \t 0.16666666666666666\n",
            "i 0.24925926699999512 \t 0.16666666666666666\n",
            "i 0.24925926799999512 \t 0.16666666666666666\n",
            "i 0.24925926899999512 \t 0.16666666666666666\n",
            "i 0.24925926999999512 \t 0.16666666666666666\n",
            "i 0.24925927099999512 \t 0.16666666666666666\n",
            "i 0.24925927199999512 \t 0.16666666666666666\n",
            "i 0.24925927299999512 \t 0.16666666666666666\n",
            "i 0.24925927399999512 \t 0.16666666666666666\n",
            "i 0.24925927499999512 \t 0.16666666666666666\n",
            "i 0.24925927599999512 \t 0.16666666666666666\n",
            "i 0.24925927699999512 \t 0.16666666666666666\n",
            "i 0.24925927799999512 \t 0.16666666666666666\n",
            "i 0.24925927899999512 \t 0.16666666666666666\n",
            "i 0.24925927999999511 \t 0.16666666666666666\n",
            "i 0.24925928099999511 \t 0.16666666666666666\n",
            "i 0.2492592819999951 \t 0.16666666666666666\n",
            "i 0.2492592829999951 \t 0.16666666666666666\n",
            "i 0.2492592839999951 \t 0.16666666666666666\n",
            "i 0.2492592849999951 \t 0.16666666666666666\n",
            "i 0.2492592859999951 \t 0.16666666666666666\n",
            "i 0.2492592869999951 \t 0.16666666666666666\n",
            "i 0.2492592879999951 \t 0.16666666666666666\n",
            "i 0.2492592889999951 \t 0.16666666666666666\n",
            "i 0.2492592899999951 \t 0.16666666666666666\n",
            "i 0.2492592909999951 \t 0.16666666666666666\n",
            "i 0.2492592919999951 \t 0.16666666666666666\n",
            "i 0.2492592929999951 \t 0.16666666666666666\n",
            "i 0.2492592939999951 \t 0.16666666666666666\n",
            "i 0.2492592949999951 \t 0.16666666666666666\n",
            "i 0.2492592959999951 \t 0.16666666666666666\n",
            "i 0.2492592969999951 \t 0.16666666666666666\n",
            "i 0.2492592979999951 \t 0.16666666666666666\n",
            "i 0.2492592989999951 \t 0.16666666666666666\n",
            "i 0.2492592999999951 \t 0.16666666666666666\n",
            "i 0.2492593009999951 \t 0.16666666666666666\n",
            "i 0.2492593019999951 \t 0.16666666666666666\n",
            "i 0.2492593029999951 \t 0.16666666666666666\n",
            "i 0.2492593039999951 \t 0.16666666666666666\n",
            "i 0.2492593049999951 \t 0.16666666666666666\n",
            "i 0.2492593059999951 \t 0.16666666666666666\n",
            "i 0.2492593069999951 \t 0.16666666666666666\n",
            "i 0.2492593079999951 \t 0.16666666666666666\n",
            "i 0.2492593089999951 \t 0.16666666666666666\n",
            "i 0.2492593099999951 \t 0.16666666666666666\n",
            "i 0.2492593109999951 \t 0.16666666666666666\n",
            "i 0.2492593119999951 \t 0.16666666666666666\n",
            "i 0.2492593129999951 \t 0.16666666666666666\n",
            "i 0.2492593139999951 \t 0.16666666666666666\n",
            "i 0.2492593149999951 \t 0.16666666666666666\n",
            "i 0.2492593159999951 \t 0.16666666666666666\n",
            "i 0.2492593169999951 \t 0.16666666666666666\n",
            "i 0.2492593179999951 \t 0.16666666666666666\n",
            "i 0.2492593189999951 \t 0.16666666666666666\n",
            "i 0.2492593199999951 \t 0.16666666666666666\n",
            "i 0.2492593209999951 \t 0.16666666666666666\n",
            "i 0.2492593219999951 \t 0.16666666666666666\n",
            "i 0.2492593229999951 \t 0.16666666666666666\n",
            "i 0.2492593239999951 \t 0.16666666666666666\n",
            "i 0.2492593249999951 \t 0.16666666666666666\n",
            "i 0.2492593259999951 \t 0.16666666666666666\n",
            "i 0.2492593269999951 \t 0.16666666666666666\n",
            "i 0.2492593279999951 \t 0.16666666666666666\n",
            "i 0.2492593289999951 \t 0.16666666666666666\n",
            "i 0.2492593299999951 \t 0.16666666666666666\n",
            "i 0.2492593309999951 \t 0.16666666666666666\n",
            "i 0.2492593319999951 \t 0.16666666666666666\n",
            "i 0.2492593329999951 \t 0.16666666666666666\n",
            "i 0.2492593339999951 \t 0.16666666666666666\n",
            "i 0.24925933499999509 \t 0.16666666666666666\n",
            "i 0.24925933599999509 \t 0.16666666666666666\n",
            "i 0.24925933699999508 \t 0.16666666666666666\n",
            "i 0.24925933799999508 \t 0.16666666666666666\n",
            "i 0.24925933899999508 \t 0.16666666666666666\n",
            "i 0.24925933999999508 \t 0.16666666666666666\n",
            "i 0.24925934099999508 \t 0.16666666666666666\n",
            "i 0.24925934199999508 \t 0.16666666666666666\n",
            "i 0.24925934299999508 \t 0.16666666666666666\n",
            "i 0.24925934399999508 \t 0.16666666666666666\n",
            "i 0.24925934499999508 \t 0.16666666666666666\n",
            "i 0.24925934599999508 \t 0.16666666666666666\n",
            "i 0.24925934699999508 \t 0.16666666666666666\n",
            "i 0.24925934799999508 \t 0.16666666666666666\n",
            "i 0.24925934899999508 \t 0.16666666666666666\n",
            "i 0.24925934999999508 \t 0.16666666666666666\n",
            "i 0.24925935099999508 \t 0.16666666666666666\n",
            "i 0.24925935199999508 \t 0.16666666666666666\n",
            "i 0.24925935299999508 \t 0.16666666666666666\n",
            "i 0.24925935399999508 \t 0.16666666666666666\n",
            "i 0.24925935499999508 \t 0.16666666666666666\n",
            "i 0.24925935599999507 \t 0.16666666666666666\n",
            "i 0.24925935699999507 \t 0.16666666666666666\n",
            "i 0.24925935799999507 \t 0.16666666666666666\n",
            "i 0.24925935899999507 \t 0.16666666666666666\n",
            "i 0.24925935999999507 \t 0.16666666666666666\n",
            "i 0.24925936099999507 \t 0.16666666666666666\n",
            "i 0.24925936199999507 \t 0.16666666666666666\n",
            "i 0.24925936299999507 \t 0.16666666666666666\n",
            "i 0.24925936399999507 \t 0.16666666666666666\n",
            "i 0.24925936499999507 \t 0.16666666666666666\n",
            "i 0.24925936599999507 \t 0.16666666666666666\n",
            "i 0.24925936699999507 \t 0.16666666666666666\n",
            "i 0.24925936799999507 \t 0.16666666666666666\n",
            "i 0.24925936899999507 \t 0.16666666666666666\n",
            "i 0.24925936999999507 \t 0.16666666666666666\n",
            "i 0.24925937099999507 \t 0.16666666666666666\n",
            "i 0.24925937199999507 \t 0.16666666666666666\n",
            "i 0.24925937299999507 \t 0.16666666666666666\n",
            "i 0.24925937399999507 \t 0.16666666666666666\n",
            "i 0.24925937499999506 \t 0.16666666666666666\n",
            "i 0.24925937599999506 \t 0.16666666666666666\n",
            "i 0.24925937699999506 \t 0.16666666666666666\n",
            "i 0.24925937799999506 \t 0.16666666666666666\n",
            "i 0.24925937899999506 \t 0.16666666666666666\n",
            "i 0.24925937999999506 \t 0.16666666666666666\n",
            "i 0.24925938099999506 \t 0.16666666666666666\n",
            "i 0.24925938199999506 \t 0.16666666666666666\n",
            "i 0.24925938299999506 \t 0.16666666666666666\n",
            "i 0.24925938399999506 \t 0.16666666666666666\n",
            "i 0.24925938499999506 \t 0.16666666666666666\n",
            "i 0.24925938599999506 \t 0.16666666666666666\n",
            "i 0.24925938699999506 \t 0.16666666666666666\n",
            "i 0.24925938799999506 \t 0.16666666666666666\n",
            "i 0.24925938899999506 \t 0.16666666666666666\n",
            "i 0.24925938999999506 \t 0.16666666666666666\n",
            "i 0.24925939099999506 \t 0.16666666666666666\n",
            "i 0.24925939199999506 \t 0.16666666666666666\n",
            "i 0.24925939299999506 \t 0.16666666666666666\n",
            "i 0.24925939399999505 \t 0.16666666666666666\n",
            "i 0.24925939499999505 \t 0.16666666666666666\n",
            "i 0.24925939599999505 \t 0.16666666666666666\n",
            "i 0.24925939699999505 \t 0.16666666666666666\n",
            "i 0.24925939799999505 \t 0.16666666666666666\n",
            "i 0.24925939899999505 \t 0.16666666666666666\n",
            "i 0.24925939999999505 \t 0.16666666666666666\n",
            "i 0.24925940099999505 \t 0.16666666666666666\n",
            "i 0.24925940199999505 \t 0.16666666666666666\n",
            "i 0.24925940299999505 \t 0.16666666666666666\n",
            "i 0.24925940399999505 \t 0.16666666666666666\n",
            "i 0.24925940499999505 \t 0.16666666666666666\n",
            "i 0.24925940599999505 \t 0.16666666666666666\n",
            "i 0.24925940699999505 \t 0.16666666666666666\n",
            "i 0.24925940799999505 \t 0.16666666666666666\n",
            "i 0.24925940899999505 \t 0.16666666666666666\n",
            "i 0.24925940999999505 \t 0.16666666666666666\n",
            "i 0.24925941099999505 \t 0.16666666666666666\n",
            "i 0.24925941199999505 \t 0.16666666666666666\n",
            "i 0.24925941299999504 \t 0.16666666666666666\n",
            "i 0.24925941399999504 \t 0.16666666666666666\n",
            "i 0.24925941499999504 \t 0.16666666666666666\n",
            "i 0.24925941599999504 \t 0.16666666666666666\n",
            "i 0.24925941699999504 \t 0.16666666666666666\n",
            "i 0.24925941799999504 \t 0.16666666666666666\n",
            "i 0.24925941899999504 \t 0.16666666666666666\n",
            "i 0.24925941999999504 \t 0.16666666666666666\n",
            "i 0.24925942099999504 \t 0.16666666666666666\n",
            "i 0.24925942199999504 \t 0.16666666666666666\n",
            "i 0.24925942299999504 \t 0.16666666666666666\n",
            "i 0.24925942399999504 \t 0.16666666666666666\n",
            "i 0.24925942499999504 \t 0.16666666666666666\n",
            "i 0.24925942599999504 \t 0.16666666666666666\n",
            "i 0.24925942699999504 \t 0.16666666666666666\n",
            "i 0.24925942799999504 \t 0.16666666666666666\n",
            "i 0.24925942899999504 \t 0.16666666666666666\n",
            "i 0.24925942999999504 \t 0.16666666666666666\n",
            "i 0.24925943099999504 \t 0.16666666666666666\n",
            "i 0.24925943199999503 \t 0.16666666666666666\n",
            "i 0.24925943299999503 \t 0.16666666666666666\n",
            "i 0.24925943399999503 \t 0.16666666666666666\n",
            "i 0.24925943499999503 \t 0.16666666666666666\n",
            "i 0.24925943599999503 \t 0.16666666666666666\n",
            "i 0.24925943699999503 \t 0.16666666666666666\n",
            "i 0.24925943799999503 \t 0.16666666666666666\n",
            "i 0.24925943899999503 \t 0.16666666666666666\n",
            "i 0.24925943999999503 \t 0.16666666666666666\n",
            "i 0.24925944099999503 \t 0.16666666666666666\n",
            "i 0.24925944199999503 \t 0.16666666666666666\n",
            "i 0.24925944299999503 \t 0.16666666666666666\n",
            "i 0.24925944399999503 \t 0.16666666666666666\n",
            "i 0.24925944499999503 \t 0.16666666666666666\n",
            "i 0.24925944599999503 \t 0.16666666666666666\n",
            "i 0.24925944699999503 \t 0.16666666666666666\n",
            "i 0.24925944799999503 \t 0.16666666666666666\n",
            "i 0.24925944899999503 \t 0.16666666666666666\n",
            "i 0.24925944999999503 \t 0.16666666666666666\n",
            "i 0.24925945099999502 \t 0.16666666666666666\n",
            "i 0.24925945199999502 \t 0.16666666666666666\n",
            "i 0.24925945299999502 \t 0.16666666666666666\n",
            "i 0.24925945399999502 \t 0.16666666666666666\n",
            "i 0.24925945499999502 \t 0.16666666666666666\n",
            "i 0.24925945599999502 \t 0.16666666666666666\n",
            "i 0.24925945699999502 \t 0.16666666666666666\n",
            "i 0.24925945799999502 \t 0.16666666666666666\n",
            "i 0.24925945899999502 \t 0.16666666666666666\n",
            "i 0.24925945999999502 \t 0.16666666666666666\n",
            "i 0.24925946099999502 \t 0.16666666666666666\n",
            "i 0.24925946199999502 \t 0.16666666666666666\n",
            "i 0.24925946299999502 \t 0.16666666666666666\n",
            "i 0.24925946399999502 \t 0.16666666666666666\n",
            "i 0.24925946499999502 \t 0.16666666666666666\n",
            "i 0.24925946599999502 \t 0.16666666666666666\n",
            "i 0.24925946699999502 \t 0.16666666666666666\n",
            "i 0.24925946799999502 \t 0.16666666666666666\n",
            "i 0.24925946899999502 \t 0.16666666666666666\n",
            "i 0.24925946999999501 \t 0.16666666666666666\n",
            "i 0.24925947099999501 \t 0.16666666666666666\n",
            "i 0.249259471999995 \t 0.16666666666666666\n",
            "i 0.249259472999995 \t 0.16666666666666666\n",
            "i 0.249259473999995 \t 0.16666666666666666\n",
            "i 0.249259474999995 \t 0.16666666666666666\n",
            "i 0.249259475999995 \t 0.16666666666666666\n",
            "i 0.249259476999995 \t 0.16666666666666666\n",
            "i 0.249259477999995 \t 0.16666666666666666\n",
            "i 0.249259478999995 \t 0.16666666666666666\n",
            "i 0.249259479999995 \t 0.16666666666666666\n",
            "i 0.249259480999995 \t 0.16666666666666666\n",
            "i 0.249259481999995 \t 0.16666666666666666\n",
            "i 0.249259482999995 \t 0.16666666666666666\n",
            "i 0.249259483999995 \t 0.16666666666666666\n",
            "i 0.249259484999995 \t 0.16666666666666666\n",
            "i 0.249259485999995 \t 0.16666666666666666\n",
            "i 0.249259486999995 \t 0.16666666666666666\n",
            "i 0.249259487999995 \t 0.16666666666666666\n",
            "i 0.249259488999995 \t 0.16666666666666666\n",
            "i 0.249259489999995 \t 0.16666666666666666\n",
            "i 0.249259490999995 \t 0.16666666666666666\n",
            "i 0.249259491999995 \t 0.16666666666666666\n",
            "i 0.249259492999995 \t 0.16666666666666666\n",
            "i 0.249259493999995 \t 0.16666666666666666\n",
            "i 0.249259494999995 \t 0.16666666666666666\n",
            "i 0.249259495999995 \t 0.16666666666666666\n",
            "i 0.249259496999995 \t 0.16666666666666666\n",
            "i 0.249259497999995 \t 0.16666666666666666\n",
            "i 0.249259498999995 \t 0.16666666666666666\n",
            "i 0.249259499999995 \t 0.16666666666666666\n",
            "i 0.249259500999995 \t 0.16666666666666666\n",
            "i 0.249259501999995 \t 0.16666666666666666\n",
            "i 0.249259502999995 \t 0.16666666666666666\n",
            "i 0.249259503999995 \t 0.16666666666666666\n",
            "i 0.249259504999995 \t 0.16666666666666666\n",
            "i 0.249259505999995 \t 0.16666666666666666\n",
            "i 0.249259506999995 \t 0.16666666666666666\n",
            "i 0.249259507999995 \t 0.16666666666666666\n",
            "i 0.249259508999995 \t 0.16666666666666666\n",
            "i 0.249259509999995 \t 0.16666666666666666\n",
            "i 0.249259510999995 \t 0.16666666666666666\n",
            "i 0.249259511999995 \t 0.16666666666666666\n",
            "i 0.249259512999995 \t 0.16666666666666666\n",
            "i 0.249259513999995 \t 0.16666666666666666\n",
            "i 0.249259514999995 \t 0.16666666666666666\n",
            "i 0.249259515999995 \t 0.16666666666666666\n",
            "i 0.249259516999995 \t 0.16666666666666666\n",
            "i 0.249259517999995 \t 0.16666666666666666\n",
            "i 0.249259518999995 \t 0.16666666666666666\n",
            "i 0.249259519999995 \t 0.16666666666666666\n",
            "i 0.249259520999995 \t 0.16666666666666666\n",
            "i 0.249259521999995 \t 0.16666666666666666\n",
            "i 0.249259522999995 \t 0.16666666666666666\n",
            "i 0.249259523999995 \t 0.16666666666666666\n",
            "i 0.24925952499999499 \t 0.16666666666666666\n",
            "i 0.24925952599999499 \t 0.16666666666666666\n",
            "i 0.24925952699999498 \t 0.16666666666666666\n",
            "i 0.24925952799999498 \t 0.16666666666666666\n",
            "i 0.24925952899999498 \t 0.16666666666666666\n",
            "i 0.24925952999999498 \t 0.16666666666666666\n",
            "i 0.24925953099999498 \t 0.16666666666666666\n",
            "i 0.24925953199999498 \t 0.16666666666666666\n",
            "i 0.24925953299999498 \t 0.16666666666666666\n",
            "i 0.24925953399999498 \t 0.16666666666666666\n",
            "i 0.24925953499999498 \t 0.16666666666666666\n",
            "i 0.24925953599999498 \t 0.16666666666666666\n",
            "i 0.24925953699999498 \t 0.16666666666666666\n",
            "i 0.24925953799999498 \t 0.16666666666666666\n",
            "i 0.24925953899999498 \t 0.16666666666666666\n",
            "i 0.24925953999999498 \t 0.16666666666666666\n",
            "i 0.24925954099999498 \t 0.16666666666666666\n",
            "i 0.24925954199999498 \t 0.16666666666666666\n",
            "i 0.24925954299999498 \t 0.16666666666666666\n",
            "i 0.24925954399999498 \t 0.16666666666666666\n",
            "i 0.24925954499999498 \t 0.16666666666666666\n",
            "i 0.24925954599999497 \t 0.16666666666666666\n",
            "i 0.24925954699999497 \t 0.16666666666666666\n",
            "i 0.24925954799999497 \t 0.16666666666666666\n",
            "i 0.24925954899999497 \t 0.16666666666666666\n",
            "i 0.24925954999999497 \t 0.16666666666666666\n",
            "i 0.24925955099999497 \t 0.16666666666666666\n",
            "i 0.24925955199999497 \t 0.16666666666666666\n",
            "i 0.24925955299999497 \t 0.16666666666666666\n",
            "i 0.24925955399999497 \t 0.16666666666666666\n",
            "i 0.24925955499999497 \t 0.16666666666666666\n",
            "i 0.24925955599999497 \t 0.16666666666666666\n",
            "i 0.24925955699999497 \t 0.16666666666666666\n",
            "i 0.24925955799999497 \t 0.16666666666666666\n",
            "i 0.24925955899999497 \t 0.16666666666666666\n",
            "i 0.24925955999999497 \t 0.16666666666666666\n",
            "i 0.24925956099999497 \t 0.16666666666666666\n",
            "i 0.24925956199999497 \t 0.16666666666666666\n",
            "i 0.24925956299999497 \t 0.16666666666666666\n",
            "i 0.24925956399999497 \t 0.16666666666666666\n",
            "i 0.24925956499999496 \t 0.16666666666666666\n",
            "i 0.24925956599999496 \t 0.16666666666666666\n",
            "i 0.24925956699999496 \t 0.16666666666666666\n",
            "i 0.24925956799999496 \t 0.16666666666666666\n",
            "i 0.24925956899999496 \t 0.16666666666666666\n",
            "i 0.24925956999999496 \t 0.16666666666666666\n",
            "i 0.24925957099999496 \t 0.16666666666666666\n",
            "i 0.24925957199999496 \t 0.16666666666666666\n",
            "i 0.24925957299999496 \t 0.16666666666666666\n",
            "i 0.24925957399999496 \t 0.16666666666666666\n",
            "i 0.24925957499999496 \t 0.16666666666666666\n",
            "i 0.24925957599999496 \t 0.16666666666666666\n",
            "i 0.24925957699999496 \t 0.16666666666666666\n",
            "i 0.24925957799999496 \t 0.16666666666666666\n",
            "i 0.24925957899999496 \t 0.16666666666666666\n",
            "i 0.24925957999999496 \t 0.16666666666666666\n",
            "i 0.24925958099999496 \t 0.16666666666666666\n",
            "i 0.24925958199999496 \t 0.16666666666666666\n",
            "i 0.24925958299999496 \t 0.16666666666666666\n",
            "i 0.24925958399999495 \t 0.16666666666666666\n",
            "i 0.24925958499999495 \t 0.16666666666666666\n",
            "i 0.24925958599999495 \t 0.16666666666666666\n",
            "i 0.24925958699999495 \t 0.16666666666666666\n",
            "i 0.24925958799999495 \t 0.16666666666666666\n",
            "i 0.24925958899999495 \t 0.16666666666666666\n",
            "i 0.24925958999999495 \t 0.16666666666666666\n",
            "i 0.24925959099999495 \t 0.16666666666666666\n",
            "i 0.24925959199999495 \t 0.16666666666666666\n",
            "i 0.24925959299999495 \t 0.16666666666666666\n",
            "i 0.24925959399999495 \t 0.16666666666666666\n",
            "i 0.24925959499999495 \t 0.16666666666666666\n",
            "i 0.24925959599999495 \t 0.16666666666666666\n",
            "i 0.24925959699999495 \t 0.16666666666666666\n",
            "i 0.24925959799999495 \t 0.16666666666666666\n",
            "i 0.24925959899999495 \t 0.16666666666666666\n",
            "i 0.24925959999999495 \t 0.16666666666666666\n",
            "i 0.24925960099999495 \t 0.16666666666666666\n",
            "i 0.24925960199999495 \t 0.16666666666666666\n",
            "i 0.24925960299999494 \t 0.16666666666666666\n",
            "i 0.24925960399999494 \t 0.16666666666666666\n",
            "i 0.24925960499999494 \t 0.16666666666666666\n",
            "i 0.24925960599999494 \t 0.16666666666666666\n",
            "i 0.24925960699999494 \t 0.16666666666666666\n",
            "i 0.24925960799999494 \t 0.16666666666666666\n",
            "i 0.24925960899999494 \t 0.16666666666666666\n",
            "i 0.24925960999999494 \t 0.16666666666666666\n",
            "i 0.24925961099999494 \t 0.16666666666666666\n",
            "i 0.24925961199999494 \t 0.16666666666666666\n",
            "i 0.24925961299999494 \t 0.16666666666666666\n",
            "i 0.24925961399999494 \t 0.16666666666666666\n",
            "i 0.24925961499999494 \t 0.16666666666666666\n",
            "i 0.24925961599999494 \t 0.16666666666666666\n",
            "i 0.24925961699999494 \t 0.16666666666666666\n",
            "i 0.24925961799999494 \t 0.16666666666666666\n",
            "i 0.24925961899999494 \t 0.16666666666666666\n",
            "i 0.24925961999999494 \t 0.16666666666666666\n",
            "i 0.24925962099999494 \t 0.16666666666666666\n",
            "i 0.24925962199999493 \t 0.16666666666666666\n",
            "i 0.24925962299999493 \t 0.16666666666666666\n",
            "i 0.24925962399999493 \t 0.16666666666666666\n",
            "i 0.24925962499999493 \t 0.16666666666666666\n",
            "i 0.24925962599999493 \t 0.16666666666666666\n",
            "i 0.24925962699999493 \t 0.16666666666666666\n",
            "i 0.24925962799999493 \t 0.16666666666666666\n",
            "i 0.24925962899999493 \t 0.16666666666666666\n",
            "i 0.24925962999999493 \t 0.16666666666666666\n",
            "i 0.24925963099999493 \t 0.16666666666666666\n",
            "i 0.24925963199999493 \t 0.16666666666666666\n",
            "i 0.24925963299999493 \t 0.16666666666666666\n",
            "i 0.24925963399999493 \t 0.16666666666666666\n",
            "i 0.24925963499999493 \t 0.16666666666666666\n",
            "i 0.24925963599999493 \t 0.16666666666666666\n",
            "i 0.24925963699999493 \t 0.16666666666666666\n",
            "i 0.24925963799999493 \t 0.16666666666666666\n",
            "i 0.24925963899999493 \t 0.16666666666666666\n",
            "i 0.24925963999999493 \t 0.16666666666666666\n",
            "i 0.24925964099999492 \t 0.16666666666666666\n",
            "i 0.24925964199999492 \t 0.16666666666666666\n",
            "i 0.24925964299999492 \t 0.16666666666666666\n",
            "i 0.24925964399999492 \t 0.16666666666666666\n",
            "i 0.24925964499999492 \t 0.16666666666666666\n",
            "i 0.24925964599999492 \t 0.16666666666666666\n",
            "i 0.24925964699999492 \t 0.16666666666666666\n",
            "i 0.24925964799999492 \t 0.16666666666666666\n",
            "i 0.24925964899999492 \t 0.16666666666666666\n",
            "i 0.24925964999999492 \t 0.16666666666666666\n",
            "i 0.24925965099999492 \t 0.16666666666666666\n",
            "i 0.24925965199999492 \t 0.16666666666666666\n",
            "i 0.24925965299999492 \t 0.16666666666666666\n",
            "i 0.24925965399999492 \t 0.16666666666666666\n",
            "i 0.24925965499999492 \t 0.16666666666666666\n",
            "i 0.24925965599999492 \t 0.16666666666666666\n",
            "i 0.24925965699999492 \t 0.16666666666666666\n",
            "i 0.24925965799999492 \t 0.16666666666666666\n",
            "i 0.24925965899999492 \t 0.16666666666666666\n",
            "i 0.24925965999999491 \t 0.16666666666666666\n",
            "i 0.24925966099999491 \t 0.16666666666666666\n",
            "i 0.2492596619999949 \t 0.16666666666666666\n",
            "i 0.2492596629999949 \t 0.16666666666666666\n",
            "i 0.2492596639999949 \t 0.16666666666666666\n",
            "i 0.2492596649999949 \t 0.16666666666666666\n",
            "i 0.2492596659999949 \t 0.16666666666666666\n",
            "i 0.2492596669999949 \t 0.16666666666666666\n",
            "i 0.2492596679999949 \t 0.16666666666666666\n",
            "i 0.2492596689999949 \t 0.16666666666666666\n",
            "i 0.2492596699999949 \t 0.16666666666666666\n",
            "i 0.2492596709999949 \t 0.16666666666666666\n",
            "i 0.2492596719999949 \t 0.16666666666666666\n",
            "i 0.2492596729999949 \t 0.16666666666666666\n",
            "i 0.2492596739999949 \t 0.16666666666666666\n",
            "i 0.2492596749999949 \t 0.16666666666666666\n",
            "i 0.2492596759999949 \t 0.16666666666666666\n",
            "i 0.2492596769999949 \t 0.16666666666666666\n",
            "i 0.2492596779999949 \t 0.16666666666666666\n",
            "i 0.2492596789999949 \t 0.16666666666666666\n",
            "i 0.2492596799999949 \t 0.16666666666666666\n",
            "i 0.2492596809999949 \t 0.16666666666666666\n",
            "i 0.2492596819999949 \t 0.16666666666666666\n",
            "i 0.2492596829999949 \t 0.16666666666666666\n",
            "i 0.2492596839999949 \t 0.16666666666666666\n",
            "i 0.2492596849999949 \t 0.16666666666666666\n",
            "i 0.2492596859999949 \t 0.16666666666666666\n",
            "i 0.2492596869999949 \t 0.16666666666666666\n",
            "i 0.2492596879999949 \t 0.16666666666666666\n",
            "i 0.2492596889999949 \t 0.16666666666666666\n",
            "i 0.2492596899999949 \t 0.16666666666666666\n",
            "i 0.2492596909999949 \t 0.16666666666666666\n",
            "i 0.2492596919999949 \t 0.16666666666666666\n",
            "i 0.2492596929999949 \t 0.16666666666666666\n",
            "i 0.2492596939999949 \t 0.16666666666666666\n",
            "i 0.2492596949999949 \t 0.16666666666666666\n",
            "i 0.2492596959999949 \t 0.16666666666666666\n",
            "i 0.2492596969999949 \t 0.16666666666666666\n",
            "i 0.2492596979999949 \t 0.16666666666666666\n",
            "i 0.2492596989999949 \t 0.16666666666666666\n",
            "i 0.2492596999999949 \t 0.16666666666666666\n",
            "i 0.2492597009999949 \t 0.16666666666666666\n",
            "i 0.2492597019999949 \t 0.16666666666666666\n",
            "i 0.2492597029999949 \t 0.16666666666666666\n",
            "i 0.2492597039999949 \t 0.16666666666666666\n",
            "i 0.2492597049999949 \t 0.16666666666666666\n",
            "i 0.2492597059999949 \t 0.16666666666666666\n",
            "i 0.2492597069999949 \t 0.16666666666666666\n",
            "i 0.2492597079999949 \t 0.16666666666666666\n",
            "i 0.2492597089999949 \t 0.16666666666666666\n",
            "i 0.2492597099999949 \t 0.16666666666666666\n",
            "i 0.2492597109999949 \t 0.16666666666666666\n",
            "i 0.2492597119999949 \t 0.16666666666666666\n",
            "i 0.2492597129999949 \t 0.16666666666666666\n",
            "i 0.2492597139999949 \t 0.16666666666666666\n",
            "i 0.24925971499999489 \t 0.16666666666666666\n",
            "i 0.24925971599999489 \t 0.16666666666666666\n",
            "i 0.24925971699999488 \t 0.16666666666666666\n",
            "i 0.24925971799999488 \t 0.16666666666666666\n",
            "i 0.24925971899999488 \t 0.16666666666666666\n",
            "i 0.24925971999999488 \t 0.16666666666666666\n",
            "i 0.24925972099999488 \t 0.16666666666666666\n",
            "i 0.24925972199999488 \t 0.16666666666666666\n",
            "i 0.24925972299999488 \t 0.16666666666666666\n",
            "i 0.24925972399999488 \t 0.16666666666666666\n",
            "i 0.24925972499999488 \t 0.16666666666666666\n",
            "i 0.24925972599999488 \t 0.16666666666666666\n",
            "i 0.24925972699999488 \t 0.16666666666666666\n",
            "i 0.24925972799999488 \t 0.16666666666666666\n",
            "i 0.24925972899999488 \t 0.16666666666666666\n",
            "i 0.24925972999999488 \t 0.16666666666666666\n",
            "i 0.24925973099999488 \t 0.16666666666666666\n",
            "i 0.24925973199999488 \t 0.16666666666666666\n",
            "i 0.24925973299999488 \t 0.16666666666666666\n",
            "i 0.24925973399999488 \t 0.16666666666666666\n",
            "i 0.24925973499999488 \t 0.16666666666666666\n",
            "i 0.24925973599999487 \t 0.16666666666666666\n",
            "i 0.24925973699999487 \t 0.16666666666666666\n",
            "i 0.24925973799999487 \t 0.16666666666666666\n",
            "i 0.24925973899999487 \t 0.16666666666666666\n",
            "i 0.24925973999999487 \t 0.16666666666666666\n",
            "i 0.24925974099999487 \t 0.16666666666666666\n",
            "i 0.24925974199999487 \t 0.16666666666666666\n",
            "i 0.24925974299999487 \t 0.16666666666666666\n",
            "i 0.24925974399999487 \t 0.16666666666666666\n",
            "i 0.24925974499999487 \t 0.16666666666666666\n",
            "i 0.24925974599999487 \t 0.16666666666666666\n",
            "i 0.24925974699999487 \t 0.16666666666666666\n",
            "i 0.24925974799999487 \t 0.16666666666666666\n",
            "i 0.24925974899999487 \t 0.16666666666666666\n",
            "i 0.24925974999999487 \t 0.16666666666666666\n",
            "i 0.24925975099999487 \t 0.16666666666666666\n",
            "i 0.24925975199999487 \t 0.16666666666666666\n",
            "i 0.24925975299999487 \t 0.16666666666666666\n",
            "i 0.24925975399999487 \t 0.16666666666666666\n",
            "i 0.24925975499999486 \t 0.16666666666666666\n",
            "i 0.24925975599999486 \t 0.16666666666666666\n",
            "i 0.24925975699999486 \t 0.16666666666666666\n",
            "i 0.24925975799999486 \t 0.16666666666666666\n",
            "i 0.24925975899999486 \t 0.16666666666666666\n",
            "i 0.24925975999999486 \t 0.16666666666666666\n",
            "i 0.24925976099999486 \t 0.16666666666666666\n",
            "i 0.24925976199999486 \t 0.16666666666666666\n",
            "i 0.24925976299999486 \t 0.16666666666666666\n",
            "i 0.24925976399999486 \t 0.16666666666666666\n",
            "i 0.24925976499999486 \t 0.16666666666666666\n",
            "i 0.24925976599999486 \t 0.16666666666666666\n",
            "i 0.24925976699999486 \t 0.16666666666666666\n",
            "i 0.24925976799999486 \t 0.16666666666666666\n",
            "i 0.24925976899999486 \t 0.16666666666666666\n",
            "i 0.24925976999999486 \t 0.16666666666666666\n",
            "i 0.24925977099999486 \t 0.16666666666666666\n",
            "i 0.24925977199999486 \t 0.16666666666666666\n",
            "i 0.24925977299999486 \t 0.16666666666666666\n",
            "i 0.24925977399999485 \t 0.16666666666666666\n",
            "i 0.24925977499999485 \t 0.16666666666666666\n",
            "i 0.24925977599999485 \t 0.16666666666666666\n",
            "i 0.24925977699999485 \t 0.16666666666666666\n",
            "i 0.24925977799999485 \t 0.16666666666666666\n",
            "i 0.24925977899999485 \t 0.16666666666666666\n",
            "i 0.24925977999999485 \t 0.16666666666666666\n",
            "i 0.24925978099999485 \t 0.16666666666666666\n",
            "i 0.24925978199999485 \t 0.16666666666666666\n",
            "i 0.24925978299999485 \t 0.16666666666666666\n",
            "i 0.24925978399999485 \t 0.16666666666666666\n",
            "i 0.24925978499999485 \t 0.16666666666666666\n",
            "i 0.24925978599999485 \t 0.16666666666666666\n",
            "i 0.24925978699999485 \t 0.16666666666666666\n",
            "i 0.24925978799999485 \t 0.16666666666666666\n",
            "i 0.24925978899999485 \t 0.16666666666666666\n",
            "i 0.24925978999999485 \t 0.16666666666666666\n",
            "i 0.24925979099999485 \t 0.16666666666666666\n",
            "i 0.24925979199999485 \t 0.16666666666666666\n",
            "i 0.24925979299999484 \t 0.16666666666666666\n",
            "i 0.24925979399999484 \t 0.16666666666666666\n",
            "i 0.24925979499999484 \t 0.16666666666666666\n",
            "i 0.24925979599999484 \t 0.16666666666666666\n",
            "i 0.24925979699999484 \t 0.16666666666666666\n",
            "i 0.24925979799999484 \t 0.16666666666666666\n",
            "i 0.24925979899999484 \t 0.16666666666666666\n",
            "i 0.24925979999999484 \t 0.16666666666666666\n",
            "i 0.24925980099999484 \t 0.16666666666666666\n",
            "i 0.24925980199999484 \t 0.16666666666666666\n",
            "i 0.24925980299999484 \t 0.16666666666666666\n",
            "i 0.24925980399999484 \t 0.16666666666666666\n",
            "i 0.24925980499999484 \t 0.16666666666666666\n",
            "i 0.24925980599999484 \t 0.16666666666666666\n",
            "i 0.24925980699999484 \t 0.16666666666666666\n",
            "i 0.24925980799999484 \t 0.16666666666666666\n",
            "i 0.24925980899999484 \t 0.16666666666666666\n",
            "i 0.24925980999999484 \t 0.16666666666666666\n",
            "i 0.24925981099999484 \t 0.16666666666666666\n",
            "i 0.24925981199999483 \t 0.16666666666666666\n",
            "i 0.24925981299999483 \t 0.16666666666666666\n",
            "i 0.24925981399999483 \t 0.16666666666666666\n",
            "i 0.24925981499999483 \t 0.16666666666666666\n",
            "i 0.24925981599999483 \t 0.16666666666666666\n",
            "i 0.24925981699999483 \t 0.16666666666666666\n",
            "i 0.24925981799999483 \t 0.16666666666666666\n",
            "i 0.24925981899999483 \t 0.16666666666666666\n",
            "i 0.24925981999999483 \t 0.16666666666666666\n",
            "i 0.24925982099999483 \t 0.16666666666666666\n",
            "i 0.24925982199999483 \t 0.16666666666666666\n",
            "i 0.24925982299999483 \t 0.16666666666666666\n",
            "i 0.24925982399999483 \t 0.16666666666666666\n",
            "i 0.24925982499999483 \t 0.16666666666666666\n",
            "i 0.24925982599999483 \t 0.16666666666666666\n",
            "i 0.24925982699999483 \t 0.16666666666666666\n",
            "i 0.24925982799999483 \t 0.16666666666666666\n",
            "i 0.24925982899999483 \t 0.16666666666666666\n",
            "i 0.24925982999999483 \t 0.16666666666666666\n",
            "i 0.24925983099999482 \t 0.16666666666666666\n",
            "i 0.24925983199999482 \t 0.16666666666666666\n",
            "i 0.24925983299999482 \t 0.16666666666666666\n",
            "i 0.24925983399999482 \t 0.16666666666666666\n",
            "i 0.24925983499999482 \t 0.16666666666666666\n",
            "i 0.24925983599999482 \t 0.16666666666666666\n",
            "i 0.24925983699999482 \t 0.16666666666666666\n",
            "i 0.24925983799999482 \t 0.16666666666666666\n",
            "i 0.24925983899999482 \t 0.16666666666666666\n",
            "i 0.24925983999999482 \t 0.16666666666666666\n",
            "i 0.24925984099999482 \t 0.16666666666666666\n",
            "i 0.24925984199999482 \t 0.16666666666666666\n",
            "i 0.24925984299999482 \t 0.16666666666666666\n",
            "i 0.24925984399999482 \t 0.16666666666666666\n",
            "i 0.24925984499999482 \t 0.16666666666666666\n",
            "i 0.24925984599999482 \t 0.16666666666666666\n",
            "i 0.24925984699999482 \t 0.16666666666666666\n",
            "i 0.24925984799999482 \t 0.16666666666666666\n",
            "i 0.24925984899999482 \t 0.16666666666666666\n",
            "i 0.24925984999999481 \t 0.16666666666666666\n",
            "i 0.24925985099999481 \t 0.16666666666666666\n",
            "i 0.2492598519999948 \t 0.16666666666666666\n",
            "i 0.2492598529999948 \t 0.16666666666666666\n",
            "i 0.2492598539999948 \t 0.16666666666666666\n",
            "i 0.2492598549999948 \t 0.16666666666666666\n",
            "i 0.2492598559999948 \t 0.16666666666666666\n",
            "i 0.2492598569999948 \t 0.16666666666666666\n",
            "i 0.2492598579999948 \t 0.16666666666666666\n",
            "i 0.2492598589999948 \t 0.16666666666666666\n",
            "i 0.2492598599999948 \t 0.16666666666666666\n",
            "i 0.2492598609999948 \t 0.16666666666666666\n",
            "i 0.2492598619999948 \t 0.16666666666666666\n",
            "i 0.2492598629999948 \t 0.16666666666666666\n",
            "i 0.2492598639999948 \t 0.16666666666666666\n",
            "i 0.2492598649999948 \t 0.16666666666666666\n",
            "i 0.2492598659999948 \t 0.16666666666666666\n",
            "i 0.2492598669999948 \t 0.16666666666666666\n",
            "i 0.2492598679999948 \t 0.16666666666666666\n",
            "i 0.2492598689999948 \t 0.16666666666666666\n",
            "i 0.2492598699999948 \t 0.16666666666666666\n",
            "i 0.2492598709999948 \t 0.16666666666666666\n",
            "i 0.2492598719999948 \t 0.16666666666666666\n",
            "i 0.2492598729999948 \t 0.16666666666666666\n",
            "i 0.2492598739999948 \t 0.16666666666666666\n",
            "i 0.2492598749999948 \t 0.16666666666666666\n",
            "i 0.2492598759999948 \t 0.16666666666666666\n",
            "i 0.2492598769999948 \t 0.16666666666666666\n",
            "i 0.2492598779999948 \t 0.16666666666666666\n",
            "i 0.2492598789999948 \t 0.16666666666666666\n",
            "i 0.2492598799999948 \t 0.16666666666666666\n",
            "i 0.2492598809999948 \t 0.16666666666666666\n",
            "i 0.2492598819999948 \t 0.16666666666666666\n",
            "i 0.2492598829999948 \t 0.16666666666666666\n",
            "i 0.2492598839999948 \t 0.16666666666666666\n",
            "i 0.2492598849999948 \t 0.16666666666666666\n",
            "i 0.2492598859999948 \t 0.16666666666666666\n",
            "i 0.2492598869999948 \t 0.16666666666666666\n",
            "i 0.2492598879999948 \t 0.16666666666666666\n",
            "i 0.2492598889999948 \t 0.16666666666666666\n",
            "i 0.2492598899999948 \t 0.16666666666666666\n",
            "i 0.2492598909999948 \t 0.16666666666666666\n",
            "i 0.2492598919999948 \t 0.16666666666666666\n",
            "i 0.2492598929999948 \t 0.16666666666666666\n",
            "i 0.2492598939999948 \t 0.16666666666666666\n",
            "i 0.2492598949999948 \t 0.16666666666666666\n",
            "i 0.2492598959999948 \t 0.16666666666666666\n",
            "i 0.2492598969999948 \t 0.16666666666666666\n",
            "i 0.2492598979999948 \t 0.16666666666666666\n",
            "i 0.2492598989999948 \t 0.16666666666666666\n",
            "i 0.2492598999999948 \t 0.16666666666666666\n",
            "i 0.2492599009999948 \t 0.16666666666666666\n",
            "i 0.2492599019999948 \t 0.16666666666666666\n",
            "i 0.2492599029999948 \t 0.16666666666666666\n",
            "i 0.2492599039999948 \t 0.16666666666666666\n",
            "i 0.24925990499999479 \t 0.16666666666666666\n",
            "i 0.24925990599999479 \t 0.16666666666666666\n",
            "i 0.24925990699999478 \t 0.16666666666666666\n",
            "i 0.24925990799999478 \t 0.16666666666666666\n",
            "i 0.24925990899999478 \t 0.16666666666666666\n",
            "i 0.24925990999999478 \t 0.16666666666666666\n",
            "i 0.24925991099999478 \t 0.16666666666666666\n",
            "i 0.24925991199999478 \t 0.16666666666666666\n",
            "i 0.24925991299999478 \t 0.16666666666666666\n",
            "i 0.24925991399999478 \t 0.16666666666666666\n",
            "i 0.24925991499999478 \t 0.16666666666666666\n",
            "i 0.24925991599999478 \t 0.16666666666666666\n",
            "i 0.24925991699999478 \t 0.16666666666666666\n",
            "i 0.24925991799999478 \t 0.16666666666666666\n",
            "i 0.24925991899999478 \t 0.16666666666666666\n",
            "i 0.24925991999999478 \t 0.16666666666666666\n",
            "i 0.24925992099999478 \t 0.16666666666666666\n",
            "i 0.24925992199999478 \t 0.16666666666666666\n",
            "i 0.24925992299999478 \t 0.16666666666666666\n",
            "i 0.24925992399999478 \t 0.16666666666666666\n",
            "i 0.24925992499999478 \t 0.16666666666666666\n",
            "i 0.24925992599999477 \t 0.16666666666666666\n",
            "i 0.24925992699999477 \t 0.16666666666666666\n",
            "i 0.24925992799999477 \t 0.16666666666666666\n",
            "i 0.24925992899999477 \t 0.16666666666666666\n",
            "i 0.24925992999999477 \t 0.16666666666666666\n",
            "i 0.24925993099999477 \t 0.16666666666666666\n",
            "i 0.24925993199999477 \t 0.16666666666666666\n",
            "i 0.24925993299999477 \t 0.16666666666666666\n",
            "i 0.24925993399999477 \t 0.16666666666666666\n",
            "i 0.24925993499999477 \t 0.16666666666666666\n",
            "i 0.24925993599999477 \t 0.16666666666666666\n",
            "i 0.24925993699999477 \t 0.16666666666666666\n",
            "i 0.24925993799999477 \t 0.16666666666666666\n",
            "i 0.24925993899999477 \t 0.16666666666666666\n",
            "i 0.24925993999999477 \t 0.16666666666666666\n",
            "i 0.24925994099999477 \t 0.16666666666666666\n",
            "i 0.24925994199999477 \t 0.16666666666666666\n",
            "i 0.24925994299999477 \t 0.16666666666666666\n",
            "i 0.24925994399999477 \t 0.16666666666666666\n",
            "i 0.24925994499999476 \t 0.16666666666666666\n",
            "i 0.24925994599999476 \t 0.16666666666666666\n",
            "i 0.24925994699999476 \t 0.16666666666666666\n",
            "i 0.24925994799999476 \t 0.16666666666666666\n",
            "i 0.24925994899999476 \t 0.16666666666666666\n",
            "i 0.24925994999999476 \t 0.16666666666666666\n",
            "i 0.24925995099999476 \t 0.16666666666666666\n",
            "i 0.24925995199999476 \t 0.16666666666666666\n",
            "i 0.24925995299999476 \t 0.16666666666666666\n",
            "i 0.24925995399999476 \t 0.16666666666666666\n",
            "i 0.24925995499999476 \t 0.16666666666666666\n",
            "i 0.24925995599999476 \t 0.16666666666666666\n",
            "i 0.24925995699999476 \t 0.16666666666666666\n",
            "i 0.24925995799999476 \t 0.16666666666666666\n",
            "i 0.24925995899999476 \t 0.16666666666666666\n",
            "i 0.24925995999999476 \t 0.16666666666666666\n",
            "i 0.24925996099999476 \t 0.16666666666666666\n",
            "i 0.24925996199999476 \t 0.16666666666666666\n",
            "i 0.24925996299999476 \t 0.16666666666666666\n",
            "i 0.24925996399999475 \t 0.16666666666666666\n",
            "i 0.24925996499999475 \t 0.16666666666666666\n",
            "i 0.24925996599999475 \t 0.16666666666666666\n",
            "i 0.24925996699999475 \t 0.16666666666666666\n",
            "i 0.24925996799999475 \t 0.16666666666666666\n",
            "i 0.24925996899999475 \t 0.16666666666666666\n",
            "i 0.24925996999999475 \t 0.16666666666666666\n",
            "i 0.24925997099999475 \t 0.16666666666666666\n",
            "i 0.24925997199999475 \t 0.16666666666666666\n",
            "i 0.24925997299999475 \t 0.16666666666666666\n",
            "i 0.24925997399999475 \t 0.16666666666666666\n",
            "i 0.24925997499999475 \t 0.16666666666666666\n",
            "i 0.24925997599999475 \t 0.16666666666666666\n",
            "i 0.24925997699999475 \t 0.16666666666666666\n",
            "i 0.24925997799999475 \t 0.16666666666666666\n",
            "i 0.24925997899999475 \t 0.16666666666666666\n",
            "i 0.24925997999999475 \t 0.16666666666666666\n",
            "i 0.24925998099999475 \t 0.16666666666666666\n",
            "i 0.24925998199999475 \t 0.16666666666666666\n",
            "i 0.24925998299999474 \t 0.16666666666666666\n",
            "i 0.24925998399999474 \t 0.16666666666666666\n",
            "i 0.24925998499999474 \t 0.16666666666666666\n",
            "i 0.24925998599999474 \t 0.16666666666666666\n",
            "i 0.24925998699999474 \t 0.16666666666666666\n",
            "i 0.24925998799999474 \t 0.16666666666666666\n",
            "i 0.24925998899999474 \t 0.16666666666666666\n",
            "i 0.24925998999999474 \t 0.16666666666666666\n",
            "i 0.24925999099999474 \t 0.16666666666666666\n",
            "i 0.24925999199999474 \t 0.16666666666666666\n",
            "i 0.24925999299999474 \t 0.16666666666666666\n",
            "i 0.24925999399999474 \t 0.16666666666666666\n",
            "i 0.24925999499999474 \t 0.16666666666666666\n",
            "i 0.24925999599999474 \t 0.16666666666666666\n",
            "i 0.24925999699999474 \t 0.16666666666666666\n",
            "i 0.24925999799999474 \t 0.16666666666666666\n",
            "i 0.24925999899999474 \t 0.16666666666666666\n",
            "i 0.24925999999999474 \t 0.16666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cJKMyCJX1HO7",
        "outputId": "a7ef21db-896a-4ce8-f869-ec808cb74de3"
      },
      "source": [
        "ind=0.01\n",
        "#ecdf2 = ECDF(samplecdf[13])\n",
        "#pyplot.plot(ecdf2.x,ecdf2.y)\n",
        "lambdastar=1000\n",
        "distmax=1000#infinity\n",
        "#abc1=ecdfbest.copy()\n",
        "#ecdfbest=np.array(ecdfbest)\n",
        "#ecdfworst=np.array(ecdfworst)clienttrainingloss\n",
        "ecdf2 = ECDF(samplecdf[54])\n",
        "#abc1.sort()\n",
        "F1=ecdf2(samplecdf[54])\n",
        "\n",
        "clienttestingloss=[]\n",
        "for ind2 in range(1,len(samplecdf1),2):\n",
        "  clienttestingloss.append(samplecdf1[ind2])\n",
        "clienttestingloss=np.array(clienttestingloss)\n",
        "for ind1 in range(0,10000):\n",
        "  #abc2=ecdfworst.copy()\n",
        "  #print(abc2[13])\n",
        "  #abc2.pop(9)\n",
        "  ind=ind+0.01\n",
        "  ecdf4 = ECDF(clienttestingloss)\n",
        "  #abc2.sort()\n",
        "  F2=ecdf4(clienttestingloss)/ecdf4(ind)\n",
        "  for indd in range(len(F2)):\n",
        "    if F2[indd]>1:\n",
        "      F2[indd]=1\n",
        "  ksdist,ksvalue = ks_2samp(F1,F2)\n",
        "  print(\"i\",ind,\"\\t\",ksdist)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i 0.02 \t 0.999\n",
            "i 0.03 \t 0.999\n",
            "i 0.04 \t 0.999\n",
            "i 0.05 \t 0.999\n",
            "i 0.060000000000000005 \t 0.9906666666666667\n",
            "i 0.07 \t 0.9406666666666667\n",
            "i 0.08 \t 0.924\n",
            "i 0.09 \t 0.799\n",
            "i 0.09999999999999999 \t 0.6656666666666666\n",
            "i 0.10999999999999999 \t 0.524\n",
            "i 0.11999999999999998 \t 0.39066666666666666\n",
            "i 0.12999999999999998 \t 0.274\n",
            "i 0.13999999999999999 \t 0.20733333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i 0.15 \t 0.14066666666666666\n",
            "i 0.16 \t 0.11566666666666667\n",
            "i 0.17 \t 0.099\n",
            "i 0.18000000000000002 \t 0.099\n",
            "i 0.19000000000000003 \t 0.09066666666666667\n",
            "i 0.20000000000000004 \t 0.08333333333333333\n",
            "i 0.21000000000000005 \t 0.08333333333333333\n",
            "i 0.22000000000000006 \t 0.08333333333333333\n",
            "i 0.23000000000000007 \t 0.08333333333333333\n",
            "i 0.24000000000000007 \t 0.08333333333333333\n",
            "i 0.25000000000000006 \t 0.08333333333333333\n",
            "i 0.26000000000000006 \t 0.08333333333333333\n",
            "i 0.2700000000000001 \t 0.08333333333333333\n",
            "i 0.2800000000000001 \t 0.08333333333333333\n",
            "i 0.2900000000000001 \t 0.08333333333333333\n",
            "i 0.3000000000000001 \t 0.08333333333333333\n",
            "i 0.3100000000000001 \t 0.08333333333333333\n",
            "i 0.3200000000000001 \t 0.08333333333333333\n",
            "i 0.3300000000000001 \t 0.08333333333333333\n",
            "i 0.34000000000000014 \t 0.08333333333333333\n",
            "i 0.35000000000000014 \t 0.08333333333333333\n",
            "i 0.36000000000000015 \t 0.08333333333333333\n",
            "i 0.37000000000000016 \t 0.08333333333333333\n",
            "i 0.38000000000000017 \t 0.08333333333333333\n",
            "i 0.3900000000000002 \t 0.08333333333333333\n",
            "i 0.4000000000000002 \t 0.08333333333333333\n",
            "i 0.4100000000000002 \t 0.08333333333333333\n",
            "i 0.4200000000000002 \t 0.08333333333333333\n",
            "i 0.4300000000000002 \t 0.09166666666666666\n",
            "i 0.4400000000000002 \t 0.09166666666666666\n",
            "i 0.45000000000000023 \t 0.09166666666666666\n",
            "i 0.46000000000000024 \t 0.09166666666666666\n",
            "i 0.47000000000000025 \t 0.09166666666666666\n",
            "i 0.48000000000000026 \t 0.09166666666666666\n",
            "i 0.49000000000000027 \t 0.09166666666666666\n",
            "i 0.5000000000000002 \t 0.09166666666666666\n",
            "i 0.5100000000000002 \t 0.09166666666666666\n",
            "i 0.5200000000000002 \t 0.09166666666666666\n",
            "i 0.5300000000000002 \t 0.09166666666666666\n",
            "i 0.5400000000000003 \t 0.09166666666666666\n",
            "i 0.5500000000000003 \t 0.09166666666666666\n",
            "i 0.5600000000000003 \t 0.09166666666666666\n",
            "i 0.5700000000000003 \t 0.09166666666666666\n",
            "i 0.5800000000000003 \t 0.09166666666666666\n",
            "i 0.5900000000000003 \t 0.09166666666666666\n",
            "i 0.6000000000000003 \t 0.09166666666666666\n",
            "i 0.6100000000000003 \t 0.09166666666666666\n",
            "i 0.6200000000000003 \t 0.09166666666666666\n",
            "i 0.6300000000000003 \t 0.09166666666666666\n",
            "i 0.6400000000000003 \t 0.09166666666666666\n",
            "i 0.6500000000000004 \t 0.09166666666666666\n",
            "i 0.6600000000000004 \t 0.09166666666666666\n",
            "i 0.6700000000000004 \t 0.09166666666666666\n",
            "i 0.6800000000000004 \t 0.09166666666666666\n",
            "i 0.6900000000000004 \t 0.09166666666666666\n",
            "i 0.7000000000000004 \t 0.09166666666666666\n",
            "i 0.7100000000000004 \t 0.09166666666666666\n",
            "i 0.7200000000000004 \t 0.09166666666666666\n",
            "i 0.7300000000000004 \t 0.09166666666666666\n",
            "i 0.7400000000000004 \t 0.09166666666666666\n",
            "i 0.7500000000000004 \t 0.09166666666666666\n",
            "i 0.7600000000000005 \t 0.09166666666666666\n",
            "i 0.7700000000000005 \t 0.09166666666666666\n",
            "i 0.7800000000000005 \t 0.09166666666666666\n",
            "i 0.7900000000000005 \t 0.09166666666666666\n",
            "i 0.8000000000000005 \t 0.09166666666666666\n",
            "i 0.8100000000000005 \t 0.09166666666666666\n",
            "i 0.8200000000000005 \t 0.09166666666666666\n",
            "i 0.8300000000000005 \t 0.09166666666666666\n",
            "i 0.8400000000000005 \t 0.09166666666666666\n",
            "i 0.8500000000000005 \t 0.09166666666666666\n",
            "i 0.8600000000000005 \t 0.09166666666666666\n",
            "i 0.8700000000000006 \t 0.09166666666666666\n",
            "i 0.8800000000000006 \t 0.09166666666666666\n",
            "i 0.8900000000000006 \t 0.09166666666666666\n",
            "i 0.9000000000000006 \t 0.09166666666666666\n",
            "i 0.9100000000000006 \t 0.09166666666666666\n",
            "i 0.9200000000000006 \t 0.09166666666666666\n",
            "i 0.9300000000000006 \t 0.09166666666666666\n",
            "i 0.9400000000000006 \t 0.09166666666666666\n",
            "i 0.9500000000000006 \t 0.09166666666666666\n",
            "i 0.9600000000000006 \t 0.09166666666666666\n",
            "i 0.9700000000000006 \t 0.09166666666666666\n",
            "i 0.9800000000000006 \t 0.09166666666666666\n",
            "i 0.9900000000000007 \t 0.09166666666666666\n",
            "i 1.0000000000000007 \t 0.09166666666666666\n",
            "i 1.0100000000000007 \t 0.09166666666666666\n",
            "i 1.0200000000000007 \t 0.09166666666666666\n",
            "i 1.0300000000000007 \t 0.09166666666666666\n",
            "i 1.0400000000000007 \t 0.09166666666666666\n",
            "i 1.0500000000000007 \t 0.09166666666666666\n",
            "i 1.0600000000000007 \t 0.09166666666666666\n",
            "i 1.0700000000000007 \t 0.09166666666666666\n",
            "i 1.0800000000000007 \t 0.09166666666666666\n",
            "i 1.0900000000000007 \t 0.09166666666666666\n",
            "i 1.1000000000000008 \t 0.09166666666666666\n",
            "i 1.1100000000000008 \t 0.09166666666666666\n",
            "i 1.1200000000000008 \t 0.09166666666666666\n",
            "i 1.1300000000000008 \t 0.09166666666666666\n",
            "i 1.1400000000000008 \t 0.09166666666666666\n",
            "i 1.1500000000000008 \t 0.09166666666666666\n",
            "i 1.1600000000000008 \t 0.09166666666666666\n",
            "i 1.1700000000000008 \t 0.09166666666666666\n",
            "i 1.1800000000000008 \t 0.09166666666666666\n",
            "i 1.1900000000000008 \t 0.09166666666666666\n",
            "i 1.2000000000000008 \t 0.09166666666666666\n",
            "i 1.2100000000000009 \t 0.09166666666666666\n",
            "i 1.2200000000000009 \t 0.09166666666666666\n",
            "i 1.2300000000000009 \t 0.09166666666666666\n",
            "i 1.2400000000000009 \t 0.09166666666666666\n",
            "i 1.2500000000000009 \t 0.09166666666666666\n",
            "i 1.260000000000001 \t 0.09166666666666666\n",
            "i 1.270000000000001 \t 0.09166666666666666\n",
            "i 1.280000000000001 \t 0.09166666666666666\n",
            "i 1.290000000000001 \t 0.09166666666666666\n",
            "i 1.300000000000001 \t 0.09166666666666666\n",
            "i 1.310000000000001 \t 0.09166666666666666\n",
            "i 1.320000000000001 \t 0.09166666666666666\n",
            "i 1.330000000000001 \t 0.09166666666666666\n",
            "i 1.340000000000001 \t 0.09166666666666666\n",
            "i 1.350000000000001 \t 0.09166666666666666\n",
            "i 1.360000000000001 \t 0.09166666666666666\n",
            "i 1.370000000000001 \t 0.09166666666666666\n",
            "i 1.380000000000001 \t 0.09166666666666666\n",
            "i 1.390000000000001 \t 0.09166666666666666\n",
            "i 1.400000000000001 \t 0.09166666666666666\n",
            "i 1.410000000000001 \t 0.09166666666666666\n",
            "i 1.420000000000001 \t 0.09166666666666666\n",
            "i 1.430000000000001 \t 0.09166666666666666\n",
            "i 1.440000000000001 \t 0.09166666666666666\n",
            "i 1.450000000000001 \t 0.09166666666666666\n",
            "i 1.460000000000001 \t 0.09166666666666666\n",
            "i 1.470000000000001 \t 0.09166666666666666\n",
            "i 1.480000000000001 \t 0.09166666666666666\n",
            "i 1.490000000000001 \t 0.09166666666666666\n",
            "i 1.500000000000001 \t 0.09166666666666666\n",
            "i 1.5100000000000011 \t 0.09166666666666666\n",
            "i 1.5200000000000011 \t 0.09166666666666666\n",
            "i 1.5300000000000011 \t 0.09166666666666666\n",
            "i 1.5400000000000011 \t 0.09166666666666666\n",
            "i 1.5500000000000012 \t 0.09166666666666666\n",
            "i 1.5600000000000012 \t 0.09166666666666666\n",
            "i 1.5700000000000012 \t 0.09166666666666666\n",
            "i 1.5800000000000012 \t 0.09166666666666666\n",
            "i 1.5900000000000012 \t 0.09166666666666666\n",
            "i 1.6000000000000012 \t 0.09166666666666666\n",
            "i 1.6100000000000012 \t 0.09166666666666666\n",
            "i 1.6200000000000012 \t 0.09166666666666666\n",
            "i 1.6300000000000012 \t 0.09166666666666666\n",
            "i 1.6400000000000012 \t 0.09166666666666666\n",
            "i 1.6500000000000012 \t 0.09166666666666666\n",
            "i 1.6600000000000013 \t 0.09166666666666666\n",
            "i 1.6700000000000013 \t 0.09166666666666666\n",
            "i 1.6800000000000013 \t 0.09166666666666666\n",
            "i 1.6900000000000013 \t 0.09166666666666666\n",
            "i 1.7000000000000013 \t 0.09166666666666666\n",
            "i 1.7100000000000013 \t 0.09166666666666666\n",
            "i 1.7200000000000013 \t 0.09166666666666666\n",
            "i 1.7300000000000013 \t 0.09166666666666666\n",
            "i 1.7400000000000013 \t 0.09166666666666666\n",
            "i 1.7500000000000013 \t 0.09166666666666666\n",
            "i 1.7600000000000013 \t 0.09166666666666666\n",
            "i 1.7700000000000014 \t 0.09166666666666666\n",
            "i 1.7800000000000014 \t 0.09166666666666666\n",
            "i 1.7900000000000014 \t 0.09166666666666666\n",
            "i 1.8000000000000014 \t 0.09166666666666666\n",
            "i 1.8100000000000014 \t 0.09166666666666666\n",
            "i 1.8200000000000014 \t 0.09166666666666666\n",
            "i 1.8300000000000014 \t 0.09166666666666666\n",
            "i 1.8400000000000014 \t 0.09166666666666666\n",
            "i 1.8500000000000014 \t 0.09166666666666666\n",
            "i 1.8600000000000014 \t 0.09166666666666666\n",
            "i 1.8700000000000014 \t 0.09166666666666666\n",
            "i 1.8800000000000014 \t 0.09166666666666666\n",
            "i 1.8900000000000015 \t 0.09166666666666666\n",
            "i 1.9000000000000015 \t 0.09166666666666666\n",
            "i 1.9100000000000015 \t 0.09166666666666666\n",
            "i 1.9200000000000015 \t 0.09166666666666666\n",
            "i 1.9300000000000015 \t 0.09166666666666666\n",
            "i 1.9400000000000015 \t 0.09166666666666666\n",
            "i 1.9500000000000015 \t 0.09166666666666666\n",
            "i 1.9600000000000015 \t 0.09166666666666666\n",
            "i 1.9700000000000015 \t 0.09166666666666666\n",
            "i 1.9800000000000015 \t 0.09166666666666666\n",
            "i 1.9900000000000015 \t 0.09166666666666666\n",
            "i 2.0000000000000013 \t 0.09166666666666666\n",
            "i 2.010000000000001 \t 0.09166666666666666\n",
            "i 2.020000000000001 \t 0.09166666666666666\n",
            "i 2.0300000000000007 \t 0.09166666666666666\n",
            "i 2.0400000000000005 \t 0.09166666666666666\n",
            "i 2.0500000000000003 \t 0.09166666666666666\n",
            "i 2.06 \t 0.09166666666666666\n",
            "i 2.07 \t 0.09166666666666666\n",
            "i 2.0799999999999996 \t 0.09166666666666666\n",
            "i 2.0899999999999994 \t 0.09166666666666666\n",
            "i 2.099999999999999 \t 0.09166666666666666\n",
            "i 2.109999999999999 \t 0.09166666666666666\n",
            "i 2.1199999999999988 \t 0.09166666666666666\n",
            "i 2.1299999999999986 \t 0.09166666666666666\n",
            "i 2.1399999999999983 \t 0.09166666666666666\n",
            "i 2.149999999999998 \t 0.09166666666666666\n",
            "i 2.159999999999998 \t 0.09166666666666666\n",
            "i 2.1699999999999977 \t 0.09166666666666666\n",
            "i 2.1799999999999975 \t 0.09166666666666666\n",
            "i 2.1899999999999973 \t 0.09166666666666666\n",
            "i 2.199999999999997 \t 0.09166666666666666\n",
            "i 2.209999999999997 \t 0.09166666666666666\n",
            "i 2.2199999999999966 \t 0.09166666666666666\n",
            "i 2.2299999999999964 \t 0.09166666666666666\n",
            "i 2.239999999999996 \t 0.09166666666666666\n",
            "i 2.249999999999996 \t 0.09166666666666666\n",
            "i 2.259999999999996 \t 0.09166666666666666\n",
            "i 2.2699999999999956 \t 0.09166666666666666\n",
            "i 2.2799999999999954 \t 0.09166666666666666\n",
            "i 2.289999999999995 \t 0.09166666666666666\n",
            "i 2.299999999999995 \t 0.09166666666666666\n",
            "i 2.3099999999999947 \t 0.09166666666666666\n",
            "i 2.3199999999999945 \t 0.09166666666666666\n",
            "i 2.3299999999999943 \t 0.09166666666666666\n",
            "i 2.339999999999994 \t 0.09166666666666666\n",
            "i 2.349999999999994 \t 0.09166666666666666\n",
            "i 2.3599999999999937 \t 0.09166666666666666\n",
            "i 2.3699999999999934 \t 0.09166666666666666\n",
            "i 2.3799999999999932 \t 0.09166666666666666\n",
            "i 2.389999999999993 \t 0.09166666666666666\n",
            "i 2.399999999999993 \t 0.09166666666666666\n",
            "i 2.4099999999999926 \t 0.09166666666666666\n",
            "i 2.4199999999999924 \t 0.09166666666666666\n",
            "i 2.429999999999992 \t 0.09166666666666666\n",
            "i 2.439999999999992 \t 0.09166666666666666\n",
            "i 2.4499999999999917 \t 0.09166666666666666\n",
            "i 2.4599999999999915 \t 0.09166666666666666\n",
            "i 2.4699999999999913 \t 0.09166666666666666\n",
            "i 2.479999999999991 \t 0.09166666666666666\n",
            "i 2.489999999999991 \t 0.09166666666666666\n",
            "i 2.4999999999999907 \t 0.09166666666666666\n",
            "i 2.5099999999999905 \t 0.09166666666666666\n",
            "i 2.5199999999999902 \t 0.09166666666666666\n",
            "i 2.52999999999999 \t 0.09166666666666666\n",
            "i 2.53999999999999 \t 0.09166666666666666\n",
            "i 2.5499999999999896 \t 0.09166666666666666\n",
            "i 2.5599999999999894 \t 0.09166666666666666\n",
            "i 2.569999999999989 \t 0.09166666666666666\n",
            "i 2.579999999999989 \t 0.09166666666666666\n",
            "i 2.5899999999999888 \t 0.09166666666666666\n",
            "i 2.5999999999999885 \t 0.09166666666666666\n",
            "i 2.6099999999999883 \t 0.09166666666666666\n",
            "i 2.619999999999988 \t 0.09166666666666666\n",
            "i 2.629999999999988 \t 0.09166666666666666\n",
            "i 2.6399999999999877 \t 0.09166666666666666\n",
            "i 2.6499999999999875 \t 0.09166666666666666\n",
            "i 2.6599999999999873 \t 0.09166666666666666\n",
            "i 2.669999999999987 \t 0.09166666666666666\n",
            "i 2.679999999999987 \t 0.09166666666666666\n",
            "i 2.6899999999999866 \t 0.09166666666666666\n",
            "i 2.6999999999999864 \t 0.09166666666666666\n",
            "i 2.709999999999986 \t 0.09166666666666666\n",
            "i 2.719999999999986 \t 0.09166666666666666\n",
            "i 2.7299999999999858 \t 0.09166666666666666\n",
            "i 2.7399999999999856 \t 0.09166666666666666\n",
            "i 2.7499999999999853 \t 0.09166666666666666\n",
            "i 2.759999999999985 \t 0.09166666666666666\n",
            "i 2.769999999999985 \t 0.09166666666666666\n",
            "i 2.7799999999999847 \t 0.09166666666666666\n",
            "i 2.7899999999999845 \t 0.09166666666666666\n",
            "i 2.7999999999999843 \t 0.09166666666666666\n",
            "i 2.809999999999984 \t 0.09166666666666666\n",
            "i 2.819999999999984 \t 0.09166666666666666\n",
            "i 2.8299999999999836 \t 0.09166666666666666\n",
            "i 2.8399999999999834 \t 0.09166666666666666\n",
            "i 2.849999999999983 \t 0.09166666666666666\n",
            "i 2.859999999999983 \t 0.09166666666666666\n",
            "i 2.869999999999983 \t 0.09166666666666666\n",
            "i 2.8799999999999826 \t 0.09166666666666666\n",
            "i 2.8899999999999824 \t 0.09166666666666666\n",
            "i 2.899999999999982 \t 0.09166666666666666\n",
            "i 2.909999999999982 \t 0.09166666666666666\n",
            "i 2.9199999999999817 \t 0.09166666666666666\n",
            "i 2.9299999999999815 \t 0.09166666666666666\n",
            "i 2.9399999999999813 \t 0.09166666666666666\n",
            "i 2.949999999999981 \t 0.09166666666666666\n",
            "i 2.959999999999981 \t 0.09166666666666666\n",
            "i 2.9699999999999807 \t 0.09166666666666666\n",
            "i 2.9799999999999804 \t 0.09166666666666666\n",
            "i 2.9899999999999802 \t 0.09166666666666666\n",
            "i 2.99999999999998 \t 0.09166666666666666\n",
            "i 3.00999999999998 \t 0.09166666666666666\n",
            "i 3.0199999999999796 \t 0.09166666666666666\n",
            "i 3.0299999999999794 \t 0.09166666666666666\n",
            "i 3.039999999999979 \t 0.09166666666666666\n",
            "i 3.049999999999979 \t 0.09166666666666666\n",
            "i 3.0599999999999787 \t 0.09166666666666666\n",
            "i 3.0699999999999785 \t 0.09166666666666666\n",
            "i 3.0799999999999783 \t 0.09166666666666666\n",
            "i 3.089999999999978 \t 0.09166666666666666\n",
            "i 3.099999999999978 \t 0.09166666666666666\n",
            "i 3.1099999999999777 \t 0.09166666666666666\n",
            "i 3.1199999999999775 \t 0.09166666666666666\n",
            "i 3.1299999999999772 \t 0.09166666666666666\n",
            "i 3.139999999999977 \t 0.09166666666666666\n",
            "i 3.149999999999977 \t 0.09166666666666666\n",
            "i 3.1599999999999766 \t 0.09166666666666666\n",
            "i 3.1699999999999764 \t 0.09166666666666666\n",
            "i 3.179999999999976 \t 0.09166666666666666\n",
            "i 3.189999999999976 \t 0.09166666666666666\n",
            "i 3.1999999999999758 \t 0.09166666666666666\n",
            "i 3.2099999999999755 \t 0.09166666666666666\n",
            "i 3.2199999999999753 \t 0.09166666666666666\n",
            "i 3.229999999999975 \t 0.09166666666666666\n",
            "i 3.239999999999975 \t 0.09166666666666666\n",
            "i 3.2499999999999747 \t 0.09166666666666666\n",
            "i 3.2599999999999745 \t 0.09166666666666666\n",
            "i 3.2699999999999743 \t 0.09166666666666666\n",
            "i 3.279999999999974 \t 0.09166666666666666\n",
            "i 3.289999999999974 \t 0.09166666666666666\n",
            "i 3.2999999999999736 \t 0.09166666666666666\n",
            "i 3.3099999999999734 \t 0.09166666666666666\n",
            "i 3.319999999999973 \t 0.09166666666666666\n",
            "i 3.329999999999973 \t 0.09166666666666666\n",
            "i 3.3399999999999728 \t 0.09166666666666666\n",
            "i 3.3499999999999726 \t 0.09166666666666666\n",
            "i 3.3599999999999723 \t 0.09166666666666666\n",
            "i 3.369999999999972 \t 0.09166666666666666\n",
            "i 3.379999999999972 \t 0.09166666666666666\n",
            "i 3.3899999999999717 \t 0.09166666666666666\n",
            "i 3.3999999999999715 \t 0.09166666666666666\n",
            "i 3.4099999999999713 \t 0.09166666666666666\n",
            "i 3.419999999999971 \t 0.09166666666666666\n",
            "i 3.429999999999971 \t 0.09166666666666666\n",
            "i 3.4399999999999706 \t 0.09166666666666666\n",
            "i 3.4499999999999704 \t 0.09166666666666666\n",
            "i 3.45999999999997 \t 0.09166666666666666\n",
            "i 3.46999999999997 \t 0.09166666666666666\n",
            "i 3.47999999999997 \t 0.09166666666666666\n",
            "i 3.4899999999999696 \t 0.09166666666666666\n",
            "i 3.4999999999999694 \t 0.09166666666666666\n",
            "i 3.509999999999969 \t 0.09166666666666666\n",
            "i 3.519999999999969 \t 0.09166666666666666\n",
            "i 3.5299999999999687 \t 0.09166666666666666\n",
            "i 3.5399999999999685 \t 0.09166666666666666\n",
            "i 3.5499999999999683 \t 0.09166666666666666\n",
            "i 3.559999999999968 \t 0.09166666666666666\n",
            "i 3.569999999999968 \t 0.09166666666666666\n",
            "i 3.5799999999999677 \t 0.09166666666666666\n",
            "i 3.5899999999999674 \t 0.09166666666666666\n",
            "i 3.5999999999999672 \t 0.09166666666666666\n",
            "i 3.609999999999967 \t 0.09166666666666666\n",
            "i 3.619999999999967 \t 0.09166666666666666\n",
            "i 3.6299999999999666 \t 0.09166666666666666\n",
            "i 3.6399999999999664 \t 0.09166666666666666\n",
            "i 3.649999999999966 \t 0.09166666666666666\n",
            "i 3.659999999999966 \t 0.09166666666666666\n",
            "i 3.6699999999999657 \t 0.09166666666666666\n",
            "i 3.6799999999999655 \t 0.09166666666666666\n",
            "i 3.6899999999999653 \t 0.09166666666666666\n",
            "i 3.699999999999965 \t 0.09166666666666666\n",
            "i 3.709999999999965 \t 0.09166666666666666\n",
            "i 3.7199999999999647 \t 0.09166666666666666\n",
            "i 3.7299999999999645 \t 0.09166666666666666\n",
            "i 3.7399999999999642 \t 0.09166666666666666\n",
            "i 3.749999999999964 \t 0.09166666666666666\n",
            "i 3.759999999999964 \t 0.09166666666666666\n",
            "i 3.7699999999999636 \t 0.09166666666666666\n",
            "i 3.7799999999999634 \t 0.09166666666666666\n",
            "i 3.789999999999963 \t 0.09166666666666666\n",
            "i 3.799999999999963 \t 0.09166666666666666\n",
            "i 3.8099999999999627 \t 0.09166666666666666\n",
            "i 3.8199999999999625 \t 0.09166666666666666\n",
            "i 3.8299999999999623 \t 0.09166666666666666\n",
            "i 3.839999999999962 \t 0.09166666666666666\n",
            "i 3.849999999999962 \t 0.09166666666666666\n",
            "i 3.8599999999999617 \t 0.09166666666666666\n",
            "i 3.8699999999999615 \t 0.09166666666666666\n",
            "i 3.8799999999999613 \t 0.09166666666666666\n",
            "i 3.889999999999961 \t 0.09166666666666666\n",
            "i 3.899999999999961 \t 0.09166666666666666\n",
            "i 3.9099999999999606 \t 0.09166666666666666\n",
            "i 3.9199999999999604 \t 0.09166666666666666\n",
            "i 3.92999999999996 \t 0.09166666666666666\n",
            "i 3.93999999999996 \t 0.09166666666666666\n",
            "i 3.9499999999999598 \t 0.09166666666666666\n",
            "i 3.9599999999999596 \t 0.09166666666666666\n",
            "i 3.9699999999999593 \t 0.09166666666666666\n",
            "i 3.979999999999959 \t 0.09166666666666666\n",
            "i 3.989999999999959 \t 0.09166666666666666\n",
            "i 3.9999999999999587 \t 0.09166666666666666\n",
            "i 4.009999999999959 \t 0.09166666666666666\n",
            "i 4.019999999999959 \t 0.09166666666666666\n",
            "i 4.0299999999999585 \t 0.09166666666666666\n",
            "i 4.039999999999958 \t 0.09166666666666666\n",
            "i 4.049999999999958 \t 0.09166666666666666\n",
            "i 4.059999999999958 \t 0.09166666666666666\n",
            "i 4.069999999999958 \t 0.09166666666666666\n",
            "i 4.079999999999957 \t 0.09166666666666666\n",
            "i 4.089999999999957 \t 0.09166666666666666\n",
            "i 4.099999999999957 \t 0.09166666666666666\n",
            "i 4.109999999999957 \t 0.09166666666666666\n",
            "i 4.119999999999957 \t 0.09166666666666666\n",
            "i 4.129999999999956 \t 0.09166666666666666\n",
            "i 4.139999999999956 \t 0.09166666666666666\n",
            "i 4.149999999999956 \t 0.09166666666666666\n",
            "i 4.159999999999956 \t 0.09166666666666666\n",
            "i 4.1699999999999555 \t 0.09166666666666666\n",
            "i 4.179999999999955 \t 0.09166666666666666\n",
            "i 4.189999999999955 \t 0.09166666666666666\n",
            "i 4.199999999999955 \t 0.09166666666666666\n",
            "i 4.209999999999955 \t 0.09166666666666666\n",
            "i 4.2199999999999545 \t 0.09166666666666666\n",
            "i 4.229999999999954 \t 0.09166666666666666\n",
            "i 4.239999999999954 \t 0.09166666666666666\n",
            "i 4.249999999999954 \t 0.09166666666666666\n",
            "i 4.259999999999954 \t 0.09166666666666666\n",
            "i 4.269999999999953 \t 0.09166666666666666\n",
            "i 4.279999999999953 \t 0.09166666666666666\n",
            "i 4.289999999999953 \t 0.09166666666666666\n",
            "i 4.299999999999953 \t 0.09166666666666666\n",
            "i 4.3099999999999525 \t 0.09166666666666666\n",
            "i 4.319999999999952 \t 0.09166666666666666\n",
            "i 4.329999999999952 \t 0.09166666666666666\n",
            "i 4.339999999999952 \t 0.09166666666666666\n",
            "i 4.349999999999952 \t 0.09166666666666666\n",
            "i 4.3599999999999515 \t 0.09166666666666666\n",
            "i 4.369999999999951 \t 0.09166666666666666\n",
            "i 4.379999999999951 \t 0.09166666666666666\n",
            "i 4.389999999999951 \t 0.09166666666666666\n",
            "i 4.399999999999951 \t 0.09166666666666666\n",
            "i 4.40999999999995 \t 0.09166666666666666\n",
            "i 4.41999999999995 \t 0.09166666666666666\n",
            "i 4.42999999999995 \t 0.09166666666666666\n",
            "i 4.43999999999995 \t 0.09166666666666666\n",
            "i 4.4499999999999496 \t 0.09166666666666666\n",
            "i 4.459999999999949 \t 0.09166666666666666\n",
            "i 4.469999999999949 \t 0.09166666666666666\n",
            "i 4.479999999999949 \t 0.09166666666666666\n",
            "i 4.489999999999949 \t 0.09166666666666666\n",
            "i 4.4999999999999485 \t 0.09166666666666666\n",
            "i 4.509999999999948 \t 0.09166666666666666\n",
            "i 4.519999999999948 \t 0.09166666666666666\n",
            "i 4.529999999999948 \t 0.09166666666666666\n",
            "i 4.539999999999948 \t 0.09166666666666666\n",
            "i 4.549999999999947 \t 0.09166666666666666\n",
            "i 4.559999999999947 \t 0.09166666666666666\n",
            "i 4.569999999999947 \t 0.09166666666666666\n",
            "i 4.579999999999947 \t 0.09166666666666666\n",
            "i 4.589999999999947 \t 0.09166666666666666\n",
            "i 4.599999999999946 \t 0.09166666666666666\n",
            "i 4.609999999999946 \t 0.09166666666666666\n",
            "i 4.619999999999946 \t 0.09166666666666666\n",
            "i 4.629999999999946 \t 0.09166666666666666\n",
            "i 4.6399999999999455 \t 0.09166666666666666\n",
            "i 4.649999999999945 \t 0.09166666666666666\n",
            "i 4.659999999999945 \t 0.09166666666666666\n",
            "i 4.669999999999945 \t 0.09166666666666666\n",
            "i 4.679999999999945 \t 0.09166666666666666\n",
            "i 4.689999999999944 \t 0.09166666666666666\n",
            "i 4.699999999999944 \t 0.09166666666666666\n",
            "i 4.709999999999944 \t 0.09166666666666666\n",
            "i 4.719999999999944 \t 0.09166666666666666\n",
            "i 4.729999999999944 \t 0.09166666666666666\n",
            "i 4.739999999999943 \t 0.09166666666666666\n",
            "i 4.749999999999943 \t 0.09166666666666666\n",
            "i 4.759999999999943 \t 0.09166666666666666\n",
            "i 4.769999999999943 \t 0.09166666666666666\n",
            "i 4.7799999999999425 \t 0.09166666666666666\n",
            "i 4.789999999999942 \t 0.09166666666666666\n",
            "i 4.799999999999942 \t 0.09166666666666666\n",
            "i 4.809999999999942 \t 0.09166666666666666\n",
            "i 4.819999999999942 \t 0.09166666666666666\n",
            "i 4.8299999999999415 \t 0.09166666666666666\n",
            "i 4.839999999999941 \t 0.09166666666666666\n",
            "i 4.849999999999941 \t 0.09166666666666666\n",
            "i 4.859999999999941 \t 0.09166666666666666\n",
            "i 4.869999999999941 \t 0.09166666666666666\n",
            "i 4.87999999999994 \t 0.09166666666666666\n",
            "i 4.88999999999994 \t 0.09166666666666666\n",
            "i 4.89999999999994 \t 0.09166666666666666\n",
            "i 4.90999999999994 \t 0.09166666666666666\n",
            "i 4.9199999999999395 \t 0.09166666666666666\n",
            "i 4.929999999999939 \t 0.09166666666666666\n",
            "i 4.939999999999939 \t 0.09166666666666666\n",
            "i 4.949999999999939 \t 0.09166666666666666\n",
            "i 4.959999999999939 \t 0.09166666666666666\n",
            "i 4.9699999999999385 \t 0.09166666666666666\n",
            "i 4.979999999999938 \t 0.09166666666666666\n",
            "i 4.989999999999938 \t 0.09166666666666666\n",
            "i 4.999999999999938 \t 0.09166666666666666\n",
            "i 5.009999999999938 \t 0.09166666666666666\n",
            "i 5.019999999999937 \t 0.09166666666666666\n",
            "i 5.029999999999937 \t 0.09166666666666666\n",
            "i 5.039999999999937 \t 0.09166666666666666\n",
            "i 5.049999999999937 \t 0.09166666666666666\n",
            "i 5.0599999999999365 \t 0.09166666666666666\n",
            "i 5.069999999999936 \t 0.09166666666666666\n",
            "i 5.079999999999936 \t 0.09166666666666666\n",
            "i 5.089999999999936 \t 0.09166666666666666\n",
            "i 5.099999999999936 \t 0.09166666666666666\n",
            "i 5.1099999999999355 \t 0.09166666666666666\n",
            "i 5.119999999999935 \t 0.09166666666666666\n",
            "i 5.129999999999935 \t 0.09166666666666666\n",
            "i 5.139999999999935 \t 0.09166666666666666\n",
            "i 5.149999999999935 \t 0.09166666666666666\n",
            "i 5.159999999999934 \t 0.09166666666666666\n",
            "i 5.169999999999934 \t 0.09166666666666666\n",
            "i 5.179999999999934 \t 0.09166666666666666\n",
            "i 5.189999999999934 \t 0.09166666666666666\n",
            "i 5.199999999999934 \t 0.09166666666666666\n",
            "i 5.209999999999933 \t 0.09166666666666666\n",
            "i 5.219999999999933 \t 0.09166666666666666\n",
            "i 5.229999999999933 \t 0.09166666666666666\n",
            "i 5.239999999999933 \t 0.09166666666666666\n",
            "i 5.2499999999999325 \t 0.09166666666666666\n",
            "i 5.259999999999932 \t 0.09166666666666666\n",
            "i 5.269999999999932 \t 0.09166666666666666\n",
            "i 5.279999999999932 \t 0.09166666666666666\n",
            "i 5.289999999999932 \t 0.09166666666666666\n",
            "i 5.299999999999931 \t 0.09166666666666666\n",
            "i 5.309999999999931 \t 0.09166666666666666\n",
            "i 5.319999999999931 \t 0.09166666666666666\n",
            "i 5.329999999999931 \t 0.09166666666666666\n",
            "i 5.339999999999931 \t 0.09166666666666666\n",
            "i 5.34999999999993 \t 0.09166666666666666\n",
            "i 5.35999999999993 \t 0.09166666666666666\n",
            "i 5.36999999999993 \t 0.09166666666666666\n",
            "i 5.37999999999993 \t 0.09166666666666666\n",
            "i 5.3899999999999295 \t 0.09166666666666666\n",
            "i 5.399999999999929 \t 0.09166666666666666\n",
            "i 5.409999999999929 \t 0.09166666666666666\n",
            "i 5.419999999999929 \t 0.09166666666666666\n",
            "i 5.429999999999929 \t 0.09166666666666666\n",
            "i 5.4399999999999284 \t 0.09166666666666666\n",
            "i 5.449999999999928 \t 0.09166666666666666\n",
            "i 5.459999999999928 \t 0.09166666666666666\n",
            "i 5.469999999999928 \t 0.09166666666666666\n",
            "i 5.479999999999928 \t 0.09166666666666666\n",
            "i 5.489999999999927 \t 0.09166666666666666\n",
            "i 5.499999999999927 \t 0.09166666666666666\n",
            "i 5.509999999999927 \t 0.09166666666666666\n",
            "i 5.519999999999927 \t 0.09166666666666666\n",
            "i 5.5299999999999265 \t 0.09166666666666666\n",
            "i 5.539999999999926 \t 0.09166666666666666\n",
            "i 5.549999999999926 \t 0.09166666666666666\n",
            "i 5.559999999999926 \t 0.09166666666666666\n",
            "i 5.569999999999926 \t 0.09166666666666666\n",
            "i 5.5799999999999255 \t 0.09166666666666666\n",
            "i 5.589999999999925 \t 0.09166666666666666\n",
            "i 5.599999999999925 \t 0.09166666666666666\n",
            "i 5.609999999999925 \t 0.09166666666666666\n",
            "i 5.619999999999925 \t 0.09166666666666666\n",
            "i 5.629999999999924 \t 0.09166666666666666\n",
            "i 5.639999999999924 \t 0.09166666666666666\n",
            "i 5.649999999999924 \t 0.09166666666666666\n",
            "i 5.659999999999924 \t 0.09166666666666666\n",
            "i 5.6699999999999235 \t 0.09166666666666666\n",
            "i 5.679999999999923 \t 0.09166666666666666\n",
            "i 5.689999999999923 \t 0.09166666666666666\n",
            "i 5.699999999999923 \t 0.09166666666666666\n",
            "i 5.709999999999923 \t 0.09166666666666666\n",
            "i 5.7199999999999225 \t 0.09166666666666666\n",
            "i 5.729999999999922 \t 0.09166666666666666\n",
            "i 5.739999999999922 \t 0.09166666666666666\n",
            "i 5.749999999999922 \t 0.09166666666666666\n",
            "i 5.759999999999922 \t 0.09166666666666666\n",
            "i 5.769999999999921 \t 0.09166666666666666\n",
            "i 5.779999999999921 \t 0.09166666666666666\n",
            "i 5.789999999999921 \t 0.09166666666666666\n",
            "i 5.799999999999921 \t 0.09166666666666666\n",
            "i 5.809999999999921 \t 0.09166666666666666\n",
            "i 5.81999999999992 \t 0.09166666666666666\n",
            "i 5.82999999999992 \t 0.09166666666666666\n",
            "i 5.83999999999992 \t 0.09166666666666666\n",
            "i 5.84999999999992 \t 0.09166666666666666\n",
            "i 5.8599999999999195 \t 0.09166666666666666\n",
            "i 5.869999999999919 \t 0.09166666666666666\n",
            "i 5.879999999999919 \t 0.09166666666666666\n",
            "i 5.889999999999919 \t 0.09166666666666666\n",
            "i 5.899999999999919 \t 0.09166666666666666\n",
            "i 5.909999999999918 \t 0.09166666666666666\n",
            "i 5.919999999999918 \t 0.09166666666666666\n",
            "i 5.929999999999918 \t 0.09166666666666666\n",
            "i 5.939999999999918 \t 0.09166666666666666\n",
            "i 5.949999999999918 \t 0.09166666666666666\n",
            "i 5.959999999999917 \t 0.09166666666666666\n",
            "i 5.969999999999917 \t 0.09166666666666666\n",
            "i 5.979999999999917 \t 0.09166666666666666\n",
            "i 5.989999999999917 \t 0.09166666666666666\n",
            "i 5.9999999999999165 \t 0.09166666666666666\n",
            "i 6.009999999999916 \t 0.09166666666666666\n",
            "i 6.019999999999916 \t 0.09166666666666666\n",
            "i 6.029999999999916 \t 0.09166666666666666\n",
            "i 6.039999999999916 \t 0.09166666666666666\n",
            "i 6.0499999999999154 \t 0.09166666666666666\n",
            "i 6.059999999999915 \t 0.09166666666666666\n",
            "i 6.069999999999915 \t 0.09166666666666666\n",
            "i 6.079999999999915 \t 0.09166666666666666\n",
            "i 6.089999999999915 \t 0.09166666666666666\n",
            "i 6.099999999999914 \t 0.09166666666666666\n",
            "i 6.109999999999914 \t 0.09166666666666666\n",
            "i 6.119999999999914 \t 0.09166666666666666\n",
            "i 6.129999999999914 \t 0.09166666666666666\n",
            "i 6.1399999999999135 \t 0.09166666666666666\n",
            "i 6.149999999999913 \t 0.09166666666666666\n",
            "i 6.159999999999913 \t 0.09166666666666666\n",
            "i 6.169999999999913 \t 0.09166666666666666\n",
            "i 6.179999999999913 \t 0.09166666666666666\n",
            "i 6.1899999999999125 \t 0.09166666666666666\n",
            "i 6.199999999999912 \t 0.09166666666666666\n",
            "i 6.209999999999912 \t 0.09166666666666666\n",
            "i 6.219999999999912 \t 0.09166666666666666\n",
            "i 6.229999999999912 \t 0.09166666666666666\n",
            "i 6.239999999999911 \t 0.09166666666666666\n",
            "i 6.249999999999911 \t 0.09166666666666666\n",
            "i 6.259999999999911 \t 0.09166666666666666\n",
            "i 6.269999999999911 \t 0.09166666666666666\n",
            "i 6.2799999999999105 \t 0.09166666666666666\n",
            "i 6.28999999999991 \t 0.09166666666666666\n",
            "i 6.29999999999991 \t 0.09166666666666666\n",
            "i 6.30999999999991 \t 0.09166666666666666\n",
            "i 6.31999999999991 \t 0.09166666666666666\n",
            "i 6.3299999999999095 \t 0.09166666666666666\n",
            "i 6.339999999999909 \t 0.09166666666666666\n",
            "i 6.349999999999909 \t 0.09166666666666666\n",
            "i 6.359999999999909 \t 0.09166666666666666\n",
            "i 6.369999999999909 \t 0.09166666666666666\n",
            "i 6.379999999999908 \t 0.09166666666666666\n",
            "i 6.389999999999908 \t 0.09166666666666666\n",
            "i 6.399999999999908 \t 0.09166666666666666\n",
            "i 6.409999999999908 \t 0.09166666666666666\n",
            "i 6.419999999999908 \t 0.09166666666666666\n",
            "i 6.429999999999907 \t 0.09166666666666666\n",
            "i 6.439999999999907 \t 0.09166666666666666\n",
            "i 6.449999999999907 \t 0.09166666666666666\n",
            "i 6.459999999999907 \t 0.09166666666666666\n",
            "i 6.4699999999999065 \t 0.09166666666666666\n",
            "i 6.479999999999906 \t 0.09166666666666666\n",
            "i 6.489999999999906 \t 0.09166666666666666\n",
            "i 6.499999999999906 \t 0.09166666666666666\n",
            "i 6.509999999999906 \t 0.09166666666666666\n",
            "i 6.519999999999905 \t 0.09166666666666666\n",
            "i 6.529999999999905 \t 0.09166666666666666\n",
            "i 6.539999999999905 \t 0.09166666666666666\n",
            "i 6.549999999999905 \t 0.09166666666666666\n",
            "i 6.559999999999905 \t 0.09166666666666666\n",
            "i 6.569999999999904 \t 0.09166666666666666\n",
            "i 6.579999999999904 \t 0.09166666666666666\n",
            "i 6.589999999999904 \t 0.09166666666666666\n",
            "i 6.599999999999904 \t 0.09166666666666666\n",
            "i 6.6099999999999035 \t 0.09166666666666666\n",
            "i 6.619999999999903 \t 0.09166666666666666\n",
            "i 6.629999999999903 \t 0.09166666666666666\n",
            "i 6.639999999999903 \t 0.09166666666666666\n",
            "i 6.649999999999903 \t 0.09166666666666666\n",
            "i 6.659999999999902 \t 0.09166666666666666\n",
            "i 6.669999999999902 \t 0.09166666666666666\n",
            "i 6.679999999999902 \t 0.09166666666666666\n",
            "i 6.689999999999902 \t 0.09166666666666666\n",
            "i 6.699999999999902 \t 0.09166666666666666\n",
            "i 6.709999999999901 \t 0.09166666666666666\n",
            "i 6.719999999999901 \t 0.09166666666666666\n",
            "i 6.729999999999901 \t 0.09166666666666666\n",
            "i 6.739999999999901 \t 0.09166666666666666\n",
            "i 6.7499999999999005 \t 0.09166666666666666\n",
            "i 6.7599999999999 \t 0.09166666666666666\n",
            "i 6.7699999999999 \t 0.09166666666666666\n",
            "i 6.7799999999999 \t 0.09166666666666666\n",
            "i 6.7899999999999 \t 0.09166666666666666\n",
            "i 6.7999999999998995 \t 0.09166666666666666\n",
            "i 6.809999999999899 \t 0.09166666666666666\n",
            "i 6.819999999999899 \t 0.09166666666666666\n",
            "i 6.829999999999899 \t 0.09166666666666666\n",
            "i 6.839999999999899 \t 0.09166666666666666\n",
            "i 6.849999999999898 \t 0.09166666666666666\n",
            "i 6.859999999999898 \t 0.09166666666666666\n",
            "i 6.869999999999898 \t 0.09166666666666666\n",
            "i 6.879999999999898 \t 0.09166666666666666\n",
            "i 6.8899999999998975 \t 0.09166666666666666\n",
            "i 6.899999999999897 \t 0.09166666666666666\n",
            "i 6.909999999999897 \t 0.09166666666666666\n",
            "i 6.919999999999897 \t 0.09166666666666666\n",
            "i 6.929999999999897 \t 0.09166666666666666\n",
            "i 6.9399999999998965 \t 0.09166666666666666\n",
            "i 6.949999999999896 \t 0.09166666666666666\n",
            "i 6.959999999999896 \t 0.09166666666666666\n",
            "i 6.969999999999896 \t 0.09166666666666666\n",
            "i 6.979999999999896 \t 0.09166666666666666\n",
            "i 6.989999999999895 \t 0.09166666666666666\n",
            "i 6.999999999999895 \t 0.09166666666666666\n",
            "i 7.009999999999895 \t 0.09166666666666666\n",
            "i 7.019999999999895 \t 0.09166666666666666\n",
            "i 7.0299999999998946 \t 0.09166666666666666\n",
            "i 7.039999999999894 \t 0.09166666666666666\n",
            "i 7.049999999999894 \t 0.09166666666666666\n",
            "i 7.059999999999894 \t 0.09166666666666666\n",
            "i 7.069999999999894 \t 0.09166666666666666\n",
            "i 7.0799999999998935 \t 0.09166666666666666\n",
            "i 7.089999999999893 \t 0.09166666666666666\n",
            "i 7.099999999999893 \t 0.09166666666666666\n",
            "i 7.109999999999893 \t 0.09166666666666666\n",
            "i 7.119999999999893 \t 0.09166666666666666\n",
            "i 7.129999999999892 \t 0.09166666666666666\n",
            "i 7.139999999999892 \t 0.09166666666666666\n",
            "i 7.149999999999892 \t 0.09166666666666666\n",
            "i 7.159999999999892 \t 0.09166666666666666\n",
            "i 7.169999999999892 \t 0.09166666666666666\n",
            "i 7.179999999999891 \t 0.09166666666666666\n",
            "i 7.189999999999891 \t 0.09166666666666666\n",
            "i 7.199999999999891 \t 0.09166666666666666\n",
            "i 7.209999999999891 \t 0.09166666666666666\n",
            "i 7.2199999999998905 \t 0.09166666666666666\n",
            "i 7.22999999999989 \t 0.09166666666666666\n",
            "i 7.23999999999989 \t 0.09166666666666666\n",
            "i 7.24999999999989 \t 0.09166666666666666\n",
            "i 7.25999999999989 \t 0.09166666666666666\n",
            "i 7.269999999999889 \t 0.09166666666666666\n",
            "i 7.279999999999889 \t 0.09166666666666666\n",
            "i 7.289999999999889 \t 0.09166666666666666\n",
            "i 7.299999999999889 \t 0.09166666666666666\n",
            "i 7.309999999999889 \t 0.09166666666666666\n",
            "i 7.319999999999888 \t 0.09166666666666666\n",
            "i 7.329999999999888 \t 0.09166666666666666\n",
            "i 7.339999999999888 \t 0.09166666666666666\n",
            "i 7.349999999999888 \t 0.09166666666666666\n",
            "i 7.3599999999998875 \t 0.09166666666666666\n",
            "i 7.369999999999887 \t 0.09166666666666666\n",
            "i 7.379999999999887 \t 0.09166666666666666\n",
            "i 7.389999999999887 \t 0.09166666666666666\n",
            "i 7.399999999999887 \t 0.09166666666666666\n",
            "i 7.4099999999998865 \t 0.09166666666666666\n",
            "i 7.419999999999886 \t 0.09166666666666666\n",
            "i 7.429999999999886 \t 0.09166666666666666\n",
            "i 7.439999999999886 \t 0.09166666666666666\n",
            "i 7.449999999999886 \t 0.09166666666666666\n",
            "i 7.459999999999885 \t 0.09166666666666666\n",
            "i 7.469999999999885 \t 0.09166666666666666\n",
            "i 7.479999999999885 \t 0.09166666666666666\n",
            "i 7.489999999999885 \t 0.09166666666666666\n",
            "i 7.4999999999998845 \t 0.09166666666666666\n",
            "i 7.509999999999884 \t 0.09166666666666666\n",
            "i 7.519999999999884 \t 0.09166666666666666\n",
            "i 7.529999999999884 \t 0.09166666666666666\n",
            "i 7.539999999999884 \t 0.09166666666666666\n",
            "i 7.5499999999998835 \t 0.09166666666666666\n",
            "i 7.559999999999883 \t 0.09166666666666666\n",
            "i 7.569999999999883 \t 0.09166666666666666\n",
            "i 7.579999999999883 \t 0.09166666666666666\n",
            "i 7.589999999999883 \t 0.09166666666666666\n",
            "i 7.599999999999882 \t 0.09166666666666666\n",
            "i 7.609999999999882 \t 0.09166666666666666\n",
            "i 7.619999999999882 \t 0.09166666666666666\n",
            "i 7.629999999999882 \t 0.09166666666666666\n",
            "i 7.6399999999998816 \t 0.09166666666666666\n",
            "i 7.649999999999881 \t 0.09166666666666666\n",
            "i 7.659999999999881 \t 0.09166666666666666\n",
            "i 7.669999999999881 \t 0.09166666666666666\n",
            "i 7.679999999999881 \t 0.09166666666666666\n",
            "i 7.6899999999998805 \t 0.09166666666666666\n",
            "i 7.69999999999988 \t 0.09166666666666666\n",
            "i 7.70999999999988 \t 0.09166666666666666\n",
            "i 7.71999999999988 \t 0.09166666666666666\n",
            "i 7.72999999999988 \t 0.09166666666666666\n",
            "i 7.739999999999879 \t 0.09166666666666666\n",
            "i 7.749999999999879 \t 0.09166666666666666\n",
            "i 7.759999999999879 \t 0.09166666666666666\n",
            "i 7.769999999999879 \t 0.09166666666666666\n",
            "i 7.779999999999879 \t 0.09166666666666666\n",
            "i 7.789999999999878 \t 0.09166666666666666\n",
            "i 7.799999999999878 \t 0.09166666666666666\n",
            "i 7.809999999999878 \t 0.09166666666666666\n",
            "i 7.819999999999878 \t 0.09166666666666666\n",
            "i 7.8299999999998775 \t 0.09166666666666666\n",
            "i 7.839999999999877 \t 0.09166666666666666\n",
            "i 7.849999999999877 \t 0.09166666666666666\n",
            "i 7.859999999999877 \t 0.09166666666666666\n",
            "i 7.869999999999877 \t 0.09166666666666666\n",
            "i 7.879999999999876 \t 0.09166666666666666\n",
            "i 7.889999999999876 \t 0.09166666666666666\n",
            "i 7.899999999999876 \t 0.09166666666666666\n",
            "i 7.909999999999876 \t 0.09166666666666666\n",
            "i 7.919999999999876 \t 0.09166666666666666\n",
            "i 7.929999999999875 \t 0.09166666666666666\n",
            "i 7.939999999999875 \t 0.09166666666666666\n",
            "i 7.949999999999875 \t 0.09166666666666666\n",
            "i 7.959999999999875 \t 0.09166666666666666\n",
            "i 7.9699999999998745 \t 0.09166666666666666\n",
            "i 7.979999999999874 \t 0.09166666666666666\n",
            "i 7.989999999999874 \t 0.09166666666666666\n",
            "i 7.999999999999874 \t 0.09166666666666666\n",
            "i 8.009999999999874 \t 0.09166666666666666\n",
            "i 8.019999999999873 \t 0.09166666666666666\n",
            "i 8.029999999999873 \t 0.09166666666666666\n",
            "i 8.039999999999873 \t 0.09166666666666666\n",
            "i 8.049999999999873 \t 0.09166666666666666\n",
            "i 8.059999999999873 \t 0.09166666666666666\n",
            "i 8.069999999999872 \t 0.09166666666666666\n",
            "i 8.079999999999872 \t 0.09166666666666666\n",
            "i 8.089999999999872 \t 0.09166666666666666\n",
            "i 8.099999999999872 \t 0.09166666666666666\n",
            "i 8.109999999999872 \t 0.09166666666666666\n",
            "i 8.119999999999871 \t 0.09166666666666666\n",
            "i 8.129999999999871 \t 0.09166666666666666\n",
            "i 8.139999999999871 \t 0.09166666666666666\n",
            "i 8.14999999999987 \t 0.09166666666666666\n",
            "i 8.15999999999987 \t 0.09166666666666666\n",
            "i 8.16999999999987 \t 0.09166666666666666\n",
            "i 8.17999999999987 \t 0.09166666666666666\n",
            "i 8.18999999999987 \t 0.09166666666666666\n",
            "i 8.19999999999987 \t 0.09166666666666666\n",
            "i 8.20999999999987 \t 0.09166666666666666\n",
            "i 8.21999999999987 \t 0.09166666666666666\n",
            "i 8.229999999999869 \t 0.09166666666666666\n",
            "i 8.239999999999869 \t 0.09166666666666666\n",
            "i 8.249999999999869 \t 0.09166666666666666\n",
            "i 8.259999999999868 \t 0.09166666666666666\n",
            "i 8.269999999999868 \t 0.09166666666666666\n",
            "i 8.279999999999868 \t 0.09166666666666666\n",
            "i 8.289999999999868 \t 0.09166666666666666\n",
            "i 8.299999999999867 \t 0.09166666666666666\n",
            "i 8.309999999999867 \t 0.09166666666666666\n",
            "i 8.319999999999867 \t 0.09166666666666666\n",
            "i 8.329999999999867 \t 0.09166666666666666\n",
            "i 8.339999999999867 \t 0.09166666666666666\n",
            "i 8.349999999999866 \t 0.09166666666666666\n",
            "i 8.359999999999866 \t 0.09166666666666666\n",
            "i 8.369999999999866 \t 0.09166666666666666\n",
            "i 8.379999999999866 \t 0.09166666666666666\n",
            "i 8.389999999999866 \t 0.09166666666666666\n",
            "i 8.399999999999865 \t 0.09166666666666666\n",
            "i 8.409999999999865 \t 0.09166666666666666\n",
            "i 8.419999999999865 \t 0.09166666666666666\n",
            "i 8.429999999999865 \t 0.09166666666666666\n",
            "i 8.439999999999864 \t 0.09166666666666666\n",
            "i 8.449999999999864 \t 0.09166666666666666\n",
            "i 8.459999999999864 \t 0.09166666666666666\n",
            "i 8.469999999999864 \t 0.09166666666666666\n",
            "i 8.479999999999864 \t 0.09166666666666666\n",
            "i 8.489999999999863 \t 0.09166666666666666\n",
            "i 8.499999999999863 \t 0.09166666666666666\n",
            "i 8.509999999999863 \t 0.09166666666666666\n",
            "i 8.519999999999863 \t 0.09166666666666666\n",
            "i 8.529999999999863 \t 0.09166666666666666\n",
            "i 8.539999999999862 \t 0.09166666666666666\n",
            "i 8.549999999999862 \t 0.09166666666666666\n",
            "i 8.559999999999862 \t 0.09166666666666666\n",
            "i 8.569999999999862 \t 0.09166666666666666\n",
            "i 8.579999999999862 \t 0.09166666666666666\n",
            "i 8.589999999999861 \t 0.09166666666666666\n",
            "i 8.599999999999861 \t 0.09166666666666666\n",
            "i 8.60999999999986 \t 0.09166666666666666\n",
            "i 8.61999999999986 \t 0.09166666666666666\n",
            "i 8.62999999999986 \t 0.09166666666666666\n",
            "i 8.63999999999986 \t 0.09166666666666666\n",
            "i 8.64999999999986 \t 0.09166666666666666\n",
            "i 8.65999999999986 \t 0.09166666666666666\n",
            "i 8.66999999999986 \t 0.09166666666666666\n",
            "i 8.67999999999986 \t 0.09166666666666666\n",
            "i 8.68999999999986 \t 0.09166666666666666\n",
            "i 8.699999999999859 \t 0.09166666666666666\n",
            "i 8.709999999999859 \t 0.09166666666666666\n",
            "i 8.719999999999859 \t 0.09166666666666666\n",
            "i 8.729999999999858 \t 0.09166666666666666\n",
            "i 8.739999999999858 \t 0.09166666666666666\n",
            "i 8.749999999999858 \t 0.09166666666666666\n",
            "i 8.759999999999858 \t 0.09166666666666666\n",
            "i 8.769999999999857 \t 0.09166666666666666\n",
            "i 8.779999999999857 \t 0.09166666666666666\n",
            "i 8.789999999999857 \t 0.09166666666666666\n",
            "i 8.799999999999857 \t 0.09166666666666666\n",
            "i 8.809999999999857 \t 0.09166666666666666\n",
            "i 8.819999999999856 \t 0.09166666666666666\n",
            "i 8.829999999999856 \t 0.09166666666666666\n",
            "i 8.839999999999856 \t 0.09166666666666666\n",
            "i 8.849999999999856 \t 0.09166666666666666\n",
            "i 8.859999999999856 \t 0.09166666666666666\n",
            "i 8.869999999999855 \t 0.09166666666666666\n",
            "i 8.879999999999855 \t 0.09166666666666666\n",
            "i 8.889999999999855 \t 0.09166666666666666\n",
            "i 8.899999999999855 \t 0.09166666666666666\n",
            "i 8.909999999999854 \t 0.09166666666666666\n",
            "i 8.919999999999854 \t 0.09166666666666666\n",
            "i 8.929999999999854 \t 0.09166666666666666\n",
            "i 8.939999999999854 \t 0.09166666666666666\n",
            "i 8.949999999999854 \t 0.09166666666666666\n",
            "i 8.959999999999853 \t 0.09166666666666666\n",
            "i 8.969999999999853 \t 0.09166666666666666\n",
            "i 8.979999999999853 \t 0.09166666666666666\n",
            "i 8.989999999999853 \t 0.09166666666666666\n",
            "i 8.999999999999853 \t 0.09166666666666666\n",
            "i 9.009999999999852 \t 0.09166666666666666\n",
            "i 9.019999999999852 \t 0.09166666666666666\n",
            "i 9.029999999999852 \t 0.09166666666666666\n",
            "i 9.039999999999852 \t 0.09166666666666666\n",
            "i 9.049999999999851 \t 0.09166666666666666\n",
            "i 9.059999999999851 \t 0.09166666666666666\n",
            "i 9.069999999999851 \t 0.09166666666666666\n",
            "i 9.07999999999985 \t 0.09166666666666666\n",
            "i 9.08999999999985 \t 0.09166666666666666\n",
            "i 9.09999999999985 \t 0.09166666666666666\n",
            "i 9.10999999999985 \t 0.09166666666666666\n",
            "i 9.11999999999985 \t 0.09166666666666666\n",
            "i 9.12999999999985 \t 0.09166666666666666\n",
            "i 9.13999999999985 \t 0.09166666666666666\n",
            "i 9.14999999999985 \t 0.09166666666666666\n",
            "i 9.15999999999985 \t 0.09166666666666666\n",
            "i 9.169999999999849 \t 0.09166666666666666\n",
            "i 9.179999999999849 \t 0.09166666666666666\n",
            "i 9.189999999999849 \t 0.09166666666666666\n",
            "i 9.199999999999848 \t 0.09166666666666666\n",
            "i 9.209999999999848 \t 0.09166666666666666\n",
            "i 9.219999999999848 \t 0.09166666666666666\n",
            "i 9.229999999999848 \t 0.09166666666666666\n",
            "i 9.239999999999847 \t 0.09166666666666666\n",
            "i 9.249999999999847 \t 0.09166666666666666\n",
            "i 9.259999999999847 \t 0.09166666666666666\n",
            "i 9.269999999999847 \t 0.09166666666666666\n",
            "i 9.279999999999847 \t 0.09166666666666666\n",
            "i 9.289999999999846 \t 0.09166666666666666\n",
            "i 9.299999999999846 \t 0.09166666666666666\n",
            "i 9.309999999999846 \t 0.09166666666666666\n",
            "i 9.319999999999846 \t 0.09166666666666666\n",
            "i 9.329999999999846 \t 0.09166666666666666\n",
            "i 9.339999999999845 \t 0.09166666666666666\n",
            "i 9.349999999999845 \t 0.09166666666666666\n",
            "i 9.359999999999845 \t 0.09166666666666666\n",
            "i 9.369999999999845 \t 0.09166666666666666\n",
            "i 9.379999999999844 \t 0.09166666666666666\n",
            "i 9.389999999999844 \t 0.09166666666666666\n",
            "i 9.399999999999844 \t 0.09166666666666666\n",
            "i 9.409999999999844 \t 0.09166666666666666\n",
            "i 9.419999999999844 \t 0.09166666666666666\n",
            "i 9.429999999999843 \t 0.09166666666666666\n",
            "i 9.439999999999843 \t 0.09166666666666666\n",
            "i 9.449999999999843 \t 0.09166666666666666\n",
            "i 9.459999999999843 \t 0.09166666666666666\n",
            "i 9.469999999999843 \t 0.09166666666666666\n",
            "i 9.479999999999842 \t 0.09166666666666666\n",
            "i 9.489999999999842 \t 0.09166666666666666\n",
            "i 9.499999999999842 \t 0.09166666666666666\n",
            "i 9.509999999999842 \t 0.09166666666666666\n",
            "i 9.519999999999841 \t 0.09166666666666666\n",
            "i 9.529999999999841 \t 0.09166666666666666\n",
            "i 9.539999999999841 \t 0.09166666666666666\n",
            "i 9.54999999999984 \t 0.09166666666666666\n",
            "i 9.55999999999984 \t 0.09166666666666666\n",
            "i 9.56999999999984 \t 0.09166666666666666\n",
            "i 9.57999999999984 \t 0.09166666666666666\n",
            "i 9.58999999999984 \t 0.09166666666666666\n",
            "i 9.59999999999984 \t 0.09166666666666666\n",
            "i 9.60999999999984 \t 0.09166666666666666\n",
            "i 9.61999999999984 \t 0.09166666666666666\n",
            "i 9.62999999999984 \t 0.09166666666666666\n",
            "i 9.639999999999839 \t 0.09166666666666666\n",
            "i 9.649999999999839 \t 0.09166666666666666\n",
            "i 9.659999999999838 \t 0.09166666666666666\n",
            "i 9.669999999999838 \t 0.09166666666666666\n",
            "i 9.679999999999838 \t 0.09166666666666666\n",
            "i 9.689999999999838 \t 0.09166666666666666\n",
            "i 9.699999999999838 \t 0.09166666666666666\n",
            "i 9.709999999999837 \t 0.09166666666666666\n",
            "i 9.719999999999837 \t 0.09166666666666666\n",
            "i 9.729999999999837 \t 0.09166666666666666\n",
            "i 9.739999999999837 \t 0.09166666666666666\n",
            "i 9.749999999999837 \t 0.09166666666666666\n",
            "i 9.759999999999836 \t 0.09166666666666666\n",
            "i 9.769999999999836 \t 0.09166666666666666\n",
            "i 9.779999999999836 \t 0.09166666666666666\n",
            "i 9.789999999999836 \t 0.09166666666666666\n",
            "i 9.799999999999836 \t 0.09166666666666666\n",
            "i 9.809999999999835 \t 0.09166666666666666\n",
            "i 9.819999999999835 \t 0.09166666666666666\n",
            "i 9.829999999999835 \t 0.09166666666666666\n",
            "i 9.839999999999835 \t 0.09166666666666666\n",
            "i 9.849999999999834 \t 0.09166666666666666\n",
            "i 9.859999999999834 \t 0.09166666666666666\n",
            "i 9.869999999999834 \t 0.09166666666666666\n",
            "i 9.879999999999834 \t 0.09166666666666666\n",
            "i 9.889999999999834 \t 0.09166666666666666\n",
            "i 9.899999999999833 \t 0.09166666666666666\n",
            "i 9.909999999999833 \t 0.09166666666666666\n",
            "i 9.919999999999833 \t 0.09166666666666666\n",
            "i 9.929999999999833 \t 0.09166666666666666\n",
            "i 9.939999999999833 \t 0.09166666666666666\n",
            "i 9.949999999999832 \t 0.09166666666666666\n",
            "i 9.959999999999832 \t 0.09166666666666666\n",
            "i 9.969999999999832 \t 0.09166666666666666\n",
            "i 9.979999999999832 \t 0.09166666666666666\n",
            "i 9.989999999999831 \t 0.09166666666666666\n",
            "i 9.999999999999831 \t 0.09166666666666666\n",
            "i 10.009999999999831 \t 0.09166666666666666\n",
            "i 10.01999999999983 \t 0.09166666666666666\n",
            "i 10.02999999999983 \t 0.09166666666666666\n",
            "i 10.03999999999983 \t 0.09166666666666666\n",
            "i 10.04999999999983 \t 0.09166666666666666\n",
            "i 10.05999999999983 \t 0.09166666666666666\n",
            "i 10.06999999999983 \t 0.09166666666666666\n",
            "i 10.07999999999983 \t 0.09166666666666666\n",
            "i 10.08999999999983 \t 0.09166666666666666\n",
            "i 10.09999999999983 \t 0.09166666666666666\n",
            "i 10.109999999999829 \t 0.09166666666666666\n",
            "i 10.119999999999829 \t 0.09166666666666666\n",
            "i 10.129999999999828 \t 0.09166666666666666\n",
            "i 10.139999999999828 \t 0.09166666666666666\n",
            "i 10.149999999999828 \t 0.09166666666666666\n",
            "i 10.159999999999828 \t 0.09166666666666666\n",
            "i 10.169999999999828 \t 0.09166666666666666\n",
            "i 10.179999999999827 \t 0.09166666666666666\n",
            "i 10.189999999999827 \t 0.09166666666666666\n",
            "i 10.199999999999827 \t 0.09166666666666666\n",
            "i 10.209999999999827 \t 0.09166666666666666\n",
            "i 10.219999999999827 \t 0.09166666666666666\n",
            "i 10.229999999999826 \t 0.09166666666666666\n",
            "i 10.239999999999826 \t 0.09166666666666666\n",
            "i 10.249999999999826 \t 0.09166666666666666\n",
            "i 10.259999999999826 \t 0.09166666666666666\n",
            "i 10.269999999999825 \t 0.09166666666666666\n",
            "i 10.279999999999825 \t 0.09166666666666666\n",
            "i 10.289999999999825 \t 0.09166666666666666\n",
            "i 10.299999999999825 \t 0.09166666666666666\n",
            "i 10.309999999999825 \t 0.09166666666666666\n",
            "i 10.319999999999824 \t 0.09166666666666666\n",
            "i 10.329999999999824 \t 0.09166666666666666\n",
            "i 10.339999999999824 \t 0.09166666666666666\n",
            "i 10.349999999999824 \t 0.09166666666666666\n",
            "i 10.359999999999824 \t 0.09166666666666666\n",
            "i 10.369999999999823 \t 0.09166666666666666\n",
            "i 10.379999999999823 \t 0.09166666666666666\n",
            "i 10.389999999999823 \t 0.09166666666666666\n",
            "i 10.399999999999823 \t 0.09166666666666666\n",
            "i 10.409999999999823 \t 0.09166666666666666\n",
            "i 10.419999999999822 \t 0.09166666666666666\n",
            "i 10.429999999999822 \t 0.09166666666666666\n",
            "i 10.439999999999822 \t 0.09166666666666666\n",
            "i 10.449999999999822 \t 0.09166666666666666\n",
            "i 10.459999999999821 \t 0.09166666666666666\n",
            "i 10.469999999999821 \t 0.09166666666666666\n",
            "i 10.479999999999821 \t 0.09166666666666666\n",
            "i 10.48999999999982 \t 0.09166666666666666\n",
            "i 10.49999999999982 \t 0.09166666666666666\n",
            "i 10.50999999999982 \t 0.09166666666666666\n",
            "i 10.51999999999982 \t 0.09166666666666666\n",
            "i 10.52999999999982 \t 0.09166666666666666\n",
            "i 10.53999999999982 \t 0.09166666666666666\n",
            "i 10.54999999999982 \t 0.09166666666666666\n",
            "i 10.55999999999982 \t 0.09166666666666666\n",
            "i 10.569999999999819 \t 0.09166666666666666\n",
            "i 10.579999999999819 \t 0.09166666666666666\n",
            "i 10.589999999999819 \t 0.09166666666666666\n",
            "i 10.599999999999818 \t 0.09166666666666666\n",
            "i 10.609999999999818 \t 0.09166666666666666\n",
            "i 10.619999999999818 \t 0.09166666666666666\n",
            "i 10.629999999999818 \t 0.09166666666666666\n",
            "i 10.639999999999818 \t 0.09166666666666666\n",
            "i 10.649999999999817 \t 0.09166666666666666\n",
            "i 10.659999999999817 \t 0.09166666666666666\n",
            "i 10.669999999999817 \t 0.09166666666666666\n",
            "i 10.679999999999817 \t 0.09166666666666666\n",
            "i 10.689999999999817 \t 0.09166666666666666\n",
            "i 10.699999999999816 \t 0.09166666666666666\n",
            "i 10.709999999999816 \t 0.09166666666666666\n",
            "i 10.719999999999816 \t 0.09166666666666666\n",
            "i 10.729999999999816 \t 0.09166666666666666\n",
            "i 10.739999999999815 \t 0.09166666666666666\n",
            "i 10.749999999999815 \t 0.09166666666666666\n",
            "i 10.759999999999815 \t 0.09166666666666666\n",
            "i 10.769999999999815 \t 0.09166666666666666\n",
            "i 10.779999999999815 \t 0.09166666666666666\n",
            "i 10.789999999999814 \t 0.09166666666666666\n",
            "i 10.799999999999814 \t 0.09166666666666666\n",
            "i 10.809999999999814 \t 0.09166666666666666\n",
            "i 10.819999999999814 \t 0.09166666666666666\n",
            "i 10.829999999999814 \t 0.09166666666666666\n",
            "i 10.839999999999813 \t 0.09166666666666666\n",
            "i 10.849999999999813 \t 0.09166666666666666\n",
            "i 10.859999999999813 \t 0.09166666666666666\n",
            "i 10.869999999999813 \t 0.09166666666666666\n",
            "i 10.879999999999812 \t 0.09166666666666666\n",
            "i 10.889999999999812 \t 0.09166666666666666\n",
            "i 10.899999999999812 \t 0.09166666666666666\n",
            "i 10.909999999999812 \t 0.09166666666666666\n",
            "i 10.919999999999812 \t 0.09166666666666666\n",
            "i 10.929999999999811 \t 0.09166666666666666\n",
            "i 10.939999999999811 \t 0.09166666666666666\n",
            "i 10.949999999999811 \t 0.09166666666666666\n",
            "i 10.95999999999981 \t 0.09166666666666666\n",
            "i 10.96999999999981 \t 0.09166666666666666\n",
            "i 10.97999999999981 \t 0.09166666666666666\n",
            "i 10.98999999999981 \t 0.09166666666666666\n",
            "i 10.99999999999981 \t 0.09166666666666666\n",
            "i 11.00999999999981 \t 0.09166666666666666\n",
            "i 11.01999999999981 \t 0.09166666666666666\n",
            "i 11.02999999999981 \t 0.09166666666666666\n",
            "i 11.039999999999809 \t 0.09166666666666666\n",
            "i 11.049999999999809 \t 0.09166666666666666\n",
            "i 11.059999999999809 \t 0.09166666666666666\n",
            "i 11.069999999999808 \t 0.09166666666666666\n",
            "i 11.079999999999808 \t 0.09166666666666666\n",
            "i 11.089999999999808 \t 0.09166666666666666\n",
            "i 11.099999999999808 \t 0.09166666666666666\n",
            "i 11.109999999999808 \t 0.09166666666666666\n",
            "i 11.119999999999807 \t 0.09166666666666666\n",
            "i 11.129999999999807 \t 0.09166666666666666\n",
            "i 11.139999999999807 \t 0.09166666666666666\n",
            "i 11.149999999999807 \t 0.09166666666666666\n",
            "i 11.159999999999807 \t 0.09166666666666666\n",
            "i 11.169999999999806 \t 0.09166666666666666\n",
            "i 11.179999999999806 \t 0.09166666666666666\n",
            "i 11.189999999999806 \t 0.09166666666666666\n",
            "i 11.199999999999806 \t 0.09166666666666666\n",
            "i 11.209999999999805 \t 0.09166666666666666\n",
            "i 11.219999999999805 \t 0.09166666666666666\n",
            "i 11.229999999999805 \t 0.09166666666666666\n",
            "i 11.239999999999805 \t 0.09166666666666666\n",
            "i 11.249999999999805 \t 0.09166666666666666\n",
            "i 11.259999999999804 \t 0.09166666666666666\n",
            "i 11.269999999999804 \t 0.09166666666666666\n",
            "i 11.279999999999804 \t 0.09166666666666666\n",
            "i 11.289999999999804 \t 0.09166666666666666\n",
            "i 11.299999999999804 \t 0.09166666666666666\n",
            "i 11.309999999999803 \t 0.09166666666666666\n",
            "i 11.319999999999803 \t 0.09166666666666666\n",
            "i 11.329999999999803 \t 0.09166666666666666\n",
            "i 11.339999999999803 \t 0.09166666666666666\n",
            "i 11.349999999999802 \t 0.09166666666666666\n",
            "i 11.359999999999802 \t 0.09166666666666666\n",
            "i 11.369999999999802 \t 0.09166666666666666\n",
            "i 11.379999999999802 \t 0.09166666666666666\n",
            "i 11.389999999999802 \t 0.09166666666666666\n",
            "i 11.399999999999801 \t 0.09166666666666666\n",
            "i 11.409999999999801 \t 0.09166666666666666\n",
            "i 11.419999999999801 \t 0.09166666666666666\n",
            "i 11.4299999999998 \t 0.09166666666666666\n",
            "i 11.4399999999998 \t 0.09166666666666666\n",
            "i 11.4499999999998 \t 0.09166666666666666\n",
            "i 11.4599999999998 \t 0.09166666666666666\n",
            "i 11.4699999999998 \t 0.09166666666666666\n",
            "i 11.4799999999998 \t 0.09166666666666666\n",
            "i 11.4899999999998 \t 0.09166666666666666\n",
            "i 11.4999999999998 \t 0.09166666666666666\n",
            "i 11.509999999999799 \t 0.09166666666666666\n",
            "i 11.519999999999799 \t 0.09166666666666666\n",
            "i 11.529999999999799 \t 0.09166666666666666\n",
            "i 11.539999999999798 \t 0.09166666666666666\n",
            "i 11.549999999999798 \t 0.09166666666666666\n",
            "i 11.559999999999798 \t 0.09166666666666666\n",
            "i 11.569999999999798 \t 0.09166666666666666\n",
            "i 11.579999999999798 \t 0.09166666666666666\n",
            "i 11.589999999999797 \t 0.09166666666666666\n",
            "i 11.599999999999797 \t 0.09166666666666666\n",
            "i 11.609999999999797 \t 0.09166666666666666\n",
            "i 11.619999999999797 \t 0.09166666666666666\n",
            "i 11.629999999999797 \t 0.09166666666666666\n",
            "i 11.639999999999796 \t 0.09166666666666666\n",
            "i 11.649999999999796 \t 0.09166666666666666\n",
            "i 11.659999999999796 \t 0.09166666666666666\n",
            "i 11.669999999999796 \t 0.09166666666666666\n",
            "i 11.679999999999795 \t 0.09166666666666666\n",
            "i 11.689999999999795 \t 0.09166666666666666\n",
            "i 11.699999999999795 \t 0.09166666666666666\n",
            "i 11.709999999999795 \t 0.09166666666666666\n",
            "i 11.719999999999795 \t 0.09166666666666666\n",
            "i 11.729999999999794 \t 0.09166666666666666\n",
            "i 11.739999999999794 \t 0.09166666666666666\n",
            "i 11.749999999999794 \t 0.09166666666666666\n",
            "i 11.759999999999794 \t 0.09166666666666666\n",
            "i 11.769999999999794 \t 0.09166666666666666\n",
            "i 11.779999999999793 \t 0.09166666666666666\n",
            "i 11.789999999999793 \t 0.09166666666666666\n",
            "i 11.799999999999793 \t 0.09166666666666666\n",
            "i 11.809999999999793 \t 0.09166666666666666\n",
            "i 11.819999999999792 \t 0.09166666666666666\n",
            "i 11.829999999999792 \t 0.09166666666666666\n",
            "i 11.839999999999792 \t 0.09166666666666666\n",
            "i 11.849999999999792 \t 0.09166666666666666\n",
            "i 11.859999999999792 \t 0.09166666666666666\n",
            "i 11.869999999999791 \t 0.09166666666666666\n",
            "i 11.879999999999791 \t 0.09166666666666666\n",
            "i 11.889999999999791 \t 0.09166666666666666\n",
            "i 11.89999999999979 \t 0.09166666666666666\n",
            "i 11.90999999999979 \t 0.09166666666666666\n",
            "i 11.91999999999979 \t 0.09166666666666666\n",
            "i 11.92999999999979 \t 0.09166666666666666\n",
            "i 11.93999999999979 \t 0.09166666666666666\n",
            "i 11.94999999999979 \t 0.09166666666666666\n",
            "i 11.95999999999979 \t 0.09166666666666666\n",
            "i 11.96999999999979 \t 0.09166666666666666\n",
            "i 11.979999999999789 \t 0.09166666666666666\n",
            "i 11.989999999999789 \t 0.09166666666666666\n",
            "i 11.999999999999789 \t 0.09166666666666666\n",
            "i 12.009999999999788 \t 0.09166666666666666\n",
            "i 12.019999999999788 \t 0.09166666666666666\n",
            "i 12.029999999999788 \t 0.09166666666666666\n",
            "i 12.039999999999788 \t 0.09166666666666666\n",
            "i 12.049999999999788 \t 0.09166666666666666\n",
            "i 12.059999999999787 \t 0.09166666666666666\n",
            "i 12.069999999999787 \t 0.09166666666666666\n",
            "i 12.079999999999787 \t 0.09166666666666666\n",
            "i 12.089999999999787 \t 0.09166666666666666\n",
            "i 12.099999999999786 \t 0.09166666666666666\n",
            "i 12.109999999999786 \t 0.09166666666666666\n",
            "i 12.119999999999786 \t 0.09166666666666666\n",
            "i 12.129999999999786 \t 0.09166666666666666\n",
            "i 12.139999999999786 \t 0.09166666666666666\n",
            "i 12.149999999999785 \t 0.09166666666666666\n",
            "i 12.159999999999785 \t 0.09166666666666666\n",
            "i 12.169999999999785 \t 0.09166666666666666\n",
            "i 12.179999999999785 \t 0.09166666666666666\n",
            "i 12.189999999999785 \t 0.09166666666666666\n",
            "i 12.199999999999784 \t 0.09166666666666666\n",
            "i 12.209999999999784 \t 0.09166666666666666\n",
            "i 12.219999999999784 \t 0.09166666666666666\n",
            "i 12.229999999999784 \t 0.09166666666666666\n",
            "i 12.239999999999783 \t 0.09166666666666666\n",
            "i 12.249999999999783 \t 0.09166666666666666\n",
            "i 12.259999999999783 \t 0.09166666666666666\n",
            "i 12.269999999999783 \t 0.09166666666666666\n",
            "i 12.279999999999783 \t 0.09166666666666666\n",
            "i 12.289999999999782 \t 0.09166666666666666\n",
            "i 12.299999999999782 \t 0.09166666666666666\n",
            "i 12.309999999999782 \t 0.09166666666666666\n",
            "i 12.319999999999782 \t 0.09166666666666666\n",
            "i 12.329999999999782 \t 0.09166666666666666\n",
            "i 12.339999999999781 \t 0.09166666666666666\n",
            "i 12.349999999999781 \t 0.09166666666666666\n",
            "i 12.359999999999781 \t 0.09166666666666666\n",
            "i 12.36999999999978 \t 0.09166666666666666\n",
            "i 12.37999999999978 \t 0.09166666666666666\n",
            "i 12.38999999999978 \t 0.09166666666666666\n",
            "i 12.39999999999978 \t 0.09166666666666666\n",
            "i 12.40999999999978 \t 0.09166666666666666\n",
            "i 12.41999999999978 \t 0.09166666666666666\n",
            "i 12.42999999999978 \t 0.09166666666666666\n",
            "i 12.43999999999978 \t 0.09166666666666666\n",
            "i 12.449999999999779 \t 0.09166666666666666\n",
            "i 12.459999999999779 \t 0.09166666666666666\n",
            "i 12.469999999999779 \t 0.09166666666666666\n",
            "i 12.479999999999778 \t 0.09166666666666666\n",
            "i 12.489999999999778 \t 0.09166666666666666\n",
            "i 12.499999999999778 \t 0.09166666666666666\n",
            "i 12.509999999999778 \t 0.09166666666666666\n",
            "i 12.519999999999778 \t 0.09166666666666666\n",
            "i 12.529999999999777 \t 0.09166666666666666\n",
            "i 12.539999999999777 \t 0.09166666666666666\n",
            "i 12.549999999999777 \t 0.09166666666666666\n",
            "i 12.559999999999777 \t 0.09166666666666666\n",
            "i 12.569999999999776 \t 0.09166666666666666\n",
            "i 12.579999999999776 \t 0.09166666666666666\n",
            "i 12.589999999999776 \t 0.09166666666666666\n",
            "i 12.599999999999776 \t 0.09166666666666666\n",
            "i 12.609999999999776 \t 0.09166666666666666\n",
            "i 12.619999999999775 \t 0.09166666666666666\n",
            "i 12.629999999999775 \t 0.09166666666666666\n",
            "i 12.639999999999775 \t 0.09166666666666666\n",
            "i 12.649999999999775 \t 0.09166666666666666\n",
            "i 12.659999999999775 \t 0.09166666666666666\n",
            "i 12.669999999999774 \t 0.09166666666666666\n",
            "i 12.679999999999774 \t 0.09166666666666666\n",
            "i 12.689999999999774 \t 0.09166666666666666\n",
            "i 12.699999999999774 \t 0.09166666666666666\n",
            "i 12.709999999999773 \t 0.09166666666666666\n",
            "i 12.719999999999773 \t 0.09166666666666666\n",
            "i 12.729999999999773 \t 0.09166666666666666\n",
            "i 12.739999999999773 \t 0.09166666666666666\n",
            "i 12.749999999999773 \t 0.09166666666666666\n",
            "i 12.759999999999772 \t 0.09166666666666666\n",
            "i 12.769999999999772 \t 0.09166666666666666\n",
            "i 12.779999999999772 \t 0.09166666666666666\n",
            "i 12.789999999999772 \t 0.09166666666666666\n",
            "i 12.799999999999772 \t 0.09166666666666666\n",
            "i 12.809999999999771 \t 0.09166666666666666\n",
            "i 12.819999999999771 \t 0.09166666666666666\n",
            "i 12.829999999999771 \t 0.09166666666666666\n",
            "i 12.83999999999977 \t 0.09166666666666666\n",
            "i 12.84999999999977 \t 0.09166666666666666\n",
            "i 12.85999999999977 \t 0.09166666666666666\n",
            "i 12.86999999999977 \t 0.09166666666666666\n",
            "i 12.87999999999977 \t 0.09166666666666666\n",
            "i 12.88999999999977 \t 0.09166666666666666\n",
            "i 12.89999999999977 \t 0.09166666666666666\n",
            "i 12.90999999999977 \t 0.09166666666666666\n",
            "i 12.919999999999769 \t 0.09166666666666666\n",
            "i 12.929999999999769 \t 0.09166666666666666\n",
            "i 12.939999999999769 \t 0.09166666666666666\n",
            "i 12.949999999999768 \t 0.09166666666666666\n",
            "i 12.959999999999768 \t 0.09166666666666666\n",
            "i 12.969999999999768 \t 0.09166666666666666\n",
            "i 12.979999999999768 \t 0.09166666666666666\n",
            "i 12.989999999999768 \t 0.09166666666666666\n",
            "i 12.999999999999767 \t 0.09166666666666666\n",
            "i 13.009999999999767 \t 0.09166666666666666\n",
            "i 13.019999999999767 \t 0.09166666666666666\n",
            "i 13.029999999999767 \t 0.09166666666666666\n",
            "i 13.039999999999766 \t 0.09166666666666666\n",
            "i 13.049999999999766 \t 0.09166666666666666\n",
            "i 13.059999999999766 \t 0.09166666666666666\n",
            "i 13.069999999999766 \t 0.09166666666666666\n",
            "i 13.079999999999766 \t 0.09166666666666666\n",
            "i 13.089999999999765 \t 0.09166666666666666\n",
            "i 13.099999999999765 \t 0.09166666666666666\n",
            "i 13.109999999999765 \t 0.09166666666666666\n",
            "i 13.119999999999765 \t 0.09166666666666666\n",
            "i 13.129999999999765 \t 0.09166666666666666\n",
            "i 13.139999999999764 \t 0.09166666666666666\n",
            "i 13.149999999999764 \t 0.09166666666666666\n",
            "i 13.159999999999764 \t 0.09166666666666666\n",
            "i 13.169999999999764 \t 0.09166666666666666\n",
            "i 13.179999999999763 \t 0.09166666666666666\n",
            "i 13.189999999999763 \t 0.09166666666666666\n",
            "i 13.199999999999763 \t 0.09166666666666666\n",
            "i 13.209999999999763 \t 0.09166666666666666\n",
            "i 13.219999999999763 \t 0.09166666666666666\n",
            "i 13.229999999999762 \t 0.09166666666666666\n",
            "i 13.239999999999762 \t 0.09166666666666666\n",
            "i 13.249999999999762 \t 0.09166666666666666\n",
            "i 13.259999999999762 \t 0.09166666666666666\n",
            "i 13.269999999999762 \t 0.09166666666666666\n",
            "i 13.279999999999761 \t 0.09166666666666666\n",
            "i 13.289999999999761 \t 0.09166666666666666\n",
            "i 13.299999999999761 \t 0.09166666666666666\n",
            "i 13.30999999999976 \t 0.09166666666666666\n",
            "i 13.31999999999976 \t 0.09166666666666666\n",
            "i 13.32999999999976 \t 0.09166666666666666\n",
            "i 13.33999999999976 \t 0.09166666666666666\n",
            "i 13.34999999999976 \t 0.09166666666666666\n",
            "i 13.35999999999976 \t 0.09166666666666666\n",
            "i 13.36999999999976 \t 0.09166666666666666\n",
            "i 13.37999999999976 \t 0.09166666666666666\n",
            "i 13.389999999999759 \t 0.09166666666666666\n",
            "i 13.399999999999759 \t 0.09166666666666666\n",
            "i 13.409999999999759 \t 0.09166666666666666\n",
            "i 13.419999999999758 \t 0.09166666666666666\n",
            "i 13.429999999999758 \t 0.09166666666666666\n",
            "i 13.439999999999758 \t 0.09166666666666666\n",
            "i 13.449999999999758 \t 0.09166666666666666\n",
            "i 13.459999999999757 \t 0.09166666666666666\n",
            "i 13.469999999999757 \t 0.09166666666666666\n",
            "i 13.479999999999757 \t 0.09166666666666666\n",
            "i 13.489999999999757 \t 0.09166666666666666\n",
            "i 13.499999999999757 \t 0.09166666666666666\n",
            "i 13.509999999999756 \t 0.09166666666666666\n",
            "i 13.519999999999756 \t 0.09166666666666666\n",
            "i 13.529999999999756 \t 0.09166666666666666\n",
            "i 13.539999999999756 \t 0.09166666666666666\n",
            "i 13.549999999999756 \t 0.09166666666666666\n",
            "i 13.559999999999755 \t 0.09166666666666666\n",
            "i 13.569999999999755 \t 0.09166666666666666\n",
            "i 13.579999999999755 \t 0.09166666666666666\n",
            "i 13.589999999999755 \t 0.09166666666666666\n",
            "i 13.599999999999755 \t 0.09166666666666666\n",
            "i 13.609999999999754 \t 0.09166666666666666\n",
            "i 13.619999999999754 \t 0.09166666666666666\n",
            "i 13.629999999999754 \t 0.09166666666666666\n",
            "i 13.639999999999754 \t 0.09166666666666666\n",
            "i 13.649999999999753 \t 0.09166666666666666\n",
            "i 13.659999999999753 \t 0.09166666666666666\n",
            "i 13.669999999999753 \t 0.09166666666666666\n",
            "i 13.679999999999753 \t 0.09166666666666666\n",
            "i 13.689999999999753 \t 0.09166666666666666\n",
            "i 13.699999999999752 \t 0.09166666666666666\n",
            "i 13.709999999999752 \t 0.09166666666666666\n",
            "i 13.719999999999752 \t 0.09166666666666666\n",
            "i 13.729999999999752 \t 0.09166666666666666\n",
            "i 13.739999999999752 \t 0.09166666666666666\n",
            "i 13.749999999999751 \t 0.09166666666666666\n",
            "i 13.759999999999751 \t 0.09166666666666666\n",
            "i 13.76999999999975 \t 0.09166666666666666\n",
            "i 13.77999999999975 \t 0.09166666666666666\n",
            "i 13.78999999999975 \t 0.09166666666666666\n",
            "i 13.79999999999975 \t 0.09166666666666666\n",
            "i 13.80999999999975 \t 0.09166666666666666\n",
            "i 13.81999999999975 \t 0.09166666666666666\n",
            "i 13.82999999999975 \t 0.09166666666666666\n",
            "i 13.83999999999975 \t 0.09166666666666666\n",
            "i 13.84999999999975 \t 0.09166666666666666\n",
            "i 13.859999999999749 \t 0.09166666666666666\n",
            "i 13.869999999999749 \t 0.09166666666666666\n",
            "i 13.879999999999749 \t 0.09166666666666666\n",
            "i 13.889999999999748 \t 0.09166666666666666\n",
            "i 13.899999999999748 \t 0.09166666666666666\n",
            "i 13.909999999999748 \t 0.09166666666666666\n",
            "i 13.919999999999748 \t 0.09166666666666666\n",
            "i 13.929999999999747 \t 0.09166666666666666\n",
            "i 13.939999999999747 \t 0.09166666666666666\n",
            "i 13.949999999999747 \t 0.09166666666666666\n",
            "i 13.959999999999747 \t 0.09166666666666666\n",
            "i 13.969999999999747 \t 0.09166666666666666\n",
            "i 13.979999999999746 \t 0.09166666666666666\n",
            "i 13.989999999999746 \t 0.09166666666666666\n",
            "i 13.999999999999746 \t 0.09166666666666666\n",
            "i 14.009999999999746 \t 0.09166666666666666\n",
            "i 14.019999999999746 \t 0.09166666666666666\n",
            "i 14.029999999999745 \t 0.09166666666666666\n",
            "i 14.039999999999745 \t 0.09166666666666666\n",
            "i 14.049999999999745 \t 0.09166666666666666\n",
            "i 14.059999999999745 \t 0.09166666666666666\n",
            "i 14.069999999999744 \t 0.09166666666666666\n",
            "i 14.079999999999744 \t 0.09166666666666666\n",
            "i 14.089999999999744 \t 0.09166666666666666\n",
            "i 14.099999999999744 \t 0.09166666666666666\n",
            "i 14.109999999999744 \t 0.09166666666666666\n",
            "i 14.119999999999743 \t 0.09166666666666666\n",
            "i 14.129999999999743 \t 0.09166666666666666\n",
            "i 14.139999999999743 \t 0.09166666666666666\n",
            "i 14.149999999999743 \t 0.09166666666666666\n",
            "i 14.159999999999743 \t 0.09166666666666666\n",
            "i 14.169999999999742 \t 0.09166666666666666\n",
            "i 14.179999999999742 \t 0.09166666666666666\n",
            "i 14.189999999999742 \t 0.09166666666666666\n",
            "i 14.199999999999742 \t 0.09166666666666666\n",
            "i 14.209999999999742 \t 0.09166666666666666\n",
            "i 14.219999999999741 \t 0.09166666666666666\n",
            "i 14.229999999999741 \t 0.09166666666666666\n",
            "i 14.23999999999974 \t 0.09166666666666666\n",
            "i 14.24999999999974 \t 0.09166666666666666\n",
            "i 14.25999999999974 \t 0.09166666666666666\n",
            "i 14.26999999999974 \t 0.09166666666666666\n",
            "i 14.27999999999974 \t 0.09166666666666666\n",
            "i 14.28999999999974 \t 0.09166666666666666\n",
            "i 14.29999999999974 \t 0.09166666666666666\n",
            "i 14.30999999999974 \t 0.09166666666666666\n",
            "i 14.31999999999974 \t 0.09166666666666666\n",
            "i 14.329999999999739 \t 0.09166666666666666\n",
            "i 14.339999999999739 \t 0.09166666666666666\n",
            "i 14.349999999999739 \t 0.09166666666666666\n",
            "i 14.359999999999738 \t 0.09166666666666666\n",
            "i 14.369999999999738 \t 0.09166666666666666\n",
            "i 14.379999999999738 \t 0.09166666666666666\n",
            "i 14.389999999999738 \t 0.09166666666666666\n",
            "i 14.399999999999737 \t 0.09166666666666666\n",
            "i 14.409999999999737 \t 0.09166666666666666\n",
            "i 14.419999999999737 \t 0.09166666666666666\n",
            "i 14.429999999999737 \t 0.09166666666666666\n",
            "i 14.439999999999737 \t 0.09166666666666666\n",
            "i 14.449999999999736 \t 0.09166666666666666\n",
            "i 14.459999999999736 \t 0.09166666666666666\n",
            "i 14.469999999999736 \t 0.09166666666666666\n",
            "i 14.479999999999736 \t 0.09166666666666666\n",
            "i 14.489999999999736 \t 0.09166666666666666\n",
            "i 14.499999999999735 \t 0.09166666666666666\n",
            "i 14.509999999999735 \t 0.09166666666666666\n",
            "i 14.519999999999735 \t 0.09166666666666666\n",
            "i 14.529999999999735 \t 0.09166666666666666\n",
            "i 14.539999999999734 \t 0.09166666666666666\n",
            "i 14.549999999999734 \t 0.09166666666666666\n",
            "i 14.559999999999734 \t 0.09166666666666666\n",
            "i 14.569999999999734 \t 0.09166666666666666\n",
            "i 14.579999999999734 \t 0.09166666666666666\n",
            "i 14.589999999999733 \t 0.09166666666666666\n",
            "i 14.599999999999733 \t 0.09166666666666666\n",
            "i 14.609999999999733 \t 0.09166666666666666\n",
            "i 14.619999999999733 \t 0.09166666666666666\n",
            "i 14.629999999999733 \t 0.09166666666666666\n",
            "i 14.639999999999732 \t 0.09166666666666666\n",
            "i 14.649999999999732 \t 0.09166666666666666\n",
            "i 14.659999999999732 \t 0.09166666666666666\n",
            "i 14.669999999999732 \t 0.09166666666666666\n",
            "i 14.679999999999731 \t 0.09166666666666666\n",
            "i 14.689999999999731 \t 0.09166666666666666\n",
            "i 14.699999999999731 \t 0.09166666666666666\n",
            "i 14.70999999999973 \t 0.09166666666666666\n",
            "i 14.71999999999973 \t 0.09166666666666666\n",
            "i 14.72999999999973 \t 0.09166666666666666\n",
            "i 14.73999999999973 \t 0.09166666666666666\n",
            "i 14.74999999999973 \t 0.09166666666666666\n",
            "i 14.75999999999973 \t 0.09166666666666666\n",
            "i 14.76999999999973 \t 0.09166666666666666\n",
            "i 14.77999999999973 \t 0.09166666666666666\n",
            "i 14.78999999999973 \t 0.09166666666666666\n",
            "i 14.799999999999729 \t 0.09166666666666666\n",
            "i 14.809999999999729 \t 0.09166666666666666\n",
            "i 14.819999999999729 \t 0.09166666666666666\n",
            "i 14.829999999999728 \t 0.09166666666666666\n",
            "i 14.839999999999728 \t 0.09166666666666666\n",
            "i 14.849999999999728 \t 0.09166666666666666\n",
            "i 14.859999999999728 \t 0.09166666666666666\n",
            "i 14.869999999999727 \t 0.09166666666666666\n",
            "i 14.879999999999727 \t 0.09166666666666666\n",
            "i 14.889999999999727 \t 0.09166666666666666\n",
            "i 14.899999999999727 \t 0.09166666666666666\n",
            "i 14.909999999999727 \t 0.09166666666666666\n",
            "i 14.919999999999726 \t 0.09166666666666666\n",
            "i 14.929999999999726 \t 0.09166666666666666\n",
            "i 14.939999999999726 \t 0.09166666666666666\n",
            "i 14.949999999999726 \t 0.09166666666666666\n",
            "i 14.959999999999726 \t 0.09166666666666666\n",
            "i 14.969999999999725 \t 0.09166666666666666\n",
            "i 14.979999999999725 \t 0.09166666666666666\n",
            "i 14.989999999999725 \t 0.09166666666666666\n",
            "i 14.999999999999725 \t 0.09166666666666666\n",
            "i 15.009999999999724 \t 0.09166666666666666\n",
            "i 15.019999999999724 \t 0.09166666666666666\n",
            "i 15.029999999999724 \t 0.09166666666666666\n",
            "i 15.039999999999724 \t 0.09166666666666666\n",
            "i 15.049999999999724 \t 0.09166666666666666\n",
            "i 15.059999999999723 \t 0.09166666666666666\n",
            "i 15.069999999999723 \t 0.09166666666666666\n",
            "i 15.079999999999723 \t 0.09166666666666666\n",
            "i 15.089999999999723 \t 0.09166666666666666\n",
            "i 15.099999999999723 \t 0.09166666666666666\n",
            "i 15.109999999999722 \t 0.09166666666666666\n",
            "i 15.119999999999722 \t 0.09166666666666666\n",
            "i 15.129999999999722 \t 0.09166666666666666\n",
            "i 15.139999999999722 \t 0.09166666666666666\n",
            "i 15.149999999999721 \t 0.09166666666666666\n",
            "i 15.159999999999721 \t 0.09166666666666666\n",
            "i 15.169999999999721 \t 0.09166666666666666\n",
            "i 15.17999999999972 \t 0.09166666666666666\n",
            "i 15.18999999999972 \t 0.09166666666666666\n",
            "i 15.19999999999972 \t 0.09166666666666666\n",
            "i 15.20999999999972 \t 0.09166666666666666\n",
            "i 15.21999999999972 \t 0.09166666666666666\n",
            "i 15.22999999999972 \t 0.09166666666666666\n",
            "i 15.23999999999972 \t 0.09166666666666666\n",
            "i 15.24999999999972 \t 0.09166666666666666\n",
            "i 15.25999999999972 \t 0.09166666666666666\n",
            "i 15.269999999999719 \t 0.09166666666666666\n",
            "i 15.279999999999719 \t 0.09166666666666666\n",
            "i 15.289999999999718 \t 0.09166666666666666\n",
            "i 15.299999999999718 \t 0.09166666666666666\n",
            "i 15.309999999999718 \t 0.09166666666666666\n",
            "i 15.319999999999718 \t 0.09166666666666666\n",
            "i 15.329999999999718 \t 0.09166666666666666\n",
            "i 15.339999999999717 \t 0.09166666666666666\n",
            "i 15.349999999999717 \t 0.09166666666666666\n",
            "i 15.359999999999717 \t 0.09166666666666666\n",
            "i 15.369999999999717 \t 0.09166666666666666\n",
            "i 15.379999999999717 \t 0.09166666666666666\n",
            "i 15.389999999999716 \t 0.09166666666666666\n",
            "i 15.399999999999716 \t 0.09166666666666666\n",
            "i 15.409999999999716 \t 0.09166666666666666\n",
            "i 15.419999999999716 \t 0.09166666666666666\n",
            "i 15.429999999999715 \t 0.09166666666666666\n",
            "i 15.439999999999715 \t 0.09166666666666666\n",
            "i 15.449999999999715 \t 0.09166666666666666\n",
            "i 15.459999999999715 \t 0.09166666666666666\n",
            "i 15.469999999999715 \t 0.09166666666666666\n",
            "i 15.479999999999714 \t 0.09166666666666666\n",
            "i 15.489999999999714 \t 0.09166666666666666\n",
            "i 15.499999999999714 \t 0.09166666666666666\n",
            "i 15.509999999999714 \t 0.09166666666666666\n",
            "i 15.519999999999714 \t 0.09166666666666666\n",
            "i 15.529999999999713 \t 0.09166666666666666\n",
            "i 15.539999999999713 \t 0.09166666666666666\n",
            "i 15.549999999999713 \t 0.09166666666666666\n",
            "i 15.559999999999713 \t 0.09166666666666666\n",
            "i 15.569999999999713 \t 0.09166666666666666\n",
            "i 15.579999999999712 \t 0.09166666666666666\n",
            "i 15.589999999999712 \t 0.09166666666666666\n",
            "i 15.599999999999712 \t 0.09166666666666666\n",
            "i 15.609999999999712 \t 0.09166666666666666\n",
            "i 15.619999999999711 \t 0.09166666666666666\n",
            "i 15.629999999999711 \t 0.09166666666666666\n",
            "i 15.639999999999711 \t 0.09166666666666666\n",
            "i 15.64999999999971 \t 0.09166666666666666\n",
            "i 15.65999999999971 \t 0.09166666666666666\n",
            "i 15.66999999999971 \t 0.09166666666666666\n",
            "i 15.67999999999971 \t 0.09166666666666666\n",
            "i 15.68999999999971 \t 0.09166666666666666\n",
            "i 15.69999999999971 \t 0.09166666666666666\n",
            "i 15.70999999999971 \t 0.09166666666666666\n",
            "i 15.71999999999971 \t 0.09166666666666666\n",
            "i 15.729999999999709 \t 0.09166666666666666\n",
            "i 15.739999999999709 \t 0.09166666666666666\n",
            "i 15.749999999999709 \t 0.09166666666666666\n",
            "i 15.759999999999708 \t 0.09166666666666666\n",
            "i 15.769999999999708 \t 0.09166666666666666\n",
            "i 15.779999999999708 \t 0.09166666666666666\n",
            "i 15.789999999999708 \t 0.09166666666666666\n",
            "i 15.799999999999708 \t 0.09166666666666666\n",
            "i 15.809999999999707 \t 0.09166666666666666\n",
            "i 15.819999999999707 \t 0.09166666666666666\n",
            "i 15.829999999999707 \t 0.09166666666666666\n",
            "i 15.839999999999707 \t 0.09166666666666666\n",
            "i 15.849999999999707 \t 0.09166666666666666\n",
            "i 15.859999999999706 \t 0.09166666666666666\n",
            "i 15.869999999999706 \t 0.09166666666666666\n",
            "i 15.879999999999706 \t 0.09166666666666666\n",
            "i 15.889999999999706 \t 0.09166666666666666\n",
            "i 15.899999999999705 \t 0.09166666666666666\n",
            "i 15.909999999999705 \t 0.09166666666666666\n",
            "i 15.919999999999705 \t 0.09166666666666666\n",
            "i 15.929999999999705 \t 0.09166666666666666\n",
            "i 15.939999999999705 \t 0.09166666666666666\n",
            "i 15.949999999999704 \t 0.09166666666666666\n",
            "i 15.959999999999704 \t 0.09166666666666666\n",
            "i 15.969999999999704 \t 0.09166666666666666\n",
            "i 15.979999999999704 \t 0.09166666666666666\n",
            "i 15.989999999999704 \t 0.09166666666666666\n",
            "i 15.999999999999703 \t 0.09166666666666666\n",
            "i 16.009999999999703 \t 0.09166666666666666\n",
            "i 16.019999999999705 \t 0.09166666666666666\n",
            "i 16.029999999999706 \t 0.09166666666666666\n",
            "i 16.039999999999708 \t 0.09166666666666666\n",
            "i 16.04999999999971 \t 0.09166666666666666\n",
            "i 16.05999999999971 \t 0.09166666666666666\n",
            "i 16.069999999999713 \t 0.09166666666666666\n",
            "i 16.079999999999714 \t 0.09166666666666666\n",
            "i 16.089999999999716 \t 0.09166666666666666\n",
            "i 16.099999999999717 \t 0.09166666666666666\n",
            "i 16.10999999999972 \t 0.09166666666666666\n",
            "i 16.11999999999972 \t 0.09166666666666666\n",
            "i 16.129999999999722 \t 0.09166666666666666\n",
            "i 16.139999999999723 \t 0.09166666666666666\n",
            "i 16.149999999999725 \t 0.09166666666666666\n",
            "i 16.159999999999727 \t 0.09166666666666666\n",
            "i 16.169999999999728 \t 0.09166666666666666\n",
            "i 16.17999999999973 \t 0.09166666666666666\n",
            "i 16.18999999999973 \t 0.09166666666666666\n",
            "i 16.199999999999733 \t 0.09166666666666666\n",
            "i 16.209999999999734 \t 0.09166666666666666\n",
            "i 16.219999999999736 \t 0.09166666666666666\n",
            "i 16.229999999999738 \t 0.09166666666666666\n",
            "i 16.23999999999974 \t 0.09166666666666666\n",
            "i 16.24999999999974 \t 0.09166666666666666\n",
            "i 16.259999999999742 \t 0.09166666666666666\n",
            "i 16.269999999999744 \t 0.09166666666666666\n",
            "i 16.279999999999745 \t 0.09166666666666666\n",
            "i 16.289999999999747 \t 0.09166666666666666\n",
            "i 16.29999999999975 \t 0.09166666666666666\n",
            "i 16.30999999999975 \t 0.09166666666666666\n",
            "i 16.31999999999975 \t 0.09166666666666666\n",
            "i 16.329999999999753 \t 0.09166666666666666\n",
            "i 16.339999999999755 \t 0.09166666666666666\n",
            "i 16.349999999999756 \t 0.09166666666666666\n",
            "i 16.359999999999758 \t 0.09166666666666666\n",
            "i 16.36999999999976 \t 0.09166666666666666\n",
            "i 16.37999999999976 \t 0.09166666666666666\n",
            "i 16.389999999999763 \t 0.09166666666666666\n",
            "i 16.399999999999764 \t 0.09166666666666666\n",
            "i 16.409999999999766 \t 0.09166666666666666\n",
            "i 16.419999999999767 \t 0.09166666666666666\n",
            "i 16.42999999999977 \t 0.09166666666666666\n",
            "i 16.43999999999977 \t 0.09166666666666666\n",
            "i 16.449999999999772 \t 0.09166666666666666\n",
            "i 16.459999999999773 \t 0.09166666666666666\n",
            "i 16.469999999999775 \t 0.09166666666666666\n",
            "i 16.479999999999777 \t 0.09166666666666666\n",
            "i 16.489999999999778 \t 0.09166666666666666\n",
            "i 16.49999999999978 \t 0.09166666666666666\n",
            "i 16.50999999999978 \t 0.09166666666666666\n",
            "i 16.519999999999783 \t 0.09166666666666666\n",
            "i 16.529999999999784 \t 0.09166666666666666\n",
            "i 16.539999999999786 \t 0.09166666666666666\n",
            "i 16.549999999999788 \t 0.09166666666666666\n",
            "i 16.55999999999979 \t 0.09166666666666666\n",
            "i 16.56999999999979 \t 0.09166666666666666\n",
            "i 16.579999999999792 \t 0.09166666666666666\n",
            "i 16.589999999999794 \t 0.09166666666666666\n",
            "i 16.599999999999795 \t 0.09166666666666666\n",
            "i 16.609999999999797 \t 0.09166666666666666\n",
            "i 16.6199999999998 \t 0.09166666666666666\n",
            "i 16.6299999999998 \t 0.09166666666666666\n",
            "i 16.6399999999998 \t 0.09166666666666666\n",
            "i 16.649999999999803 \t 0.09166666666666666\n",
            "i 16.659999999999805 \t 0.09166666666666666\n",
            "i 16.669999999999806 \t 0.09166666666666666\n",
            "i 16.679999999999808 \t 0.09166666666666666\n",
            "i 16.68999999999981 \t 0.09166666666666666\n",
            "i 16.69999999999981 \t 0.09166666666666666\n",
            "i 16.709999999999813 \t 0.09166666666666666\n",
            "i 16.719999999999814 \t 0.09166666666666666\n",
            "i 16.729999999999816 \t 0.09166666666666666\n",
            "i 16.739999999999817 \t 0.09166666666666666\n",
            "i 16.74999999999982 \t 0.09166666666666666\n",
            "i 16.75999999999982 \t 0.09166666666666666\n",
            "i 16.769999999999822 \t 0.09166666666666666\n",
            "i 16.779999999999824 \t 0.09166666666666666\n",
            "i 16.789999999999825 \t 0.09166666666666666\n",
            "i 16.799999999999827 \t 0.09166666666666666\n",
            "i 16.809999999999828 \t 0.09166666666666666\n",
            "i 16.81999999999983 \t 0.09166666666666666\n",
            "i 16.82999999999983 \t 0.09166666666666666\n",
            "i 16.839999999999833 \t 0.09166666666666666\n",
            "i 16.849999999999834 \t 0.09166666666666666\n",
            "i 16.859999999999836 \t 0.09166666666666666\n",
            "i 16.869999999999838 \t 0.09166666666666666\n",
            "i 16.87999999999984 \t 0.09166666666666666\n",
            "i 16.88999999999984 \t 0.09166666666666666\n",
            "i 16.899999999999842 \t 0.09166666666666666\n",
            "i 16.909999999999844 \t 0.09166666666666666\n",
            "i 16.919999999999845 \t 0.09166666666666666\n",
            "i 16.929999999999847 \t 0.09166666666666666\n",
            "i 16.93999999999985 \t 0.09166666666666666\n",
            "i 16.94999999999985 \t 0.09166666666666666\n",
            "i 16.95999999999985 \t 0.09166666666666666\n",
            "i 16.969999999999853 \t 0.09166666666666666\n",
            "i 16.979999999999855 \t 0.09166666666666666\n",
            "i 16.989999999999856 \t 0.09166666666666666\n",
            "i 16.999999999999858 \t 0.09166666666666666\n",
            "i 17.00999999999986 \t 0.09166666666666666\n",
            "i 17.01999999999986 \t 0.09166666666666666\n",
            "i 17.029999999999863 \t 0.09166666666666666\n",
            "i 17.039999999999864 \t 0.09166666666666666\n",
            "i 17.049999999999866 \t 0.09166666666666666\n",
            "i 17.059999999999867 \t 0.09166666666666666\n",
            "i 17.06999999999987 \t 0.09166666666666666\n",
            "i 17.07999999999987 \t 0.09166666666666666\n",
            "i 17.089999999999872 \t 0.09166666666666666\n",
            "i 17.099999999999874 \t 0.09166666666666666\n",
            "i 17.109999999999875 \t 0.09166666666666666\n",
            "i 17.119999999999877 \t 0.09166666666666666\n",
            "i 17.129999999999878 \t 0.09166666666666666\n",
            "i 17.13999999999988 \t 0.09166666666666666\n",
            "i 17.14999999999988 \t 0.09166666666666666\n",
            "i 17.159999999999883 \t 0.09166666666666666\n",
            "i 17.169999999999884 \t 0.09166666666666666\n",
            "i 17.179999999999886 \t 0.09166666666666666\n",
            "i 17.189999999999888 \t 0.09166666666666666\n",
            "i 17.19999999999989 \t 0.09166666666666666\n",
            "i 17.20999999999989 \t 0.09166666666666666\n",
            "i 17.219999999999892 \t 0.09166666666666666\n",
            "i 17.229999999999894 \t 0.09166666666666666\n",
            "i 17.239999999999895 \t 0.09166666666666666\n",
            "i 17.249999999999897 \t 0.09166666666666666\n",
            "i 17.2599999999999 \t 0.09166666666666666\n",
            "i 17.2699999999999 \t 0.09166666666666666\n",
            "i 17.2799999999999 \t 0.09166666666666666\n",
            "i 17.289999999999903 \t 0.09166666666666666\n",
            "i 17.299999999999905 \t 0.09166666666666666\n",
            "i 17.309999999999906 \t 0.09166666666666666\n",
            "i 17.319999999999908 \t 0.09166666666666666\n",
            "i 17.32999999999991 \t 0.09166666666666666\n",
            "i 17.33999999999991 \t 0.09166666666666666\n",
            "i 17.349999999999913 \t 0.09166666666666666\n",
            "i 17.359999999999914 \t 0.09166666666666666\n",
            "i 17.369999999999916 \t 0.09166666666666666\n",
            "i 17.379999999999917 \t 0.09166666666666666\n",
            "i 17.38999999999992 \t 0.09166666666666666\n",
            "i 17.39999999999992 \t 0.09166666666666666\n",
            "i 17.409999999999922 \t 0.09166666666666666\n",
            "i 17.419999999999924 \t 0.09166666666666666\n",
            "i 17.429999999999925 \t 0.09166666666666666\n",
            "i 17.439999999999927 \t 0.09166666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-dbe5868ac15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mF2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mF2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mksdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mks_2samp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mks_2samp\u001b[0;34m(data1, data2, alternative, mode)\u001b[0m\n\u001b[1;32m   6229\u001b[0m                         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_prob_outside_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6230\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6231\u001b[0;31m                         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_compute_prob_inside_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6232\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6233\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m_compute_prob_inside_method\u001b[0;34m(m, n, g, h)\u001b[0m\n\u001b[1;32m   5946\u001b[0m         \u001b[0;31m# First calculate the sliding window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5947\u001b[0m         \u001b[0mlastminj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlastmaxj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlastlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5948\u001b[0;31m         \u001b[0mminj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5949\u001b[0m         \u001b[0mminj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5950\u001b[0m         \u001b[0mmaxj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BopJf0HoI74h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNGJEzy9eV05",
        "outputId": "feb91c29-f4ac-48e6-afd7-b8eaadb0d524"
      },
      "source": [
        "print(clientsstatus)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'g', 11: 'g', 54: 'g', 45: 'g', 33: 'g', 35: 'g', 19: 'g', 28: 'g', 58: 'g', 50: 'g', 20: 'g', 18: 'g', 40: 'g', 10: 'g', 57: 'g', 59: 'g', 42: 'g', 31: 'g', 52: 'g', 17: 'g', 4: 'g', 7: 'g', 53: 'g', 12: 'g', 34: 'g', 15: 'g', 55: 'g', 46: 'g', 47: 'g', 49: 'g', 2: 'g', 14: 'g', 6: 'g', 26: 'g', 51: 'g', 41: 'g', 21: 'g', 8: 'g', 23: 'g', 43: 'g', 56: 'g', 27: 'g', 32: 'g', 9: 'g', 3: 'g', 44: 'g', 24: 'g', 37: 'g', 48: 'g', 30: 'g', 22: 'g', 29: 'g', 13: 'g', 16: 'g', 39: 'b', 38: 'b', 0: 'b', 5: 'b', 25: 'b', 36: 'b'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVg24azMgyJ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}